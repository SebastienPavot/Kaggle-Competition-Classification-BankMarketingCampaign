{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle / Sébastien PAVOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/c/mbd2019-in-class-competition-bank-mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'data.table' was built under R version 3.6.1\"Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'caret' was built under R version 3.6.2\"Loading required package: lattice\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "Warning message:\n",
      "\"package 'e1071' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.6.2\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following objects are masked from 'package:data.table':\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Warning message:\n",
      "\"package 'varhandle' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'tidyverse' was built under R version 3.6.1\"-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v tibble  2.1.3     v purrr   0.3.3\n",
      "v tidyr   1.0.2     v stringr 1.4.0\n",
      "v readr   1.3.1     v forcats 0.4.0\n",
      "Warning message:\n",
      "\"package 'tibble' was built under R version 3.6.1\"Warning message:\n",
      "\"package 'tidyr' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'readr' was built under R version 3.6.1\"Warning message:\n",
      "\"package 'purrr' was built under R version 3.6.1\"Warning message:\n",
      "\"package 'stringr' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'forcats' was built under R version 3.6.1\"-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::between()       masks data.table::between()\n",
      "x dplyr::combine()       masks randomForest::combine()\n",
      "x dplyr::filter()        masks stats::filter()\n",
      "x dplyr::first()         masks data.table::first()\n",
      "x dplyr::lag()           masks stats::lag()\n",
      "x dplyr::last()          masks data.table::last()\n",
      "x purrr::lift()          masks caret::lift()\n",
      "x randomForest::margin() masks ggplot2::margin()\n",
      "x purrr::transpose()     masks data.table::transpose()\n",
      "Warning message:\n",
      "\"package 'ISLR' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'leaps' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'Boruta' was built under R version 3.6.3\"Loading required package: ranger\n",
      "Warning message:\n",
      "\"package 'ranger' was built under R version 3.6.3\"\n",
      "Attaching package: 'ranger'\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    importance\n",
      "\n",
      "Warning message:\n",
      "\"package 'mlr' was built under R version 3.6.3\"Loading required package: ParamHelpers\n",
      "Warning message:\n",
      "\"package 'ParamHelpers' was built under R version 3.6.2\"'mlr' is in maintenance mode since July 2019. Future development\n",
      "efforts will go into its successor 'mlr3' (<https://mlr3.mlr-org.com>).\n",
      "\n",
      "Attaching package: 'mlr'\n",
      "\n",
      "The following object is masked from 'package:e1071':\n",
      "\n",
      "    impute\n",
      "\n",
      "The following object is masked from 'package:caret':\n",
      "\n",
      "    train\n",
      "\n",
      "Warning message:\n",
      "\"package 'kernlab' was built under R version 3.6.1\"\n",
      "Attaching package: 'kernlab'\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    cross\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    alpha\n",
      "\n",
      "\n",
      "Attaching package: 'boot'\n",
      "\n",
      "The following object is masked from 'package:lattice':\n",
      "\n",
      "    melanoma\n",
      "\n",
      "Warning message:\n",
      "\"package 'DiscriMiner' was built under R version 3.6.3\"Registered S3 method overwritten by 'DiscriMiner':\n",
      "  method      from \n",
      "  print.plsda caret\n",
      "Warning message:\n",
      "\"package 'earth' was built under R version 3.6.3\"Loading required package: Formula\n",
      "Loading required package: plotmo\n",
      "Warning message:\n",
      "\"package 'plotmo' was built under R version 3.6.3\"Loading required package: plotrix\n",
      "Warning message:\n",
      "\"package 'plotrix' was built under R version 3.6.1\"Loading required package: TeachingDemos\n",
      "Warning message:\n",
      "\"package 'TeachingDemos' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'adabag' was built under R version 3.6.3\"Loading required package: rpart\n",
      "Loading required package: foreach\n",
      "\n",
      "Attaching package: 'foreach'\n",
      "\n",
      "The following objects are masked from 'package:purrr':\n",
      "\n",
      "    accumulate, when\n",
      "\n",
      "Loading required package: doParallel\n",
      "Warning message:\n",
      "\"package 'doParallel' was built under R version 3.6.3\"Loading required package: iterators\n",
      "Loading required package: parallel\n",
      "Warning message:\n",
      "\"package 'party' was built under R version 3.6.3\"Loading required package: grid\n",
      "Loading required package: mvtnorm\n",
      "Warning message:\n",
      "\"package 'mvtnorm' was built under R version 3.6.2\"Loading required package: modeltools\n",
      "Loading required package: stats4\n",
      "\n",
      "Attaching package: 'modeltools'\n",
      "\n",
      "The following object is masked from 'package:kernlab':\n",
      "\n",
      "    prior\n",
      "\n",
      "Loading required package: strucchange\n",
      "Warning message:\n",
      "\"package 'strucchange' was built under R version 3.6.3\"Loading required package: zoo\n",
      "Warning message:\n",
      "\"package 'zoo' was built under R version 3.6.2\"\n",
      "Attaching package: 'zoo'\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "Loading required package: sandwich\n",
      "Warning message:\n",
      "\"package 'sandwich' was built under R version 3.6.3\"\n",
      "Attaching package: 'strucchange'\n",
      "\n",
      "The following object is masked from 'package:stringr':\n",
      "\n",
      "    boundary\n",
      "\n",
      "Warning message:\n",
      "\"package 'C50' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'klaR' was built under R version 3.6.3\"Loading required package: MASS\n",
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    select\n",
      "\n",
      "\n",
      "Attaching package: 'klaR'\n",
      "\n",
      "The following object is masked from 'package:TeachingDemos':\n",
      "\n",
      "    triplot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(data.table)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(e1071) \n",
    "library(dplyr)\n",
    "library(woeBinning)\n",
    "library(varhandle)\n",
    "library(tidyverse)\n",
    "library(ISLR)\n",
    "library(leaps)\n",
    "library(Boruta)\n",
    "library(mlr)\n",
    "library(kernlab)\n",
    "library(boot)\n",
    "library(DiscriMiner)\n",
    "library(earth)\n",
    "library(adabag)\n",
    "library(party)\n",
    "library(C50)\n",
    "library(klaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet = read.csv(paste(data,\"bank_mkt_train.csv\", sep =\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSet = read.csv(paste(data,\"bank_mkt_test.csv\", sep =\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing, engineering and transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"client_id\"\n",
      "[1] 0\n",
      "[1] \"age\"\n",
      "[1] 0\n",
      "[1] \"job\"\n",
      "[1] 0\n",
      "[1] \"marital\"\n",
      "[1] 0\n",
      "[1] \"education\"\n",
      "[1] 0\n",
      "[1] \"default\"\n",
      "[1] 0\n",
      "[1] \"housing\"\n",
      "[1] 0\n",
      "[1] \"loan\"\n",
      "[1] 0\n",
      "[1] \"contact\"\n",
      "[1] 0\n",
      "[1] \"month\"\n",
      "[1] 0\n",
      "[1] \"day_of_week\"\n",
      "[1] 0\n",
      "[1] \"campaign\"\n",
      "[1] 0\n",
      "[1] \"pdays\"\n",
      "[1] 0\n",
      "[1] \"previous\"\n",
      "[1] 0\n",
      "[1] \"poutcome\"\n",
      "[1] 0\n",
      "[1] \"emp.var.rate\"\n",
      "[1] 0\n",
      "[1] \"cons.price.idx\"\n",
      "[1] 0\n",
      "[1] \"cons.conf.idx\"\n",
      "[1] 0\n",
      "[1] \"euribor3m\"\n",
      "[1] 0\n",
      "[1] \"nr.employed\"\n",
      "[1] 0\n",
      "[1] \"subscribe\"\n",
      "[1] 0\n"
     ]
    }
   ],
   "source": [
    "for (i in names(DataSet)){\n",
    "    print(i)\n",
    "    print(sum(is.na(DataSet[[i]])))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in any column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the structure of the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t7000 obs. of  21 variables:\n",
      " $ client_id     : int  2 3 4 5 6 7 8 9 14 15 ...\n",
      " $ age           : int  29 39 49 32 29 51 34 52 52 29 ...\n",
      " $ job           : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 4 11 2 7 1 7 2 8 1 1 ...\n",
      " $ marital       : Factor w/ 4 levels \"divorced\",\"married\",..: 3 2 2 3 3 2 2 2 2 3 ...\n",
      " $ education     : Factor w/ 8 levels \"basic.4y\",\"basic.6y\",..: 4 3 2 7 4 7 1 4 7 7 ...\n",
      " $ default       : Factor w/ 2 levels \"no\",\"unknown\": 1 2 2 1 2 2 1 1 1 1 ...\n",
      " $ housing       : Factor w/ 3 levels \"no\",\"unknown\",..: 1 3 1 3 3 3 3 3 3 3 ...\n",
      " $ loan          : Factor w/ 3 levels \"no\",\"unknown\",..: 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ contact       : Factor w/ 2 levels \"cellular\",\"telephone\": 2 2 1 1 1 2 1 1 1 1 ...\n",
      " $ month         : Factor w/ 10 levels \"apr\",\"aug\",\"dec\",..: 7 5 8 7 4 5 8 8 8 5 ...\n",
      " $ day_of_week   : Factor w/ 5 levels \"fri\",\"mon\",\"thu\",..: 2 1 4 2 1 4 4 4 3 2 ...\n",
      " $ campaign      : int  3 6 2 3 2 1 1 1 3 1 ...\n",
      " $ pdays         : int  999 999 999 999 999 999 999 999 999 999 ...\n",
      " $ previous      : int  0 0 0 1 0 0 0 0 0 0 ...\n",
      " $ poutcome      : Factor w/ 3 levels \"failure\",\"nonexistent\",..: 2 2 2 1 2 2 2 2 2 2 ...\n",
      " $ emp.var.rate  : num  1.1 1.4 -0.1 -1.8 1.4 1.4 -0.1 -0.1 -0.1 -2.9 ...\n",
      " $ cons.price.idx: num  94 94.5 93.2 92.9 93.9 ...\n",
      " $ cons.conf.idx : num  -36.4 -41.8 -42 -46.2 -42.7 -41.8 -42 -42 -42 -40.8 ...\n",
      " $ euribor3m     : num  4.86 4.96 4.15 1.3 4.96 ...\n",
      " $ nr.employed   : num  5191 5228 5196 5099 5228 ...\n",
      " $ subscribe     : int  0 0 0 0 0 0 0 0 0 0 ...\n"
     ]
    }
   ],
   "source": [
    "str(DataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we don't need to change any data type for now, they look already in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the distribution of the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1 \n",
       "6178  822 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$subscribe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.882571428571429"
      ],
      "text/latex": [
       "0.882571428571429"
      ],
      "text/markdown": [
       "0.882571428571429"
      ],
      "text/plain": [
       "[1] 0.8825714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(6178 / (6178 +822))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 88% of the customer didn't subscribe so we will use the AUC instead of accuracy as our metrics since predictiing all people as non suscriber would already lead to 88% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transform some variables in more meaningful variables by processing with feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>0%</dt><dd>18</dd><dt>20%</dt><dd>31</dd><dt>40%</dt><dd>36</dd><dt>60%</dt><dd>42</dd><dt>80%</dt><dd>50</dd><dt>100%</dt><dd>98</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[0\\textbackslash{}\\%] 18\n",
       "\\item[20\\textbackslash{}\\%] 31\n",
       "\\item[40\\textbackslash{}\\%] 36\n",
       "\\item[60\\textbackslash{}\\%] 42\n",
       "\\item[80\\textbackslash{}\\%] 50\n",
       "\\item[100\\textbackslash{}\\%] 98\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "0%\n",
       ":   1820%\n",
       ":   3140%\n",
       ":   3660%\n",
       ":   4280%\n",
       ":   50100%\n",
       ":   98\n",
       "\n"
      ],
      "text/plain": [
       "  0%  20%  40%  60%  80% 100% \n",
       "  18   31   36   42   50   98 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantile(DataSet$age, probs = seq(0,1, 0.20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nJECAkAABQgIECAkQICRAgJAAAUICBP4BOeP+xEm8yY4AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(quantile(DataSet$age, probs = seq(0,1, 0.20)), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the plot, we can define a new variables based on age and using the quantile to group the age in 5 categories. First group will be 0 - 31yo, second 31-36yo, third 36-42yo, fourth 42-50yo, fifth 50 - max(age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a function to easily apply this to the dataset & the predict set:\n",
    "agegrouped = function(data){\n",
    "    data$age_supervised_bin = \"1stQuantile\"\n",
    "    data[data$age >= 31 & data$age <= 36,]$age_supervised_bin = \"2ndQuantile\"\n",
    "    data[data$age > 36 & data$age <= 42,]$age_supervised_bin = \"3rdQuantile\"\n",
    "    data[data$age > 42 & data$age <= 50,]$age_supervised_bin = \"4thQuantile\"\n",
    "    data[data$age > 50,]$age_supervised_bin = \"5thQuantile\"\n",
    "    data$age_supervised_bin = as.factor(data$age_supervised_bin)\n",
    "}\n",
    "\n",
    "DataSet$age_supervised_bin = NA\n",
    "DataSet$age_supervised_bin = agegrouped(DataSet)\n",
    "\n",
    "PredictSet$age_supervised_bin = NA\n",
    "PredictSet$age_supervised_bin = agegrouped(PredictSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pdays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n",
       "   2    2   14   77   13    6   79    7    1   10   13    4    7   10    1    4 \n",
       "  16   17   18   19   21   22  999 \n",
       "   2    1    2    2    1    1 6741 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$pdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that pdays is a numeric variable with value between 0 to 22 but we have a lot of value as 999 which we can think it's a value to identify a specific type of pdays. Thus, we have to create a new variable to identify when the value are 999 because else the alogorithm will think that 999 is a value which is way higher than the other one and the difference between 0 to 22 are going to be reduce a lot and will not be meaningful anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet[, 'pdays_Specific'] = as.integer(DataSet$pdays == 999)\n",
    "PredictSet[, 'pdays_Specific'] = as.integer(PredictSet$pdays == 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1 \n",
       " 259 6741 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$pdays_Specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n",
       "2954 1834  920  447  268  156  124   68   56   32   25   20   20   16    6    9 \n",
       "  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n",
       "  11    7    4    3    1    1    3    3    3    2    1    1    1    1    1    1 \n",
       "  33 \n",
       "   1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$campaign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.815428571428571"
      ],
      "text/latex": [
       "0.815428571428571"
      ],
      "text/markdown": [
       "0.815428571428571"
      ],
      "text/plain": [
       "[1] 0.8154286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(sum(DataSet$campaign <= 3)) / length(DataSet$campaign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the range of values of campaign go from 1 to 33, we can see that the 3 first values cover 81% of all the different values, in this case, we can group the data by a categorical variable with low if it's 3 or bellow and high for the remaining vaues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet$campaign_supervised_bin = \"LOW\"\n",
    "DataSet[DataSet$campaign > 3,]$campaign_supervised_bin <- \"HIGH\"\n",
    "DataSet$campaign_supervised_bin = as.factor(DataSet$campaign_supervised_bin)\n",
    "\n",
    "PredictSet$campaign_supervised_bin = \"LOW\"\n",
    "PredictSet[PredictSet$campaign > 3,]$campaign_supervised_bin = \"HIGH\"\n",
    "PredictSet$campaign_supervised_bin = as.factor(PredictSet$campaign_supervised_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1    2    3    4    5    6 \n",
       "6021  799  126   40    9    4    1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this variable signify number of time the client was already contacted, we can group into 3 different categories. The first one is never conctacted, which is equal to 0, the second is contacted one time and the third is contacted at least 2 times. It will help to reduce from 7 to only 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a function to easily apply this to the dataset & the predict set:\n",
    "previous = function(data){\n",
    "    data$previous_supervised_bin <- \"Never\"\n",
    "    data[data$previous == 1,]$previous_supervised_bin = \"OneTime\"\n",
    "    data[data$previous > 1,]$previous_supervised_bin = \"TwoOrMore\"\n",
    "}\n",
    "\n",
    "DataSet$previous_supervised_bin = NA\n",
    "DataSet$previous_supervised_bin = previous(DataSet)\n",
    "DataSet$previous_supervised_bin = as.factor(DataSet$previous_supervised_bin)\n",
    "\n",
    "\n",
    "PredictSet$previous_supervised_bin = NA\n",
    "PredictSet$previous_supervised_bin = previous(PredictSet)\n",
    "PredictSet$previous_supervised_bin = as.factor(PredictSet$previous_supervised_bin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " apr  aug  dec  jul  jun  mar  may  nov  oct  sep \n",
       " 452 1036   31 1220  896   99 2323  689  143  111 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(DataSet$month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of adding a new variable summer as during those month people can be in holliday and so less conversions can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet[, 'month_summer'] = as.integer(DataSet$month %in% c('jul', 'aug'))\n",
    "# Test (holdout)\n",
    "PredictSet[, 'month_summer'] = as.integer(PredictSet$month %in% c('jul', 'aug'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bin the categories of job regarding if they suscribe or not to decrease the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>'job'</li>\n",
       "\t<li><table>\n",
       "<caption>A data.frame: 12 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Group.2</th><th scope=col>Group.1</th><th scope=col>woe</th><th scope=col>iv.total.final</th><th scope=col>1</th><th scope=col>0</th><th scope=col>col.perc.a</th><th scope=col>col.perc.b</th><th scope=col>iv.bins</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>7</th><td>blue-collar + services                             </td><td>blue-collar  </td><td>-46.7406855</td><td>0.1562879</td><td>170</td><td>2039</td><td>0.2068127</td><td>0.3300421</td><td>5.759828e-02</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>blue-collar + services                             </td><td>services     </td><td>-46.7406855</td><td>0.1562879</td><td>170</td><td>2039</td><td>0.2068127</td><td>0.3300421</td><td>5.759828e-02</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>admin. + management + technician + misc. level neg.</td><td>admin.       </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>admin. + management + technician + misc. level neg.</td><td>entrepreneur </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>admin. + management + technician + misc. level neg.</td><td>housemaid    </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>admin. + management + technician + misc. level neg.</td><td>management   </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>admin. + management + technician + misc. level neg.</td><td>technician   </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>admin. + management + technician + misc. level neg.</td><td>unknown      </td><td> -0.9402155</td><td>0.1562879</td><td>455</td><td>3452</td><td>0.5535280</td><td>0.5587569</td><td>4.916292e-05</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>misc. level pos.                                   </td><td>retired      </td><td> 76.7878915</td><td>0.1562879</td><td>197</td><td> 687</td><td>0.2396594</td><td>0.1112010</td><td>9.864044e-02</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>misc. level pos.                                   </td><td>self-employed</td><td> 76.7878915</td><td>0.1562879</td><td>197</td><td> 687</td><td>0.2396594</td><td>0.1112010</td><td>9.864044e-02</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>misc. level pos.                                   </td><td>student      </td><td> 76.7878915</td><td>0.1562879</td><td>197</td><td> 687</td><td>0.2396594</td><td>0.1112010</td><td>9.864044e-02</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>misc. level pos.                                   </td><td>unemployed   </td><td> 76.7878915</td><td>0.1562879</td><td>197</td><td> 687</td><td>0.2396594</td><td>0.1112010</td><td>9.864044e-02</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><strong>iv.total.final:</strong> 0.156287888710747</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item 'job'\n",
       "\\item A data.frame: 12 × 9\n",
       "\\begin{tabular}{r|lllllllll}\n",
       "  & Group.2 & Group.1 & woe & iv.total.final & 1 & 0 & col.perc.a & col.perc.b & iv.bins\\\\\n",
       "  & <fct> & <fct> & <dbl> & <dbl> & <int> & <int> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t7 & blue-collar + services                              & blue-collar   & -46.7406855 & 0.1562879 & 170 & 2039 & 0.2068127 & 0.3300421 & 5.759828e-02\\\\\n",
       "\t8 & blue-collar + services                              & services      & -46.7406855 & 0.1562879 & 170 & 2039 & 0.2068127 & 0.3300421 & 5.759828e-02\\\\\n",
       "\t1 & admin. + management + technician + misc. level neg. & admin.        &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t2 & admin. + management + technician + misc. level neg. & entrepreneur  &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t3 & admin. + management + technician + misc. level neg. & housemaid     &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t4 & admin. + management + technician + misc. level neg. & management    &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t5 & admin. + management + technician + misc. level neg. & technician    &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t6 & admin. + management + technician + misc. level neg. & unknown       &  -0.9402155 & 0.1562879 & 455 & 3452 & 0.5535280 & 0.5587569 & 4.916292e-05\\\\\n",
       "\t9 & misc. level pos.                                    & retired       &  76.7878915 & 0.1562879 & 197 &  687 & 0.2396594 & 0.1112010 & 9.864044e-02\\\\\n",
       "\t10 & misc. level pos.                                    & self-employed &  76.7878915 & 0.1562879 & 197 &  687 & 0.2396594 & 0.1112010 & 9.864044e-02\\\\\n",
       "\t11 & misc. level pos.                                    & student       &  76.7878915 & 0.1562879 & 197 &  687 & 0.2396594 & 0.1112010 & 9.864044e-02\\\\\n",
       "\t12 & misc. level pos.                                    & unemployed    &  76.7878915 & 0.1562879 & 197 &  687 & 0.2396594 & 0.1112010 & 9.864044e-02\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\textbf{iv.total.final:} 0.156287888710747\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 'job'\n",
       "2. \n",
       "A data.frame: 12 × 9\n",
       "\n",
       "| <!--/--> | Group.2 &lt;fct&gt; | Group.1 &lt;fct&gt; | woe &lt;dbl&gt; | iv.total.final &lt;dbl&gt; | 1 &lt;int&gt; | 0 &lt;int&gt; | col.perc.a &lt;dbl&gt; | col.perc.b &lt;dbl&gt; | iv.bins &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 7 | blue-collar + services                              | blue-collar   | -46.7406855 | 0.1562879 | 170 | 2039 | 0.2068127 | 0.3300421 | 5.759828e-02 |\n",
       "| 8 | blue-collar + services                              | services      | -46.7406855 | 0.1562879 | 170 | 2039 | 0.2068127 | 0.3300421 | 5.759828e-02 |\n",
       "| 1 | admin. + management + technician + misc. level neg. | admin.        |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 2 | admin. + management + technician + misc. level neg. | entrepreneur  |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 3 | admin. + management + technician + misc. level neg. | housemaid     |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 4 | admin. + management + technician + misc. level neg. | management    |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 5 | admin. + management + technician + misc. level neg. | technician    |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 6 | admin. + management + technician + misc. level neg. | unknown       |  -0.9402155 | 0.1562879 | 455 | 3452 | 0.5535280 | 0.5587569 | 4.916292e-05 |\n",
       "| 9 | misc. level pos.                                    | retired       |  76.7878915 | 0.1562879 | 197 |  687 | 0.2396594 | 0.1112010 | 9.864044e-02 |\n",
       "| 10 | misc. level pos.                                    | self-employed |  76.7878915 | 0.1562879 | 197 |  687 | 0.2396594 | 0.1112010 | 9.864044e-02 |\n",
       "| 11 | misc. level pos.                                    | student       |  76.7878915 | 0.1562879 | 197 |  687 | 0.2396594 | 0.1112010 | 9.864044e-02 |\n",
       "| 12 | misc. level pos.                                    | unemployed    |  76.7878915 | 0.1562879 | 197 |  687 | 0.2396594 | 0.1112010 | 9.864044e-02 |\n",
       "\n",
       "\n",
       "3. **iv.total.final:** 0.156287888710747\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] \"job\"\n",
       "\n",
       "[[2]]\n",
       "                                               Group.2       Group.1\n",
       "7                               blue-collar + services   blue-collar\n",
       "8                               blue-collar + services      services\n",
       "1  admin. + management + technician + misc. level neg.        admin.\n",
       "2  admin. + management + technician + misc. level neg.  entrepreneur\n",
       "3  admin. + management + technician + misc. level neg.     housemaid\n",
       "4  admin. + management + technician + misc. level neg.    management\n",
       "5  admin. + management + technician + misc. level neg.    technician\n",
       "6  admin. + management + technician + misc. level neg.       unknown\n",
       "9                                     misc. level pos.       retired\n",
       "10                                    misc. level pos. self-employed\n",
       "11                                    misc. level pos.       student\n",
       "12                                    misc. level pos.    unemployed\n",
       "           woe iv.total.final   1    0 col.perc.a col.perc.b      iv.bins\n",
       "7  -46.7406855      0.1562879 170 2039  0.2068127  0.3300421 5.759828e-02\n",
       "8  -46.7406855      0.1562879 170 2039  0.2068127  0.3300421 5.759828e-02\n",
       "1   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "2   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "3   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "4   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "5   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "6   -0.9402155      0.1562879 455 3452  0.5535280  0.5587569 4.916292e-05\n",
       "9   76.7878915      0.1562879 197  687  0.2396594  0.1112010 9.864044e-02\n",
       "10  76.7878915      0.1562879 197  687  0.2396594  0.1112010 9.864044e-02\n",
       "11  76.7878915      0.1562879 197  687  0.2396594  0.1112010 9.864044e-02\n",
       "12  76.7878915      0.1562879 197  687  0.2396594  0.1112010 9.864044e-02\n",
       "\n",
       "[[3]]\n",
       "iv.total.final \n",
       "     0.1562879 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouping the variable age onto 4 groups using WOE\n",
    "binning_cat <- woe.binning(DataSet, 'subscribe', 'job')\n",
    "binning_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>job</th><th scope=col>job.binned</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>housemaid    </td><td>admin. + management + technician + misc. level neg.</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>unemployed   </td><td>misc. level pos.                                   </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>blue-collar  </td><td>blue-collar + services                             </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>self-employed</td><td>misc. level pos.                                   </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>admin.       </td><td>admin. + management + technician + misc. level neg.</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>self-employed</td><td>misc. level pos.                                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & job & job.binned\\\\\n",
       "  & <fct> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & housemaid     & admin. + management + technician + misc. level neg.\\\\\n",
       "\t2 & unemployed    & misc. level pos.                                   \\\\\n",
       "\t3 & blue-collar   & blue-collar + services                             \\\\\n",
       "\t4 & self-employed & misc. level pos.                                   \\\\\n",
       "\t5 & admin.        & admin. + management + technician + misc. level neg.\\\\\n",
       "\t6 & self-employed & misc. level pos.                                   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | job &lt;fct&gt; | job.binned &lt;fct&gt; |\n",
       "|---|---|---|\n",
       "| 1 | housemaid     | admin. + management + technician + misc. level neg. |\n",
       "| 2 | unemployed    | misc. level pos.                                    |\n",
       "| 3 | blue-collar   | blue-collar + services                              |\n",
       "| 4 | self-employed | misc. level pos.                                    |\n",
       "| 5 | admin.        | admin. + management + technician + misc. level neg. |\n",
       "| 6 | self-employed | misc. level pos.                                    |\n",
       "\n"
      ],
      "text/plain": [
       "  job           job.binned                                         \n",
       "1 housemaid     admin. + management + technician + misc. level neg.\n",
       "2 unemployed    misc. level pos.                                   \n",
       "3 blue-collar   blue-collar + services                             \n",
       "4 self-employed misc. level pos.                                   \n",
       "5 admin.        admin. + management + technician + misc. level neg.\n",
       "6 self-employed misc. level pos.                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataSet <- woe.binning.deploy(DataSet, binning_cat)\n",
    "head(DataSet[, c('job', 'job.binned')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSet <- woe.binning.deploy(PredictSet, binning_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for education:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>'education'</li>\n",
       "\t<li><table>\n",
       "<caption>A data.frame: 8 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Group.2</th><th scope=col>Group.1</th><th scope=col>woe</th><th scope=col>iv.total.final</th><th scope=col>1</th><th scope=col>0</th><th scope=col>col.perc.a</th><th scope=col>col.perc.b</th><th scope=col>iv.bins</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>4</th><td>basic.9y + basic.6y                         </td><td>basic.6y           </td><td>-35.146021</td><td>0.04473058</td><td>116</td><td>1239</td><td>0.1411192</td><td>0.2005503</td><td>0.020887674</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>basic.9y + basic.6y                         </td><td>basic.9y           </td><td>-35.146021</td><td>0.04473058</td><td>116</td><td>1239</td><td>0.1411192</td><td>0.2005503</td><td>0.020887674</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>basic.4y + professional.course + high.school</td><td>basic.4y           </td><td> -6.869731</td><td>0.04473058</td><td>358</td><td>2882</td><td>0.4355231</td><td>0.4664940</td><td>0.002127617</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>basic.4y + professional.course + high.school</td><td>high.school        </td><td> -6.869731</td><td>0.04473058</td><td>358</td><td>2882</td><td>0.4355231</td><td>0.4664940</td><td>0.002127617</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>basic.4y + professional.course + high.school</td><td>professional.course</td><td> -6.869731</td><td>0.04473058</td><td>358</td><td>2882</td><td>0.4355231</td><td>0.4664940</td><td>0.002127617</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>university.degree + misc. level pos.        </td><td>illiterate         </td><td> 24.020807</td><td>0.04473058</td><td>348</td><td>2057</td><td>0.4233577</td><td>0.3329556</td><td>0.021715293</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>university.degree + misc. level pos.        </td><td>university.degree  </td><td> 24.020807</td><td>0.04473058</td><td>348</td><td>2057</td><td>0.4233577</td><td>0.3329556</td><td>0.021715293</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>university.degree + misc. level pos.        </td><td>unknown            </td><td> 24.020807</td><td>0.04473058</td><td>348</td><td>2057</td><td>0.4233577</td><td>0.3329556</td><td>0.021715293</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><strong>iv.total.final:</strong> 0.0447305843101804</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item 'education'\n",
       "\\item A data.frame: 8 × 9\n",
       "\\begin{tabular}{r|lllllllll}\n",
       "  & Group.2 & Group.1 & woe & iv.total.final & 1 & 0 & col.perc.a & col.perc.b & iv.bins\\\\\n",
       "  & <fct> & <fct> & <dbl> & <dbl> & <int> & <int> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t4 & basic.9y + basic.6y                          & basic.6y            & -35.146021 & 0.04473058 & 116 & 1239 & 0.1411192 & 0.2005503 & 0.020887674\\\\\n",
       "\t5 & basic.9y + basic.6y                          & basic.9y            & -35.146021 & 0.04473058 & 116 & 1239 & 0.1411192 & 0.2005503 & 0.020887674\\\\\n",
       "\t1 & basic.4y + professional.course + high.school & basic.4y            &  -6.869731 & 0.04473058 & 358 & 2882 & 0.4355231 & 0.4664940 & 0.002127617\\\\\n",
       "\t2 & basic.4y + professional.course + high.school & high.school         &  -6.869731 & 0.04473058 & 358 & 2882 & 0.4355231 & 0.4664940 & 0.002127617\\\\\n",
       "\t3 & basic.4y + professional.course + high.school & professional.course &  -6.869731 & 0.04473058 & 358 & 2882 & 0.4355231 & 0.4664940 & 0.002127617\\\\\n",
       "\t6 & university.degree + misc. level pos.         & illiterate          &  24.020807 & 0.04473058 & 348 & 2057 & 0.4233577 & 0.3329556 & 0.021715293\\\\\n",
       "\t7 & university.degree + misc. level pos.         & university.degree   &  24.020807 & 0.04473058 & 348 & 2057 & 0.4233577 & 0.3329556 & 0.021715293\\\\\n",
       "\t8 & university.degree + misc. level pos.         & unknown             &  24.020807 & 0.04473058 & 348 & 2057 & 0.4233577 & 0.3329556 & 0.021715293\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\textbf{iv.total.final:} 0.0447305843101804\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 'education'\n",
       "2. \n",
       "A data.frame: 8 × 9\n",
       "\n",
       "| <!--/--> | Group.2 &lt;fct&gt; | Group.1 &lt;fct&gt; | woe &lt;dbl&gt; | iv.total.final &lt;dbl&gt; | 1 &lt;int&gt; | 0 &lt;int&gt; | col.perc.a &lt;dbl&gt; | col.perc.b &lt;dbl&gt; | iv.bins &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 4 | basic.9y + basic.6y                          | basic.6y            | -35.146021 | 0.04473058 | 116 | 1239 | 0.1411192 | 0.2005503 | 0.020887674 |\n",
       "| 5 | basic.9y + basic.6y                          | basic.9y            | -35.146021 | 0.04473058 | 116 | 1239 | 0.1411192 | 0.2005503 | 0.020887674 |\n",
       "| 1 | basic.4y + professional.course + high.school | basic.4y            |  -6.869731 | 0.04473058 | 358 | 2882 | 0.4355231 | 0.4664940 | 0.002127617 |\n",
       "| 2 | basic.4y + professional.course + high.school | high.school         |  -6.869731 | 0.04473058 | 358 | 2882 | 0.4355231 | 0.4664940 | 0.002127617 |\n",
       "| 3 | basic.4y + professional.course + high.school | professional.course |  -6.869731 | 0.04473058 | 358 | 2882 | 0.4355231 | 0.4664940 | 0.002127617 |\n",
       "| 6 | university.degree + misc. level pos.         | illiterate          |  24.020807 | 0.04473058 | 348 | 2057 | 0.4233577 | 0.3329556 | 0.021715293 |\n",
       "| 7 | university.degree + misc. level pos.         | university.degree   |  24.020807 | 0.04473058 | 348 | 2057 | 0.4233577 | 0.3329556 | 0.021715293 |\n",
       "| 8 | university.degree + misc. level pos.         | unknown             |  24.020807 | 0.04473058 | 348 | 2057 | 0.4233577 | 0.3329556 | 0.021715293 |\n",
       "\n",
       "\n",
       "3. **iv.total.final:** 0.0447305843101804\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] \"education\"\n",
       "\n",
       "[[2]]\n",
       "                                       Group.2             Group.1        woe\n",
       "4                          basic.9y + basic.6y            basic.6y -35.146021\n",
       "5                          basic.9y + basic.6y            basic.9y -35.146021\n",
       "1 basic.4y + professional.course + high.school            basic.4y  -6.869731\n",
       "2 basic.4y + professional.course + high.school         high.school  -6.869731\n",
       "3 basic.4y + professional.course + high.school professional.course  -6.869731\n",
       "6         university.degree + misc. level pos.          illiterate  24.020807\n",
       "7         university.degree + misc. level pos.   university.degree  24.020807\n",
       "8         university.degree + misc. level pos.             unknown  24.020807\n",
       "  iv.total.final   1    0 col.perc.a col.perc.b     iv.bins\n",
       "4     0.04473058 116 1239  0.1411192  0.2005503 0.020887674\n",
       "5     0.04473058 116 1239  0.1411192  0.2005503 0.020887674\n",
       "1     0.04473058 358 2882  0.4355231  0.4664940 0.002127617\n",
       "2     0.04473058 358 2882  0.4355231  0.4664940 0.002127617\n",
       "3     0.04473058 358 2882  0.4355231  0.4664940 0.002127617\n",
       "6     0.04473058 348 2057  0.4233577  0.3329556 0.021715293\n",
       "7     0.04473058 348 2057  0.4233577  0.3329556 0.021715293\n",
       "8     0.04473058 348 2057  0.4233577  0.3329556 0.021715293\n",
       "\n",
       "[[3]]\n",
       "iv.total.final \n",
       "    0.04473058 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binning_cat <- woe.binning(DataSet, 'subscribe', 'education')\n",
    "binning_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>education</th><th scope=col>education.binned</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>high.school      </td><td>basic.4y + professional.course + high.school</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>basic.9y         </td><td>basic.9y + basic.6y                         </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>basic.6y         </td><td>basic.9y + basic.6y                         </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>university.degree</td><td>university.degree + misc. level pos.        </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>high.school      </td><td>basic.4y + professional.course + high.school</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>university.degree</td><td>university.degree + misc. level pos.        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & education & education.binned\\\\\n",
       "  & <fct> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & high.school       & basic.4y + professional.course + high.school\\\\\n",
       "\t2 & basic.9y          & basic.9y + basic.6y                         \\\\\n",
       "\t3 & basic.6y          & basic.9y + basic.6y                         \\\\\n",
       "\t4 & university.degree & university.degree + misc. level pos.        \\\\\n",
       "\t5 & high.school       & basic.4y + professional.course + high.school\\\\\n",
       "\t6 & university.degree & university.degree + misc. level pos.        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | education &lt;fct&gt; | education.binned &lt;fct&gt; |\n",
       "|---|---|---|\n",
       "| 1 | high.school       | basic.4y + professional.course + high.school |\n",
       "| 2 | basic.9y          | basic.9y + basic.6y                          |\n",
       "| 3 | basic.6y          | basic.9y + basic.6y                          |\n",
       "| 4 | university.degree | university.degree + misc. level pos.         |\n",
       "| 5 | high.school       | basic.4y + professional.course + high.school |\n",
       "| 6 | university.degree | university.degree + misc. level pos.         |\n",
       "\n"
      ],
      "text/plain": [
       "  education         education.binned                            \n",
       "1 high.school       basic.4y + professional.course + high.school\n",
       "2 basic.9y          basic.9y + basic.6y                         \n",
       "3 basic.6y          basic.9y + basic.6y                         \n",
       "4 university.degree university.degree + misc. level pos.        \n",
       "5 high.school       basic.4y + professional.course + high.school\n",
       "6 university.degree university.degree + misc. level pos.        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataSet <- woe.binning.deploy(DataSet, binning_cat)\n",
    "head(DataSet[, c('education', 'education.binned')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSet <- woe.binning.deploy(PredictSet, binning_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy encode the categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 66</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>client_id</th><th scope=col>age</th><th scope=col>campaign</th><th scope=col>pdays</th><th scope=col>previous</th><th scope=col>emp.var.rate</th><th scope=col>cons.price.idx</th><th scope=col>cons.conf.idx</th><th scope=col>euribor3m</th><th scope=col>nr.employed</th><th scope=col>...</th><th scope=col>age_supervised_bin.2ndQuantile</th><th scope=col>age_supervised_bin.3rdQuantile</th><th scope=col>age_supervised_bin.4thQuantile</th><th scope=col>campaign_supervised_bin.HIGH</th><th scope=col>job.binned.admin._+_management_+_technician_+_misc._level_neg.</th><th scope=col>job.binned.blue-collar_+_services</th><th scope=col>job.binned.misc._level_pos.</th><th scope=col>education.binned.basic.4y_+_professional.course_+_high.school</th><th scope=col>education.binned.basic.9y_+_basic.6y</th><th scope=col>education.binned.university.degree_+_misc._level_pos.</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2</td><td>29</td><td>3</td><td>999</td><td>0</td><td> 1.1</td><td>93.994</td><td>-36.4</td><td>4.858</td><td>5191.0</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>3</td><td>39</td><td>6</td><td>999</td><td>0</td><td> 1.4</td><td>94.465</td><td>-41.8</td><td>4.959</td><td>5228.1</td><td>...</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4</td><td>49</td><td>2</td><td>999</td><td>0</td><td>-0.1</td><td>93.200</td><td>-42.0</td><td>4.153</td><td>5195.8</td><td>...</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>5</td><td>32</td><td>3</td><td>999</td><td>1</td><td>-1.8</td><td>92.893</td><td>-46.2</td><td>1.299</td><td>5099.1</td><td>...</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>6</td><td>29</td><td>2</td><td>999</td><td>0</td><td> 1.4</td><td>93.918</td><td>-42.7</td><td>4.963</td><td>5228.1</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>7</td><td>51</td><td>1</td><td>999</td><td>0</td><td> 1.4</td><td>94.465</td><td>-41.8</td><td>4.864</td><td>5228.1</td><td>...</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 66\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & client\\_id & age & campaign & pdays & previous & emp.var.rate & cons.price.idx & cons.conf.idx & euribor3m & nr.employed & ... & age\\_supervised\\_bin.2ndQuantile & age\\_supervised\\_bin.3rdQuantile & age\\_supervised\\_bin.4thQuantile & campaign\\_supervised\\_bin.HIGH & job.binned.admin.\\_+\\_management\\_+\\_technician\\_+\\_misc.\\_level\\_neg. & job.binned.blue-collar\\_+\\_services & job.binned.misc.\\_level\\_pos. & education.binned.basic.4y\\_+\\_professional.course\\_+\\_high.school & education.binned.basic.9y\\_+\\_basic.6y & education.binned.university.degree\\_+\\_misc.\\_level\\_pos.\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 2 & 29 & 3 & 999 & 0 &  1.1 & 93.994 & -36.4 & 4.858 & 5191.0 & ... & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t2 & 3 & 39 & 6 & 999 & 0 &  1.4 & 94.465 & -41.8 & 4.959 & 5228.1 & ... & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 0\\\\\n",
       "\t3 & 4 & 49 & 2 & 999 & 0 & -0.1 & 93.200 & -42.0 & 4.153 & 5195.8 & ... & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\\\\\n",
       "\t4 & 5 & 32 & 3 & 999 & 1 & -1.8 & 92.893 & -46.2 & 1.299 & 5099.1 & ... & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
       "\t5 & 6 & 29 & 2 & 999 & 0 &  1.4 & 93.918 & -42.7 & 4.963 & 5228.1 & ... & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t6 & 7 & 51 & 1 & 999 & 0 &  1.4 & 94.465 & -41.8 & 4.864 & 5228.1 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 66\n",
       "\n",
       "| <!--/--> | client_id &lt;int&gt; | age &lt;int&gt; | campaign &lt;int&gt; | pdays &lt;int&gt; | previous &lt;int&gt; | emp.var.rate &lt;dbl&gt; | cons.price.idx &lt;dbl&gt; | cons.conf.idx &lt;dbl&gt; | euribor3m &lt;dbl&gt; | nr.employed &lt;dbl&gt; | ... ... | age_supervised_bin.2ndQuantile &lt;dbl&gt; | age_supervised_bin.3rdQuantile &lt;dbl&gt; | age_supervised_bin.4thQuantile &lt;dbl&gt; | campaign_supervised_bin.HIGH &lt;dbl&gt; | job.binned.admin._+_management_+_technician_+_misc._level_neg. &lt;dbl&gt; | job.binned.blue-collar_+_services &lt;dbl&gt; | job.binned.misc._level_pos. &lt;dbl&gt; | education.binned.basic.4y_+_professional.course_+_high.school &lt;dbl&gt; | education.binned.basic.9y_+_basic.6y &lt;dbl&gt; | education.binned.university.degree_+_misc._level_pos. &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2 | 29 | 3 | 999 | 0 |  1.1 | 93.994 | -36.4 | 4.858 | 5191.0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 |\n",
       "| 2 | 3 | 39 | 6 | 999 | 0 |  1.4 | 94.465 | -41.8 | 4.959 | 5228.1 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 |\n",
       "| 3 | 4 | 49 | 2 | 999 | 0 | -0.1 | 93.200 | -42.0 | 4.153 | 5195.8 | ... | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 |\n",
       "| 4 | 5 | 32 | 3 | 999 | 1 | -1.8 | 92.893 | -46.2 | 1.299 | 5099.1 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
       "| 5 | 6 | 29 | 2 | 999 | 0 |  1.4 | 93.918 | -42.7 | 4.963 | 5228.1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 |\n",
       "| 6 | 7 | 51 | 1 | 999 | 0 |  1.4 | 94.465 | -41.8 | 4.864 | 5228.1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  client_id age campaign pdays previous emp.var.rate cons.price.idx\n",
       "1 2         29  3        999   0         1.1         93.994        \n",
       "2 3         39  6        999   0         1.4         94.465        \n",
       "3 4         49  2        999   0        -0.1         93.200        \n",
       "4 5         32  3        999   1        -1.8         92.893        \n",
       "5 6         29  2        999   0         1.4         93.918        \n",
       "6 7         51  1        999   0         1.4         94.465        \n",
       "  cons.conf.idx euribor3m nr.employed ... age_supervised_bin.2ndQuantile\n",
       "1 -36.4         4.858     5191.0      ... 0                             \n",
       "2 -41.8         4.959     5228.1      ... 0                             \n",
       "3 -42.0         4.153     5195.8      ... 0                             \n",
       "4 -46.2         1.299     5099.1      ... 1                             \n",
       "5 -42.7         4.963     5228.1      ... 0                             \n",
       "6 -41.8         4.864     5228.1      ... 0                             \n",
       "  age_supervised_bin.3rdQuantile age_supervised_bin.4thQuantile\n",
       "1 0                              0                             \n",
       "2 1                              0                             \n",
       "3 0                              1                             \n",
       "4 0                              0                             \n",
       "5 0                              0                             \n",
       "6 0                              0                             \n",
       "  campaign_supervised_bin.HIGH\n",
       "1 0                           \n",
       "2 1                           \n",
       "3 0                           \n",
       "4 0                           \n",
       "5 0                           \n",
       "6 0                           \n",
       "  job.binned.admin._+_management_+_technician_+_misc._level_neg.\n",
       "1 1                                                             \n",
       "2 0                                                             \n",
       "3 0                                                             \n",
       "4 0                                                             \n",
       "5 1                                                             \n",
       "6 0                                                             \n",
       "  job.binned.blue-collar_+_services job.binned.misc._level_pos.\n",
       "1 0                                 0                          \n",
       "2 0                                 1                          \n",
       "3 1                                 0                          \n",
       "4 0                                 1                          \n",
       "5 0                                 0                          \n",
       "6 0                                 1                          \n",
       "  education.binned.basic.4y_+_professional.course_+_high.school\n",
       "1 1                                                            \n",
       "2 0                                                            \n",
       "3 0                                                            \n",
       "4 0                                                            \n",
       "5 1                                                            \n",
       "6 0                                                            \n",
       "  education.binned.basic.9y_+_basic.6y\n",
       "1 0                                   \n",
       "2 1                                   \n",
       "3 1                                   \n",
       "4 0                                   \n",
       "5 0                                   \n",
       "6 0                                   \n",
       "  education.binned.university.degree_+_misc._level_pos.\n",
       "1 0                                                    \n",
       "2 0                                                    \n",
       "3 0                                                    \n",
       "4 1                                                    \n",
       "5 0                                                    \n",
       "6 1                                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the IV and DV list name\n",
    "# Dependent variable (DV)\n",
    "dv_list <- c('subscribe')\n",
    "# Independent variable (IV)\n",
    "iv_list <- setdiff(colnames(DataSet), dv_list)  # Exclude the target variable\n",
    "iv_list <- setdiff(iv_list, 'client_id')  # Exclude the client_id\n",
    "\n",
    "# Pick out categorical and numerical variable\n",
    "iv_cat_list <- c()  # List to store categorical variable\n",
    "iv_num_list <- c()  # List to store numerical variable\n",
    "for (v in iv_list) {\n",
    "    if (class(DataSet[, v]) == 'factor') {  # Factor == categorical variable\n",
    "        iv_cat_list <- c(iv_cat_list, v)\n",
    "    } else {  # Non-factor == numerical variable\n",
    "        iv_num_list <- c(iv_num_list, v)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Join train and test (holdout) to process data\n",
    "PredictSet$subscribe <- NA  # Create a blank response column on test (holdout data)\n",
    "data <- rbind(DataSet, PredictSet)  # Combine train and test (holdout) data\n",
    "\n",
    "# Loop through all categorical variable\n",
    "# Drop the last column of dummy variable\n",
    "data_dummy_list <- list()\n",
    "for (v in iv_cat_list) {\n",
    "    tmp <- to.dummy(data[, v], v)\n",
    "    data_dummy_list[[length(data_dummy_list) + 1]] <- tmp[, -ncol(tmp), drop=FALSE]\n",
    "}\n",
    "data_dummy <- as.data.frame(do.call(cbind, data_dummy_list))\n",
    "\n",
    "# Drop categorical variables\n",
    "data_processed <- data[, setdiff(colnames(data), iv_cat_list)]\n",
    "\n",
    "# Add dummy vairables\n",
    "data_processed <- cbind(data_processed, data_dummy)\n",
    "\n",
    "head(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test (holdout) after dummitizing\n",
    "DataSetProc <- data_processed[!is.na(data_processed$subscribe), ]\n",
    "PredictSetProc <- data_processed[is.na(data_processed$subscribe), -ncol(data_processed)]\n",
    "PredictSetProc$subscribe = NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dummy encode all our categorical variable, we check the dimension of our new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>7000</li><li>66</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 7000\n",
       "\\item 66\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 7000\n",
       "2. 66\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 7000   66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(DataSetProc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3000</li><li>64</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3000\n",
       "\\item 64\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3000\n",
       "2. 64\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3000   64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(PredictSetProc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the only column which differs is susbscribe as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"subscribe\"\n",
      "[1] \"education.binned.university.degree_+_misc._level_pos.\"\n"
     ]
    }
   ],
   "source": [
    "'%ni%' <- Negate('%in%')\n",
    "\n",
    "for (i in names(DataSetProc)){\n",
    "    if (i %ni% names(PredictSetProc)){\n",
    "        print(i)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the variable that is in the DataSet but not in the PredictSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSetProc['education.binned.university.degree_+_misc._level_pos.'] = NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>7000</li><li>65</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 7000\n",
       "\\item 65\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 7000\n",
       "2. 65\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 7000   65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3000</li><li>64</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3000\n",
       "\\item 64\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3000\n",
       "2. 64\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3000   64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(DataSetProc)\n",
    "dim(PredictSetProc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Features selection and dimension reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform features selection, we're going to use the boruta method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will return us the variables that the method find relevant in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "boruta_output <- Boruta(subscribe ~ ., data=DataSetProc, doTrace=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'finalDecision'</li><li>'ImpHistory'</li><li>'pValue'</li><li>'maxRuns'</li><li>'light'</li><li>'mcAdj'</li><li>'timeTaken'</li><li>'roughfixed'</li><li>'call'</li><li>'impSource'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'finalDecision'\n",
       "\\item 'ImpHistory'\n",
       "\\item 'pValue'\n",
       "\\item 'maxRuns'\n",
       "\\item 'light'\n",
       "\\item 'mcAdj'\n",
       "\\item 'timeTaken'\n",
       "\\item 'roughfixed'\n",
       "\\item 'call'\n",
       "\\item 'impSource'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'finalDecision'\n",
       "2. 'ImpHistory'\n",
       "3. 'pValue'\n",
       "4. 'maxRuns'\n",
       "5. 'light'\n",
       "6. 'mcAdj'\n",
       "7. 'timeTaken'\n",
       "8. 'roughfixed'\n",
       "9. 'call'\n",
       "10. 'impSource'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"finalDecision\" \"ImpHistory\"    \"pValue\"        \"maxRuns\"      \n",
       " [5] \"light\"         \"mcAdj\"         \"timeTaken\"     \"roughfixed\"   \n",
       " [9] \"call\"          \"impSource\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(boruta_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, here's the list of all the variables that the function find meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some of them are classified as \"tentative\" which means the boruta method is not certain about their meaningfulness or not in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'age'</li><li>'pdays'</li><li>'previous'</li><li>'emp.var.rate'</li><li>'cons.price.idx'</li><li>'cons.conf.idx'</li><li>'euribor3m'</li><li>'nr.employed'</li><li>'pdays_Specific'</li><li>'month_summer'</li><li>'job.retired'</li><li>'job.student'</li><li>'marital.single'</li><li>'education.university.degree'</li><li>'default.no'</li><li>'contact.cellular'</li><li>'month.apr'</li><li>'month.aug'</li><li>'month.jun'</li><li>'month.mar'</li><li>'month.may'</li><li>'month.nov'</li><li>'month.oct'</li><li>'day_of_week.tue'</li><li>'poutcome.failure'</li><li>'poutcome.nonexistent'</li><li>'age_supervised_bin.1stQuantile'</li><li>'campaign_supervised_bin.HIGH'</li><li>'`job.binned.blue-collar_+_services`'</li><li>'job.binned.misc._level_pos.'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'age'\n",
       "\\item 'pdays'\n",
       "\\item 'previous'\n",
       "\\item 'emp.var.rate'\n",
       "\\item 'cons.price.idx'\n",
       "\\item 'cons.conf.idx'\n",
       "\\item 'euribor3m'\n",
       "\\item 'nr.employed'\n",
       "\\item 'pdays\\_Specific'\n",
       "\\item 'month\\_summer'\n",
       "\\item 'job.retired'\n",
       "\\item 'job.student'\n",
       "\\item 'marital.single'\n",
       "\\item 'education.university.degree'\n",
       "\\item 'default.no'\n",
       "\\item 'contact.cellular'\n",
       "\\item 'month.apr'\n",
       "\\item 'month.aug'\n",
       "\\item 'month.jun'\n",
       "\\item 'month.mar'\n",
       "\\item 'month.may'\n",
       "\\item 'month.nov'\n",
       "\\item 'month.oct'\n",
       "\\item 'day\\_of\\_week.tue'\n",
       "\\item 'poutcome.failure'\n",
       "\\item 'poutcome.nonexistent'\n",
       "\\item 'age\\_supervised\\_bin.1stQuantile'\n",
       "\\item 'campaign\\_supervised\\_bin.HIGH'\n",
       "\\item '`job.binned.blue-collar\\_+\\_services`'\n",
       "\\item 'job.binned.misc.\\_level\\_pos.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'age'\n",
       "2. 'pdays'\n",
       "3. 'previous'\n",
       "4. 'emp.var.rate'\n",
       "5. 'cons.price.idx'\n",
       "6. 'cons.conf.idx'\n",
       "7. 'euribor3m'\n",
       "8. 'nr.employed'\n",
       "9. 'pdays_Specific'\n",
       "10. 'month_summer'\n",
       "11. 'job.retired'\n",
       "12. 'job.student'\n",
       "13. 'marital.single'\n",
       "14. 'education.university.degree'\n",
       "15. 'default.no'\n",
       "16. 'contact.cellular'\n",
       "17. 'month.apr'\n",
       "18. 'month.aug'\n",
       "19. 'month.jun'\n",
       "20. 'month.mar'\n",
       "21. 'month.may'\n",
       "22. 'month.nov'\n",
       "23. 'month.oct'\n",
       "24. 'day_of_week.tue'\n",
       "25. 'poutcome.failure'\n",
       "26. 'poutcome.nonexistent'\n",
       "27. 'age_supervised_bin.1stQuantile'\n",
       "28. 'campaign_supervised_bin.HIGH'\n",
       "29. '`job.binned.blue-collar_+_services`'\n",
       "30. 'job.binned.misc._level_pos.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"age\"                                 \"pdays\"                              \n",
       " [3] \"previous\"                            \"emp.var.rate\"                       \n",
       " [5] \"cons.price.idx\"                      \"cons.conf.idx\"                      \n",
       " [7] \"euribor3m\"                           \"nr.employed\"                        \n",
       " [9] \"pdays_Specific\"                      \"month_summer\"                       \n",
       "[11] \"job.retired\"                         \"job.student\"                        \n",
       "[13] \"marital.single\"                      \"education.university.degree\"        \n",
       "[15] \"default.no\"                          \"contact.cellular\"                   \n",
       "[17] \"month.apr\"                           \"month.aug\"                          \n",
       "[19] \"month.jun\"                           \"month.mar\"                          \n",
       "[21] \"month.may\"                           \"month.nov\"                          \n",
       "[23] \"month.oct\"                           \"day_of_week.tue\"                    \n",
       "[25] \"poutcome.failure\"                    \"poutcome.nonexistent\"               \n",
       "[27] \"age_supervised_bin.1stQuantile\"      \"campaign_supervised_bin.HIGH\"       \n",
       "[29] \"`job.binned.blue-collar_+_services`\" \"job.binned.misc._level_pos.\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "30"
      ],
      "text/latex": [
       "30"
      ],
      "text/markdown": [
       "30"
      ],
      "text/plain": [
       "[1] 30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Confirmed = boruta_output$finalDecision == \"Confirmed\"\n",
    "Confirmed = boruta_output$finalDecision[Confirmed]\n",
    "names(Confirmed)\n",
    "length(Confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'campaign'</li><li>'marital.married'</li><li>'month.jul'</li><li>'day_of_week.mon'</li><li>'age_supervised_bin.2ndQuantile'</li><li>'`job.binned.admin._+_management_+_technician_+_misc._level_neg.`'</li><li>'`education.binned.basic.9y_+_basic.6y`'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'campaign'\n",
       "\\item 'marital.married'\n",
       "\\item 'month.jul'\n",
       "\\item 'day\\_of\\_week.mon'\n",
       "\\item 'age\\_supervised\\_bin.2ndQuantile'\n",
       "\\item '`job.binned.admin.\\_+\\_management\\_+\\_technician\\_+\\_misc.\\_level\\_neg.`'\n",
       "\\item '`education.binned.basic.9y\\_+\\_basic.6y`'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'campaign'\n",
       "2. 'marital.married'\n",
       "3. 'month.jul'\n",
       "4. 'day_of_week.mon'\n",
       "5. 'age_supervised_bin.2ndQuantile'\n",
       "6. '`job.binned.admin._+_management_+_technician_+_misc._level_neg.`'\n",
       "7. '`education.binned.basic.9y_+_basic.6y`'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"campaign\"                                                        \n",
       "[2] \"marital.married\"                                                 \n",
       "[3] \"month.jul\"                                                       \n",
       "[4] \"day_of_week.mon\"                                                 \n",
       "[5] \"age_supervised_bin.2ndQuantile\"                                  \n",
       "[6] \"`job.binned.admin._+_management_+_technician_+_misc._level_neg.`\"\n",
       "[7] \"`education.binned.basic.9y_+_basic.6y`\"                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "7"
      ],
      "text/latex": [
       "7"
      ],
      "text/markdown": [
       "7"
      ],
      "text/plain": [
       "[1] 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Notsure = boruta_output$finalDecision == \"Tentative\"\n",
    "Notsure = boruta_output$finalDecision[Notsure]\n",
    "names(Notsure)\n",
    "length(Notsure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Boruta, we can see that we have 30 variables that were selected and 7 that are not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let Boruta choose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method, boruta define by itself if the variable classified as \"tentative\" have to be kept or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"age\"                                                             \n",
      " [2] \"pdays\"                                                           \n",
      " [3] \"previous\"                                                        \n",
      " [4] \"emp.var.rate\"                                                    \n",
      " [5] \"cons.price.idx\"                                                  \n",
      " [6] \"cons.conf.idx\"                                                   \n",
      " [7] \"euribor3m\"                                                       \n",
      " [8] \"nr.employed\"                                                     \n",
      " [9] \"pdays_Specific\"                                                  \n",
      "[10] \"month_summer\"                                                    \n",
      "[11] \"job.retired\"                                                     \n",
      "[12] \"job.student\"                                                     \n",
      "[13] \"marital.single\"                                                  \n",
      "[14] \"education.university.degree\"                                     \n",
      "[15] \"default.no\"                                                      \n",
      "[16] \"contact.cellular\"                                                \n",
      "[17] \"month.apr\"                                                       \n",
      "[18] \"month.aug\"                                                       \n",
      "[19] \"month.jun\"                                                       \n",
      "[20] \"month.mar\"                                                       \n",
      "[21] \"month.may\"                                                       \n",
      "[22] \"month.nov\"                                                       \n",
      "[23] \"month.oct\"                                                       \n",
      "[24] \"day_of_week.mon\"                                                 \n",
      "[25] \"day_of_week.tue\"                                                 \n",
      "[26] \"poutcome.failure\"                                                \n",
      "[27] \"poutcome.nonexistent\"                                            \n",
      "[28] \"age_supervised_bin.1stQuantile\"                                  \n",
      "[29] \"age_supervised_bin.2ndQuantile\"                                  \n",
      "[30] \"campaign_supervised_bin.HIGH\"                                    \n",
      "[31] \"`job.binned.admin._+_management_+_technician_+_misc._level_neg.`\"\n",
      "[32] \"`job.binned.blue-collar_+_services`\"                             \n",
      "[33] \"job.binned.misc._level_pos.\"                                     \n"
     ]
    }
   ],
   "source": [
    "# Do a tentative rough fix\n",
    "roughFixMod <- TentativeRoughFix(boruta_output)\n",
    "boruta_signif <- getSelectedAttributes(roughFixMod)\n",
    "print(boruta_signif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that boruta excluded 4 variables and kept 3 others so we finish with 36 variables on the 65 at the begining which is a good reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables have a bad name due to the woe function used above, we rename them in the boruta output itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_signif['`job.binned.admin._+_management_+_technician_+_misc._level_neg.`'] = 'job.binned.admin._+_management_+_technician_+_misc._level_neg.'\n",
    "boruta_signif['`job.binned.blue-collar_+_services`'] = 'job.binned.blue-collar_+_services'\n",
    "boruta_signif['`education.binned.basic.9y_+_basic.6y`'] = 'education.binned.basic.9y_+_basic.6y'\n",
    "boruta_signif['`education.binned.university.degree_+_misc._level_pos.`'] = 'education.binned.university.degree_+_misc._level_pos.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the vector boruta_signif has clean name, we use this one to filter to only select the variables that the boruta method choosed previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>7000</li><li>36</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 7000\n",
       "\\item 36\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 7000\n",
       "2. 36\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 7000   36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DataClean = DataSetProc[,names(DataSetProc) %in% boruta_signif]\n",
    "DataClean$client_id = DataSetProc$client_id\n",
    "DataClean$subscribe = DataSetProc$subscribe\n",
    "dim(DataClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3000</li><li>35</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3000\n",
       "\\item 35\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3000\n",
       "2. 35\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3000   35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PredictClean = PredictSetProc[,names(PredictSetProc) %in% boruta_signif]\n",
    "PredictClean$client_id = PredictSetProc$client_id\n",
    "dim(PredictClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time, we check if only susbscribe is the different variable between DataSet and PredictSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"subscribe\"\n"
     ]
    }
   ],
   "source": [
    "for (i in names(DataClean)){\n",
    "    if (i %ni% names(PredictClean)){\n",
    "        print(i)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename some column into a more meaningful and easy name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataClean = DataClean %>% \n",
    "  rename(\n",
    "    'adminManaTechNeg' = 'job.binned.admin._+_management_+_technician_+_misc._level_neg.' ,\n",
    "    'BlueColServices' = 'job.binned.blue-collar_+_services',\n",
    "    'Education9Y6Y' = 'education.binned.basic.9y_+_basic.6y',\n",
    "    )\n",
    "\n",
    "PredictClean = PredictClean %>% \n",
    "  rename(\n",
    "    'adminManaTechNeg' = 'job.binned.admin._+_management_+_technician_+_misc._level_neg.' ,\n",
    "    'BlueColServices' = 'job.binned.blue-collar_+_services',\n",
    "    'Education9Y6Y' = 'education.binned.basic.9y_+_basic.6y',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our 36 variables with 34 selected by boruta and client_id to identify the customer and subscribe as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(DataClean, file = \"C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/DataClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(PredictClean, file = \"C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/PredictClean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid computing each time the preprocessing part and especially the boruta method, we save above the dataset processed and load them here so we can only perform this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'C:/Users/spavot/Desktop/Big Data/Statistical &amp; Machine Learning/Kaggle/Data/DataClean.csv'"
      ],
      "text/latex": [
       "'C:/Users/spavot/Desktop/Big Data/Statistical \\& Machine Learning/Kaggle/Data/DataClean.csv'"
      ],
      "text/markdown": [
       "'C:/Users/spavot/Desktop/Big Data/Statistical &amp; Machine Learning/Kaggle/Data/DataClean.csv'"
      ],
      "text/plain": [
       "[1] \"C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/DataClean.csv\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste(data, \"DataClean.csv\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataClean = read.csv(paste(data,\"DataClean.csv\",sep =\"\"))\n",
    "PredictClean = read.csv(paste(data, \"PredictClean.csv\",sep =\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the MLR package to compute different model as it has the possibility to create pipeline for model easily in order to test some values for the tuning parameter of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: We do not recommend launching Random Forest and XGBoost because the Gridsearch performed searches through a wide range of values with a high number of iterations resulting in an execution time of many hours. In this notebook, they are already computed so you see the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is going to be used for all the model. The first part, we set up the different parameter of our pipeline by selecting the cross validation, the model we want to fit, the data to use and after we can select the parameter to try for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the code is if statement that check if there's parameter to tune or not. If not, it perform the cross validation on the model. For example, as the logistic regression don't have any tuning parameter, the code will perform the \"else\" part which only the cross validation and the AUC. If the model has tuning parameter, then it will apply those filled in the \"tune_params\" and the control in \"ctrl\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as logistic regression don't have parameter tuning, we perform the model using a 10K fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc.train   auc.test    \n",
      "[Resample] iter 1:    0.7948093   0.8326098   \n",
      "[Resample] iter 2:    0.7976568   0.8146951   \n",
      "[Resample] iter 3:    0.7970369   0.8089104   \n",
      "[Resample] iter 4:    0.7993819   0.7937235   \n",
      "[Resample] iter 5:    0.8060192   0.7392591   \n",
      "[Resample] iter 6:    0.8047701   0.7427885   \n",
      "[Resample] iter 7:    0.7929618   0.8538747   \n",
      "[Resample] iter 8:    0.8010801   0.7801635   \n",
      "[Resample] iter 9:    0.8030220   0.7651822   \n",
      "[Resample] iter 10:   0.8013915   0.7734886   \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7904695,auc.train.mean=0.7998129\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options(warn=-1)      \n",
    "set.seed(1)\n",
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.logreg\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc, setAggregation(mlr::auc, train.mean)))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated Result: auc.test.mean=0.7904695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we estimate the test error, we select the best model and compute prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.00\n",
       "        prob.0     prob.1 response\n",
       "7001 0.9415796 0.05842040        0\n",
       "7002 0.9628539 0.03714607        0\n",
       "7003 0.6312313 0.36876867        0\n",
       "7004 0.9758291 0.02417091        0\n",
       "7005 0.9544535 0.04554651        0\n",
       "7006 0.9042745 0.09572546        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Logistic Regression/lr_1_simple.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Linear Discriminant Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try the same as above but using the linear discriminant analysis as this model appears to perform pretty well on classification of level 2 (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc.train   auc.test    \n",
      "[Resample] iter 1:    0.7903608   0.8223202   \n",
      "[Resample] iter 2:    0.7923674   0.8176071   \n",
      "[Resample] iter 3:    0.7918685   0.8027039   \n",
      "[Resample] iter 4:    0.7944053   0.7891676   \n",
      "[Resample] iter 5:    0.8019933   0.7282340   \n",
      "[Resample] iter 6:    0.8009748   0.7357034   \n",
      "[Resample] iter 7:    0.7879736   0.8531361   \n",
      "[Resample] iter 8:    0.7962206   0.7754036   \n",
      "[Resample] iter 9:    0.7983025   0.7619349   \n",
      "[Resample] iter 10:   0.7954318   0.7678996   \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7854110,auc.train.mean=0.7949899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options(warn=-1)      \n",
    "set.seed(1)\n",
    "\n",
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.lda\", predict.type = \"prob\")\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc, setAggregation(mlr::auc, train.mean)))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated Result: auc.test.mean=0.7854110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.02\n",
       "        prob.0      prob.1 response\n",
       "7001 0.9760844 0.023915557        0\n",
       "7002 0.9843126 0.015687362        0\n",
       "7003 0.2873534 0.712646624        1\n",
       "7004 0.9913704 0.008629581        0\n",
       "7005 0.9775320 0.022468025        0\n",
       "7006 0.9609310 0.039068959        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Linear Discriminant Analysis/LDA_1_simple.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Gradient Boosting - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Gradient Boosting with random search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the XGBoost algorithm which is an \"improved\" version of the gradient boosting to perform prediction now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a lot of parameters, here we use nrounds, max_depth, lamnda, eta, subsample, min_child_weight and colsample_bytree as tuning parameter and we give them a range value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the ctrl using maketuneControlRandom = 250 to try 250 different random version of the range of the parameters discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a random search way which means we give a number of iterations and a range of value and the algorithm will select randomly parameters within the range and fit the model, finally returning the AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "set.seed(1)\n",
    "# Set up cross-validation\n",
    "\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.xgboost\", predict.type = \"prob\")\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    makeIntegerParam(\"nrounds\",lower=200,upper=600),\n",
    "    makeIntegerParam(\"max_depth\",lower=3,upper=20),\n",
    "    makeNumericParam(\"lambda\",lower=0.55,upper=0.60),\n",
    "    makeNumericParam(\"eta\", lower = 0.001, upper = 0.5),\n",
    "    makeNumericParam(\"subsample\", lower = 0.10, upper = 0.80),\n",
    "    makeNumericParam(\"min_child_weight\",lower=1,upper=5),\n",
    "    makeNumericParam(\"colsample_bytree\",lower = 0.2,upper = 0.8)\n",
    ")\n",
    "\n",
    "ctrl = makeTuneControlRandom(maxit = 250L)\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc, setAggregation(mlr::auc, train.mean)))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid computing again, we put the results of computing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: auc.test.mean=0.7970922\n",
    "* nrounds=333\n",
    "* max_depth=3 \n",
    "* lambda=0.596 \n",
    "* eta=0.0241 \n",
    "* subsample=0.699 \n",
    "* min_child_weight=1.93 \n",
    "* colsample_bytree=0.771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.00\n",
       "        prob.1    prob.0 response\n",
       "7001 0.3722028 0.6277972        0\n",
       "7002 0.3722028 0.6277972        0\n",
       "7003 0.5166605 0.4833395        1\n",
       "7004 0.3640245 0.6359755        0\n",
       "7005 0.3640245 0.6359755        0\n",
       "7006 0.3722028 0.6277972        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in data.frame(client_id = PredictClean$client_id, subscribe = pred$data$prob.1): object 'PredictClean' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in data.frame(client_id = PredictClean$client_id, subscribe = pred$data$prob.1): object 'PredictClean' not found\nTraceback:\n",
      "1. data.frame(client_id = PredictClean$client_id, subscribe = pred$data$prob.1)"
     ]
    }
   ],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/XGBoost/XGB_1_random.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boosting with specified tuning parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we compute the Gradient Boosting but we specify vectors of value the algorithm has to try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that with random search, the XGBoost with the following parameters had the best AUC: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: auc.test.mean=0.7970922\n",
    "* nrounds=333\n",
    "* max_depth=3 \n",
    "* lambda=0.596 \n",
    "* eta=0.0241 \n",
    "* subsample=0.699 \n",
    "* min_child_weight=1.93 \n",
    "* colsample_bytree=0.771\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some parameters close to those to see if we can increase the AUC:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time, we use the makeTuneControlRandom() with 50L to execute only 50 iterations as the specified parameters gives 729 possibilities which is way too high to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune] Started tuning learner classif.xgboost for parameter set:\n",
      "                     Type len Def          Constr Req Tunable Trafo\n",
      "nrounds          discrete   -   -     300,330,360   -    TRUE     -\n",
      "max_depth        discrete   -   -               3   -    TRUE     -\n",
      "lambda           discrete   -   -     0.4,0.5,0.6   -    TRUE     -\n",
      "eta              discrete   -   - 0.02,0.025,0.03   -    TRUE     -\n",
      "subsample        discrete   -   -     0.6,0.7,0.8   -    TRUE     -\n",
      "min_child_weight discrete   -   -           1,2,3   -    TRUE     -\n",
      "colsample_bytree discrete   -   -     0.6,0.7,0.8   -    TRUE     -\n",
      "With control class: TuneControlRandom\n",
      "Imputation value: -0\n",
      "[Tune-x] 1: nrounds=360; max_depth=3; lambda=0.6; eta=0.02; subsample=0.7; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 1: auc.test.mean=0.7942225; time: 0.4 min\n",
      "[Tune-x] 2: nrounds=360; max_depth=3; lambda=0.4; eta=0.03; subsample=0.8; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 2: auc.test.mean=0.7917626; time: 0.4 min\n",
      "[Tune-x] 3: nrounds=300; max_depth=3; lambda=0.4; eta=0.025; subsample=0.8; min_child_weight=3; colsample_bytree=0.7\n",
      "[Tune-y] 3: auc.test.mean=0.7935955; time: 0.3 min\n",
      "[Tune-x] 4: nrounds=360; max_depth=3; lambda=0.4; eta=0.03; subsample=0.8; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 4: auc.test.mean=0.7924167; time: 0.4 min\n",
      "[Tune-x] 5: nrounds=300; max_depth=3; lambda=0.6; eta=0.025; subsample=0.6; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 5: auc.test.mean=0.7932050; time: 0.3 min\n",
      "[Tune-x] 6: nrounds=330; max_depth=3; lambda=0.6; eta=0.025; subsample=0.8; min_child_weight=1; colsample_bytree=0.6\n",
      "[Tune-y] 6: auc.test.mean=0.7932185; time: 0.4 min\n",
      "[Tune-x] 7: nrounds=360; max_depth=3; lambda=0.5; eta=0.025; subsample=0.8; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 7: auc.test.mean=0.7931784; time: 0.4 min\n",
      "[Tune-x] 8: nrounds=360; max_depth=3; lambda=0.6; eta=0.025; subsample=0.6; min_child_weight=1; colsample_bytree=0.8\n",
      "[Tune-y] 8: auc.test.mean=0.7939057; time: 0.4 min\n",
      "[Tune-x] 9: nrounds=330; max_depth=3; lambda=0.5; eta=0.025; subsample=0.8; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 9: auc.test.mean=0.7938374; time: 0.4 min\n",
      "[Tune-x] 10: nrounds=300; max_depth=3; lambda=0.6; eta=0.02; subsample=0.6; min_child_weight=1; colsample_bytree=0.8\n",
      "[Tune-y] 10: auc.test.mean=0.7934958; time: 0.4 min\n",
      "[Tune-x] 11: nrounds=360; max_depth=3; lambda=0.4; eta=0.03; subsample=0.8; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 11: auc.test.mean=0.7925156; time: 0.4 min\n",
      "[Tune-x] 12: nrounds=360; max_depth=3; lambda=0.4; eta=0.03; subsample=0.7; min_child_weight=2; colsample_bytree=0.7\n",
      "[Tune-y] 12: auc.test.mean=0.7913564; time: 0.4 min\n",
      "[Tune-x] 13: nrounds=300; max_depth=3; lambda=0.6; eta=0.025; subsample=0.7; min_child_weight=2; colsample_bytree=0.6\n",
      "[Tune-y] 13: auc.test.mean=0.7941013; time: 0.3 min\n",
      "[Tune-x] 14: nrounds=300; max_depth=3; lambda=0.4; eta=0.02; subsample=0.8; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 14: auc.test.mean=0.7938019; time: 0.3 min\n",
      "[Tune-x] 15: nrounds=300; max_depth=3; lambda=0.6; eta=0.025; subsample=0.7; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 15: auc.test.mean=0.7940891; time: 0.3 min\n",
      "[Tune-x] 16: nrounds=300; max_depth=3; lambda=0.4; eta=0.02; subsample=0.6; min_child_weight=1; colsample_bytree=0.6\n",
      "[Tune-y] 16: auc.test.mean=0.7950037; time: 0.3 min\n",
      "[Tune-x] 17: nrounds=330; max_depth=3; lambda=0.6; eta=0.02; subsample=0.6; min_child_weight=2; colsample_bytree=0.6\n",
      "[Tune-y] 17: auc.test.mean=0.7952801; time: 0.4 min\n",
      "[Tune-x] 18: nrounds=300; max_depth=3; lambda=0.5; eta=0.02; subsample=0.8; min_child_weight=1; colsample_bytree=0.8\n",
      "[Tune-y] 18: auc.test.mean=0.7950225; time: 0.3 min\n",
      "[Tune-x] 19: nrounds=360; max_depth=3; lambda=0.5; eta=0.03; subsample=0.6; min_child_weight=2; colsample_bytree=0.6\n",
      "[Tune-y] 19: auc.test.mean=0.7925744; time: 0.4 min\n",
      "[Tune-x] 20: nrounds=330; max_depth=3; lambda=0.5; eta=0.025; subsample=0.8; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 20: auc.test.mean=0.7932399; time: 0.4 min\n",
      "[Tune-x] 21: nrounds=300; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=1; colsample_bytree=0.6\n",
      "[Tune-y] 21: auc.test.mean=0.7926729; time: 0.3 min\n",
      "[Tune-x] 22: nrounds=360; max_depth=3; lambda=0.4; eta=0.03; subsample=0.8; min_child_weight=2; colsample_bytree=0.7\n",
      "[Tune-y] 22: auc.test.mean=0.7924426; time: 0.4 min\n",
      "[Tune-x] 23: nrounds=300; max_depth=3; lambda=0.4; eta=0.03; subsample=0.8; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 23: auc.test.mean=0.7938731; time: 0.4 min\n",
      "[Tune-x] 24: nrounds=360; max_depth=3; lambda=0.6; eta=0.03; subsample=0.7; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 24: auc.test.mean=0.7914468; time: 0.4 min\n",
      "[Tune-x] 25: nrounds=360; max_depth=3; lambda=0.5; eta=0.02; subsample=0.8; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 25: auc.test.mean=0.7942402; time: 0.4 min\n",
      "[Tune-x] 26: nrounds=360; max_depth=3; lambda=0.6; eta=0.03; subsample=0.8; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 26: auc.test.mean=0.7926450; time: 0.4 min\n",
      "[Tune-x] 27: nrounds=360; max_depth=3; lambda=0.6; eta=0.03; subsample=0.6; min_child_weight=3; colsample_bytree=0.7\n",
      "[Tune-y] 27: auc.test.mean=0.7919988; time: 0.4 min\n",
      "[Tune-x] 28: nrounds=360; max_depth=3; lambda=0.4; eta=0.025; subsample=0.6; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 28: auc.test.mean=0.7942180; time: 0.4 min\n",
      "[Tune-x] 29: nrounds=330; max_depth=3; lambda=0.6; eta=0.02; subsample=0.8; min_child_weight=3; colsample_bytree=0.7\n",
      "[Tune-y] 29: auc.test.mean=0.7949869; time: 0.4 min\n",
      "[Tune-x] 30: nrounds=360; max_depth=3; lambda=0.4; eta=0.02; subsample=0.7; min_child_weight=1; colsample_bytree=0.6\n",
      "[Tune-y] 30: auc.test.mean=0.7949543; time: 0.4 min\n",
      "[Tune-x] 31: nrounds=360; max_depth=3; lambda=0.6; eta=0.02; subsample=0.7; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 31: auc.test.mean=0.7934358; time: 0.4 min\n",
      "[Tune-x] 32: nrounds=300; max_depth=3; lambda=0.4; eta=0.02; subsample=0.6; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 32: auc.test.mean=0.7942407; time: 0.4 min\n",
      "[Tune-x] 33: nrounds=300; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 33: auc.test.mean=0.7943902; time: 0.3 min\n",
      "[Tune-x] 34: nrounds=300; max_depth=3; lambda=0.5; eta=0.03; subsample=0.6; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 34: auc.test.mean=0.7929331; time: 0.3 min\n",
      "[Tune-x] 35: nrounds=300; max_depth=3; lambda=0.5; eta=0.025; subsample=0.7; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 35: auc.test.mean=0.7944915; time: 0.3 min\n",
      "[Tune-x] 36: nrounds=300; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 36: auc.test.mean=0.7929361; time: 0.3 min\n",
      "[Tune-x] 37: nrounds=330; max_depth=3; lambda=0.6; eta=0.02; subsample=0.6; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 37: auc.test.mean=0.7921031; time: 0.4 min\n",
      "[Tune-x] 38: nrounds=330; max_depth=3; lambda=0.4; eta=0.025; subsample=0.7; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 38: auc.test.mean=0.7916527; time: 0.4 min\n",
      "[Tune-x] 39: nrounds=330; max_depth=3; lambda=0.6; eta=0.03; subsample=0.8; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 39: auc.test.mean=0.7932981; time: 0.4 min\n",
      "[Tune-x] 40: nrounds=360; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 40: auc.test.mean=0.7934686; time: 0.5 min\n",
      "[Tune-x] 41: nrounds=330; max_depth=3; lambda=0.6; eta=0.03; subsample=0.6; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 41: auc.test.mean=0.7933968; time: 0.4 min\n",
      "[Tune-x] 42: nrounds=360; max_depth=3; lambda=0.4; eta=0.025; subsample=0.8; min_child_weight=3; colsample_bytree=0.8\n",
      "[Tune-y] 42: auc.test.mean=0.7932590; time: 0.4 min\n",
      "[Tune-x] 43: nrounds=330; max_depth=3; lambda=0.6; eta=0.025; subsample=0.7; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 43: auc.test.mean=0.7934740; time: 0.4 min\n",
      "[Tune-x] 44: nrounds=300; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=2; colsample_bytree=0.6\n",
      "[Tune-y] 44: auc.test.mean=0.7929474; time: 0.3 min\n",
      "[Tune-x] 45: nrounds=300; max_depth=3; lambda=0.5; eta=0.02; subsample=0.8; min_child_weight=2; colsample_bytree=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-y] 45: auc.test.mean=0.7954835; time: 0.4 min\n",
      "[Tune-x] 46: nrounds=360; max_depth=3; lambda=0.6; eta=0.025; subsample=0.8; min_child_weight=2; colsample_bytree=0.8\n",
      "[Tune-y] 46: auc.test.mean=0.7932395; time: 0.5 min\n",
      "[Tune-x] 47: nrounds=330; max_depth=3; lambda=0.5; eta=0.03; subsample=0.7; min_child_weight=1; colsample_bytree=0.7\n",
      "[Tune-y] 47: auc.test.mean=0.7921854; time: 0.4 min\n",
      "[Tune-x] 48: nrounds=330; max_depth=3; lambda=0.6; eta=0.03; subsample=0.7; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 48: auc.test.mean=0.7915820; time: 0.4 min\n",
      "[Tune-x] 49: nrounds=300; max_depth=3; lambda=0.5; eta=0.02; subsample=0.7; min_child_weight=3; colsample_bytree=0.6\n",
      "[Tune-y] 49: auc.test.mean=0.7941121; time: 0.3 min\n",
      "[Tune-x] 50: nrounds=360; max_depth=3; lambda=0.5; eta=0.02; subsample=0.8; min_child_weight=1; colsample_bytree=0.8\n",
      "[Tune-y] 50: auc.test.mean=0.7939552; time: 0.4 min\n",
      "[Tune] Result: nrounds=300; max_depth=3; lambda=0.5; eta=0.02; subsample=0.8; min_child_weight=2; colsample_bytree=0.8 : auc.test.mean=0.7954835\n"
     ]
    }
   ],
   "source": [
    "options(warn=-1)\n",
    "set.seed(1)\n",
    "# Set up cross-validation\n",
    "\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.xgboost\", predict.type = \"prob\")\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    makeDiscreteParam(\"nrounds\", value = c(300, 330, 360)),\n",
    "    makeDiscreteParam(\"max_depth\", value = 3),\n",
    "    makeDiscreteParam(\"lambda\", value =c(0.4, 0.5, 0.6)),\n",
    "    makeDiscreteParam(\"eta\", value = c(0.02, 0.025, 0.03)),\n",
    "    makeDiscreteParam(\"subsample\", value = c(0.6, 0.7, 0.8)),\n",
    "    makeDiscreteParam(\"min_child_weight\", value = c(1, 2, 3)),\n",
    "    makeDiscreteParam(\"colsample_bytree\", value = c(0.6, 0.7, 0.8))\n",
    ")\n",
    "\n",
    "ctrl = makeTuneControlRandom(maxit = 50L)\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc, setAggregation(mlr::auc, train.mean)))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Result: auc.test.mean=0.7954835\n",
    " * nrounds=300\n",
    " * max_depth=3\n",
    " * lambda=0.5\n",
    " * eta=0.02 \n",
    " * subsample=0.8 \n",
    " * min_child_weight=2\n",
    " * colsample_bytree=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.00\n",
       "     prob.1    prob.0 response\n",
       "1 0.3722028 0.6277972        0\n",
       "2 0.3722028 0.6277972        0\n",
       "3 0.5166605 0.4833395        1\n",
       "4 0.3640245 0.6359755        0\n",
       "5 0.3640245 0.6359755        0\n",
       "6 0.3722028 0.6277972        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/XGBoost/XGB_1_specified.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest with random search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a random forest algorithm using range of value for the ntree, mtry and nodesize parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time, we use the makeTuneControlRandom to perform a number of random models regarding the parameters above. We use 100 iterations here as we we want to try less different tuning parameters than in XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune] Started tuning learner classif.randomForest for parameter set:\n",
      "            Type len Def    Constr Req Tunable Trafo\n",
      "ntree    integer   -   - 50 to 500   -    TRUE     -\n",
      "mtry     integer   -   -   3 to 10   -    TRUE     -\n",
      "nodesize integer   -   -  10 to 50   -    TRUE     -\n",
      "With control class: TuneControlRandom\n",
      "Imputation value: -0\n",
      "[Tune-x] 1: ntree=101; mtry=9; nodesize=20\n",
      "[Tune-y] 1: auc.test.mean=0.7776665; time: 0.9 min\n",
      "[Tune-x] 2: ntree=394; mtry=9; nodesize=34\n",
      "[Tune-y] 2: auc.test.mean=0.7791837; time: 4.1 min\n",
      "[Tune-x] 3: ntree=95; mtry=7; nodesize=14\n",
      "[Tune-y] 3: auc.test.mean=0.7781086; time: 0.9 min\n",
      "[Tune-x] 4: ntree=399; mtry=6; nodesize=27\n",
      "[Tune-y] 4: auc.test.mean=0.7812370; time: 3.2 min\n",
      "[Tune-x] 5: ntree=362; mtry=8; nodesize=20\n",
      "[Tune-y] 5: auc.test.mean=0.7822717; time: 3.2 min\n",
      "[Tune-x] 6: ntree=204; mtry=8; nodesize=26\n",
      "[Tune-y] 6: auc.test.mean=0.7780562; time: 1.7 min\n",
      "[Tune-x] 7: ntree=97; mtry=7; nodesize=17\n",
      "[Tune-y] 7: auc.test.mean=0.7789236; time: 0.8 min\n",
      "[Tune-x] 8: ntree=240; mtry=5; nodesize=39\n",
      "[Tune-y] 8: auc.test.mean=0.7749211; time: 1.8 min\n",
      "[Tune-x] 9: ntree=369; mtry=8; nodesize=47\n",
      "[Tune-y] 9: auc.test.mean=0.7787537; time: 2.9 min\n",
      "[Tune-x] 10: ntree=227; mtry=5; nodesize=25\n",
      "[Tune-y] 10: auc.test.mean=0.7762390; time: 1.7 min\n",
      "[Tune-x] 11: ntree=202; mtry=5; nodesize=15\n",
      "[Tune-y] 11: auc.test.mean=0.7807479; time: 1.6 min\n",
      "[Tune-x] 12: ntree=296; mtry=4; nodesize=18\n",
      "[Tune-y] 12: auc.test.mean=0.7811674; time: 2.2 min\n",
      "[Tune-x] 13: ntree=449; mtry=6; nodesize=40\n",
      "[Tune-y] 13: auc.test.mean=0.7738489; time: 3.5 min\n",
      "[Tune-x] 14: ntree=335; mtry=8; nodesize=35\n",
      "[Tune-y] 14: auc.test.mean=0.7757798; time: 2.8 min\n",
      "[Tune-x] 15: ntree=228; mtry=4; nodesize=46\n",
      "[Tune-y] 15: auc.test.mean=0.7747547; time: 1.6 min\n",
      "[Tune-x] 16: ntree=365; mtry=4; nodesize=24\n",
      "[Tune-y] 16: auc.test.mean=0.7803678; time: 2.7 min\n",
      "[Tune-x] 17: ntree=333; mtry=4; nodesize=35\n",
      "[Tune-y] 17: auc.test.mean=0.7702022; time: 2.5 min\n",
      "[Tune-x] 18: ntree=405; mtry=6; nodesize=17\n",
      "[Tune-y] 18: auc.test.mean=0.7859184; time: 3.4 min\n",
      "[Tune-x] 19: ntree=250; mtry=4; nodesize=36\n",
      "[Tune-y] 19: auc.test.mean=0.7721467; time: 1.8 min\n",
      "[Tune-x] 20: ntree=229; mtry=3; nodesize=37\n",
      "[Tune-y] 20: auc.test.mean=0.7710824; time: 1.5 min\n",
      "[Tune-x] 21: ntree=67; mtry=5; nodesize=24\n",
      "[Tune-y] 21: auc.test.mean=0.7665775; time: 0.5 min\n",
      "[Tune-x] 22: ntree=438; mtry=5; nodesize=31\n",
      "[Tune-y] 22: auc.test.mean=0.7775233; time: 3.5 min\n",
      "[Tune-x] 23: ntree=323; mtry=10; nodesize=50\n",
      "[Tune-y] 23: auc.test.mean=0.7726265; time: 2.9 min\n",
      "[Tune-x] 24: ntree=279; mtry=4; nodesize=45\n",
      "[Tune-y] 24: auc.test.mean=0.7684871; time: 1.8 min\n",
      "[Tune-x] 25: ntree=377; mtry=5; nodesize=27\n",
      "[Tune-y] 25: auc.test.mean=0.7758174; time: 2.4 min\n",
      "[Tune-x] 26: ntree=488; mtry=7; nodesize=28\n",
      "[Tune-y] 26: auc.test.mean=0.7813486; time: 3.2 min\n",
      "[Tune-x] 27: ntree=50; mtry=9; nodesize=17\n",
      "[Tune-y] 27: auc.test.mean=0.7690346; time: 0.4 min\n",
      "[Tune-x] 28: ntree=427; mtry=7; nodesize=44\n",
      "[Tune-y] 28: auc.test.mean=0.7777787; time: 2.7 min\n",
      "[Tune-x] 29: ntree=366; mtry=6; nodesize=18\n",
      "[Tune-y] 29: auc.test.mean=0.7809732; time: 2.4 min\n",
      "[Tune-x] 30: ntree=306; mtry=4; nodesize=17\n",
      "[Tune-y] 30: auc.test.mean=0.7808938; time: 1.9 min\n",
      "[Tune-x] 31: ntree=169; mtry=3; nodesize=38\n",
      "[Tune-y] 31: auc.test.mean=0.7737001; time: 0.9 min\n",
      "[Tune-x] 32: ntree=434; mtry=3; nodesize=46\n",
      "[Tune-y] 32: auc.test.mean=0.7742188; time: 2.3 min\n",
      "[Tune-x] 33: ntree=75; mtry=4; nodesize=48\n",
      "[Tune-y] 33: auc.test.mean=0.7601057; time: 0.4 min\n",
      "[Tune-x] 34: ntree=237; mtry=3; nodesize=13\n",
      "[Tune-y] 34: auc.test.mean=0.7811303; time: 1.3 min\n",
      "[Tune-x] 35: ntree=331; mtry=9; nodesize=20\n",
      "[Tune-y] 35: auc.test.mean=0.7812727; time: 2.3 min\n",
      "[Tune-x] 36: ntree=308; mtry=8; nodesize=30\n",
      "[Tune-y] 36: auc.test.mean=0.7814010; time: 2.0 min\n",
      "[Tune-x] 37: ntree=459; mtry=4; nodesize=43\n",
      "[Tune-y] 37: auc.test.mean=0.7748388; time: 2.6 min\n",
      "[Tune-x] 38: ntree=450; mtry=5; nodesize=27\n",
      "[Tune-y] 38: auc.test.mean=0.7763375; time: 2.8 min\n",
      "[Tune-x] 39: ntree=182; mtry=7; nodesize=11\n",
      "[Tune-y] 39: auc.test.mean=0.7848501; time: 1.2 min\n",
      "[Tune-x] 40: ntree=256; mtry=4; nodesize=45\n",
      "[Tune-y] 40: auc.test.mean=0.7734837; time: 1.3 min\n",
      "[Tune-x] 41: ntree=338; mtry=6; nodesize=40\n",
      "[Tune-y] 41: auc.test.mean=0.7820121; time: 1.8 min\n",
      "[Tune-x] 42: ntree=188; mtry=3; nodesize=46\n",
      "[Tune-y] 42: auc.test.mean=0.7649903; time: 0.8 min\n",
      "[Tune-x] 43: ntree=53; mtry=7; nodesize=13\n",
      "[Tune-y] 43: auc.test.mean=0.7715373; time: 0.3 min\n",
      "[Tune-x] 44: ntree=220; mtry=9; nodesize=15\n",
      "[Tune-y] 44: auc.test.mean=0.7822856; time: 1.3 min\n",
      "[Tune-x] 45: ntree=207; mtry=8; nodesize=44\n",
      "[Tune-y] 45: auc.test.mean=0.7693964; time: 1.1 min\n",
      "[Tune-x] 46: ntree=190; mtry=3; nodesize=34\n",
      "[Tune-y] 46: auc.test.mean=0.7684569; time: 0.9 min\n",
      "[Tune-x] 47: ntree=254; mtry=6; nodesize=23\n",
      "[Tune-y] 47: auc.test.mean=0.7842679; time: 1.4 min\n",
      "[Tune-x] 48: ntree=322; mtry=8; nodesize=47\n",
      "[Tune-y] 48: auc.test.mean=0.7748467; time: 1.8 min\n",
      "[Tune-x] 49: ntree=141; mtry=8; nodesize=38\n",
      "[Tune-y] 49: auc.test.mean=0.7757416; time: 0.8 min\n",
      "[Tune-x] 50: ntree=193; mtry=8; nodesize=49\n",
      "[Tune-y] 50: auc.test.mean=0.7676646; time: 1.0 min\n",
      "[Tune-x] 51: ntree=405; mtry=6; nodesize=49\n",
      "[Tune-y] 51: auc.test.mean=0.7658801; time: 2.0 min\n",
      "[Tune-x] 52: ntree=153; mtry=4; nodesize=50\n",
      "[Tune-y] 52: auc.test.mean=0.7701028; time: 0.6 min\n",
      "[Tune-x] 53: ntree=262; mtry=8; nodesize=50\n",
      "[Tune-y] 53: auc.test.mean=0.7661969; time: 1.2 min\n",
      "[Tune-x] 54: ntree=54; mtry=6; nodesize=46\n",
      "[Tune-y] 54: auc.test.mean=0.7647052; time: 0.2 min\n",
      "[Tune-x] 55: ntree=56; mtry=7; nodesize=32\n",
      "[Tune-y] 55: auc.test.mean=0.7695399; time: 0.3 min\n",
      "[Tune-x] 56: ntree=383; mtry=10; nodesize=45\n",
      "[Tune-y] 56: auc.test.mean=0.7773906; time: 1.9 min\n",
      "[Tune-x] 57: ntree=61; mtry=9; nodesize=16\n",
      "[Tune-y] 57: auc.test.mean=0.7697658; time: 0.3 min\n",
      "[Tune-x] 58: ntree=293; mtry=4; nodesize=41\n",
      "[Tune-y] 58: auc.test.mean=0.7729242; time: 1.2 min\n",
      "[Tune-x] 59: ntree=114; mtry=9; nodesize=29\n",
      "[Tune-y] 59: auc.test.mean=0.7762151; time: 0.6 min\n",
      "[Tune-x] 60: ntree=207; mtry=5; nodesize=36\n",
      "[Tune-y] 60: auc.test.mean=0.7732481; time: 0.9 min\n",
      "[Tune-x] 61: ntree=452; mtry=6; nodesize=46\n",
      "[Tune-y] 61: auc.test.mean=0.7821344; time: 2.1 min\n",
      "[Tune-x] 62: ntree=370; mtry=3; nodesize=27\n",
      "[Tune-y] 62: auc.test.mean=0.7782267; time: 1.5 min\n",
      "[Tune-x] 63: ntree=266; mtry=6; nodesize=12\n",
      "[Tune-y] 63: auc.test.mean=0.7837286; time: 1.3 min\n",
      "[Tune-x] 64: ntree=386; mtry=4; nodesize=24\n",
      "[Tune-y] 64: auc.test.mean=0.7769630; time: 1.7 min\n",
      "[Tune-x] 65: ntree=402; mtry=8; nodesize=29\n",
      "[Tune-y] 65: auc.test.mean=0.7800773; time: 1.9 min\n",
      "[Tune-x] 66: ntree=143; mtry=7; nodesize=17\n",
      "[Tune-y] 66: auc.test.mean=0.7758032; time: 0.7 min\n",
      "[Tune-x] 67: ntree=206; mtry=3; nodesize=11\n",
      "[Tune-y] 67: auc.test.mean=0.7769776; time: 0.9 min\n",
      "[Tune-x] 68: ntree=337; mtry=4; nodesize=33\n",
      "[Tune-y] 68: auc.test.mean=0.7764737; time: 1.5 min\n",
      "[Tune-x] 69: ntree=291; mtry=3; nodesize=30\n",
      "[Tune-y] 69: auc.test.mean=0.7687959; time: 1.2 min\n",
      "[Tune-x] 70: ntree=408; mtry=9; nodesize=21\n",
      "[Tune-y] 70: auc.test.mean=0.7744948; time: 2.0 min\n",
      "[Tune-x] 71: ntree=339; mtry=4; nodesize=28\n",
      "[Tune-y] 71: auc.test.mean=0.7775561; time: 1.5 min\n",
      "[Tune-x] 72: ntree=304; mtry=4; nodesize=39\n",
      "[Tune-y] 72: auc.test.mean=0.7732588; time: 1.3 min\n",
      "[Tune-x] 73: ntree=273; mtry=5; nodesize=34\n",
      "[Tune-y] 73: auc.test.mean=0.7728846; time: 1.2 min\n",
      "[Tune-x] 74: ntree=189; mtry=4; nodesize=22\n",
      "[Tune-y] 74: auc.test.mean=0.7705294; time: 0.8 min\n",
      "[Tune-x] 75: ntree=201; mtry=3; nodesize=43\n",
      "[Tune-y] 75: auc.test.mean=0.7653562; time: 0.8 min\n",
      "[Tune-x] 76: ntree=187; mtry=4; nodesize=18\n",
      "[Tune-y] 76: auc.test.mean=0.7780739; time: 0.8 min\n",
      "[Tune-x] 77: ntree=306; mtry=9; nodesize=44\n",
      "[Tune-y] 77: auc.test.mean=0.7735967; time: 1.5 min\n",
      "[Tune-x] 78: ntree=461; mtry=5; nodesize=30\n",
      "[Tune-y] 78: auc.test.mean=0.7778776; time: 2.1 min\n",
      "[Tune-x] 79: ntree=253; mtry=3; nodesize=43\n",
      "[Tune-y] 79: auc.test.mean=0.7722392; time: 1.0 min\n",
      "[Tune-x] 80: ntree=417; mtry=7; nodesize=23\n",
      "[Tune-y] 80: auc.test.mean=0.7804179; time: 2.0 min\n",
      "[Tune-x] 81: ntree=183; mtry=8; nodesize=15\n",
      "[Tune-y] 81: auc.test.mean=0.7828219; time: 0.9 min\n",
      "[Tune-x] 82: ntree=215; mtry=7; nodesize=22\n",
      "[Tune-y] 82: auc.test.mean=0.7808640; time: 1.0 min\n",
      "[Tune-x] 83: ntree=196; mtry=4; nodesize=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-y] 83: auc.test.mean=0.7687189; time: 0.9 min\n",
      "[Tune-x] 84: ntree=398; mtry=4; nodesize=21\n",
      "[Tune-y] 84: auc.test.mean=0.7779418; time: 1.7 min\n",
      "[Tune-x] 85: ntree=299; mtry=10; nodesize=21\n",
      "[Tune-y] 85: auc.test.mean=0.7834278; time: 1.6 min\n",
      "[Tune-x] 86: ntree=383; mtry=9; nodesize=45\n",
      "[Tune-y] 86: auc.test.mean=0.7715285; time: 1.8 min\n",
      "[Tune-x] 87: ntree=430; mtry=4; nodesize=16\n",
      "[Tune-y] 87: auc.test.mean=0.7827743; time: 1.9 min\n",
      "[Tune-x] 88: ntree=117; mtry=10; nodesize=44\n",
      "[Tune-y] 88: auc.test.mean=0.7664887; time: 0.6 min\n",
      "[Tune-x] 89: ntree=386; mtry=4; nodesize=38\n",
      "[Tune-y] 89: auc.test.mean=0.7774106; time: 1.6 min\n",
      "[Tune-x] 90: ntree=139; mtry=3; nodesize=23\n",
      "[Tune-y] 90: auc.test.mean=0.7754538; time: 0.6 min\n",
      "[Tune-x] 91: ntree=273; mtry=7; nodesize=31\n",
      "[Tune-y] 91: auc.test.mean=0.7766578; time: 1.3 min\n",
      "[Tune-x] 92: ntree=484; mtry=9; nodesize=45\n",
      "[Tune-y] 92: auc.test.mean=0.7678479; time: 2.3 min\n",
      "[Tune-x] 93: ntree=92; mtry=4; nodesize=15\n",
      "[Tune-y] 93: auc.test.mean=0.7714950; time: 0.4 min\n",
      "[Tune-x] 94: ntree=326; mtry=9; nodesize=45\n",
      "[Tune-y] 94: auc.test.mean=0.7761274; time: 1.6 min\n",
      "[Tune-x] 95: ntree=410; mtry=5; nodesize=32\n",
      "[Tune-y] 95: auc.test.mean=0.7742295; time: 1.8 min\n",
      "[Tune-x] 96: ntree=408; mtry=10; nodesize=41\n",
      "[Tune-y] 96: auc.test.mean=0.7730360; time: 2.0 min\n",
      "[Tune-x] 97: ntree=461; mtry=7; nodesize=18\n",
      "[Tune-y] 97: auc.test.mean=0.7847833; time: 2.3 min\n",
      "[Tune-x] 98: ntree=480; mtry=10; nodesize=35\n",
      "[Tune-y] 98: auc.test.mean=0.7761313; time: 2.4 min\n",
      "[Tune-x] 99: ntree=357; mtry=3; nodesize=48\n",
      "[Tune-y] 99: auc.test.mean=0.7793750; time: 1.4 min\n",
      "[Tune-x] 100: ntree=378; mtry=5; nodesize=43\n",
      "[Tune-y] 100: auc.test.mean=0.7717334; time: 1.7 min\n",
      "[Tune] Result: ntree=405; mtry=6; nodesize=17 : auc.test.mean=0.7859184\n"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "options(warn=-1)\n",
    "\n",
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.randomForest\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    makeIntegerParam(\"ntree\",lower = 50, upper = 500),\n",
    "    makeIntegerParam(\"mtry\", lower = 3, upper = 10),\n",
    "    makeIntegerParam(\"nodesize\", lower = 10, upper = 50)\n",
    ")\n",
    "ctrl = makeTuneControlRandom(maxit = 100L)\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: auc.test.mean=0.7859184\n",
    "* ntree=405\n",
    "* mtry=6 \n",
    "* nodesize=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.23\n",
       "  prob.0 prob.1 response\n",
       "1  0.812  0.188        0\n",
       "2  0.752  0.248        0\n",
       "3  0.466  0.534        1\n",
       "4  0.804  0.196        0\n",
       "5  0.696  0.304        0\n",
       "6  0.698  0.302        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Random Forest/RF_1_random.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Random Forest with specified tuning parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified some parameters resulting in 64 possible combinaisons. As it's not a huge number, we let the control grid try every possible combinaisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune] Started tuning learner classif.randomForest for parameter set:\n",
      "             Type len Def          Constr Req Tunable Trafo\n",
      "ntree    discrete   -   - 350,400,450,500   -    TRUE     -\n",
      "mtry     discrete   -   -         5,6,7,8   -    TRUE     -\n",
      "nodesize discrete   -   -     15,20,25,30   -    TRUE     -\n",
      "With control class: TuneControlGrid\n",
      "Imputation value: -0\n",
      "[Tune-x] 1: ntree=350; mtry=5; nodesize=15\n",
      "[Tune-y] 1: auc.test.mean=0.7849932; time: 0.9 min\n",
      "[Tune-x] 2: ntree=400; mtry=5; nodesize=15\n",
      "[Tune-y] 2: auc.test.mean=0.7836117; time: 1.1 min\n",
      "[Tune-x] 3: ntree=450; mtry=5; nodesize=15\n",
      "[Tune-y] 3: auc.test.mean=0.7848818; time: 1.3 min\n",
      "[Tune-x] 4: ntree=500; mtry=5; nodesize=15\n",
      "[Tune-y] 4: auc.test.mean=0.7842724; time: 1.4 min\n",
      "[Tune-x] 5: ntree=350; mtry=6; nodesize=15\n",
      "[Tune-y] 5: auc.test.mean=0.7824126; time: 1.0 min\n",
      "[Tune-x] 6: ntree=400; mtry=6; nodesize=15\n",
      "[Tune-y] 6: auc.test.mean=0.7838696; time: 1.2 min\n",
      "[Tune-x] 7: ntree=450; mtry=6; nodesize=15\n",
      "[Tune-y] 7: auc.test.mean=0.7809186; time: 1.3 min\n",
      "[Tune-x] 8: ntree=500; mtry=6; nodesize=15\n",
      "[Tune-y] 8: auc.test.mean=0.7830037; time: 1.4 min\n",
      "[Tune-x] 9: ntree=350; mtry=7; nodesize=15\n",
      "[Tune-y] 9: auc.test.mean=0.7852145; time: 1.0 min\n",
      "[Tune-x] 10: ntree=400; mtry=7; nodesize=15\n",
      "[Tune-y] 10: auc.test.mean=0.7857166; time: 1.2 min\n",
      "[Tune-x] 11: ntree=450; mtry=7; nodesize=15\n",
      "[Tune-y] 11: auc.test.mean=0.7835536; time: 1.3 min\n",
      "[Tune-x] 12: ntree=500; mtry=7; nodesize=15\n",
      "[Tune-y] 12: auc.test.mean=0.7840944; time: 1.5 min\n",
      "[Tune-x] 13: ntree=350; mtry=8; nodesize=15\n",
      "[Tune-y] 13: auc.test.mean=0.7824373; time: 1.0 min\n",
      "[Tune-x] 14: ntree=400; mtry=8; nodesize=15\n",
      "[Tune-y] 14: auc.test.mean=0.7840276; time: 1.2 min\n",
      "[Tune-x] 15: ntree=450; mtry=8; nodesize=15\n",
      "[Tune-y] 15: auc.test.mean=0.7849177; time: 1.3 min\n",
      "[Tune-x] 16: ntree=500; mtry=8; nodesize=15\n",
      "[Tune-y] 16: auc.test.mean=0.7855040; time: 1.5 min\n",
      "[Tune-x] 17: ntree=350; mtry=5; nodesize=20\n",
      "[Tune-y] 17: auc.test.mean=0.7738692; time: 1.0 min\n",
      "[Tune-x] 18: ntree=400; mtry=5; nodesize=20\n",
      "[Tune-y] 18: auc.test.mean=0.7858189; time: 1.1 min\n",
      "[Tune-x] 19: ntree=450; mtry=5; nodesize=20\n",
      "[Tune-y] 19: auc.test.mean=0.7790149; time: 1.2 min\n",
      "[Tune-x] 20: ntree=500; mtry=5; nodesize=20\n",
      "[Tune-y] 20: auc.test.mean=0.7790168; time: 1.4 min\n",
      "[Tune-x] 21: ntree=350; mtry=6; nodesize=20\n",
      "[Tune-y] 21: auc.test.mean=0.7801063; time: 1.0 min\n",
      "[Tune-x] 22: ntree=400; mtry=6; nodesize=20\n",
      "[Tune-y] 22: auc.test.mean=0.7832478; time: 1.2 min\n",
      "[Tune-x] 23: ntree=450; mtry=6; nodesize=20\n",
      "[Tune-y] 23: auc.test.mean=0.7797246; time: 1.3 min\n",
      "[Tune-x] 24: ntree=500; mtry=6; nodesize=20\n",
      "[Tune-y] 24: auc.test.mean=0.7822406; time: 1.4 min\n",
      "[Tune-x] 25: ntree=350; mtry=7; nodesize=20\n",
      "[Tune-y] 25: auc.test.mean=0.7848387; time: 1.0 min\n",
      "[Tune-x] 26: ntree=400; mtry=7; nodesize=20\n",
      "[Tune-y] 26: auc.test.mean=0.7844502; time: 1.2 min\n",
      "[Tune-x] 27: ntree=450; mtry=7; nodesize=20\n",
      "[Tune-y] 27: auc.test.mean=0.7857295; time: 1.3 min\n",
      "[Tune-x] 28: ntree=500; mtry=7; nodesize=20\n",
      "[Tune-y] 28: auc.test.mean=0.7834188; time: 1.5 min\n",
      "[Tune-x] 29: ntree=350; mtry=8; nodesize=20\n",
      "[Tune-y] 29: auc.test.mean=0.7846796; time: 1.1 min\n",
      "[Tune-x] 30: ntree=400; mtry=8; nodesize=20\n",
      "[Tune-y] 30: auc.test.mean=0.7853134; time: 1.2 min\n",
      "[Tune-x] 31: ntree=450; mtry=8; nodesize=20\n",
      "[Tune-y] 31: auc.test.mean=0.7832639; time: 1.3 min\n",
      "[Tune-x] 32: ntree=500; mtry=8; nodesize=20\n",
      "[Tune-y] 32: auc.test.mean=0.7814800; time: 1.4 min\n",
      "[Tune-x] 33: ntree=350; mtry=5; nodesize=25\n",
      "[Tune-y] 33: auc.test.mean=0.7822927; time: 0.9 min\n",
      "[Tune-x] 34: ntree=400; mtry=5; nodesize=25\n",
      "[Tune-y] 34: auc.test.mean=0.7814591; time: 1.1 min\n",
      "[Tune-x] 35: ntree=450; mtry=5; nodesize=25\n",
      "[Tune-y] 35: auc.test.mean=0.7803131; time: 1.2 min\n",
      "[Tune-x] 36: ntree=500; mtry=5; nodesize=25\n",
      "[Tune-y] 36: auc.test.mean=0.7770644; time: 1.3 min\n",
      "[Tune-x] 37: ntree=350; mtry=6; nodesize=25\n",
      "[Tune-y] 37: auc.test.mean=0.7799229; time: 1.0 min\n",
      "[Tune-x] 38: ntree=400; mtry=6; nodesize=25\n",
      "[Tune-y] 38: auc.test.mean=0.7790551; time: 1.1 min\n",
      "[Tune-x] 39: ntree=450; mtry=6; nodesize=25\n",
      "[Tune-y] 39: auc.test.mean=0.7727423; time: 1.2 min\n",
      "[Tune-x] 40: ntree=500; mtry=6; nodesize=25\n",
      "[Tune-y] 40: auc.test.mean=0.7863184; time: 1.4 min\n",
      "[Tune-x] 41: ntree=350; mtry=7; nodesize=25\n",
      "[Tune-y] 41: auc.test.mean=0.7753037; time: 1.0 min\n",
      "[Tune-x] 42: ntree=400; mtry=7; nodesize=25\n",
      "[Tune-y] 42: auc.test.mean=0.7809139; time: 1.1 min\n",
      "[Tune-x] 43: ntree=450; mtry=7; nodesize=25\n",
      "[Tune-y] 43: auc.test.mean=0.7837197; time: 1.3 min\n",
      "[Tune-x] 44: ntree=500; mtry=7; nodesize=25\n",
      "[Tune-y] 44: auc.test.mean=0.7798799; time: 1.4 min\n",
      "[Tune-x] 45: ntree=350; mtry=8; nodesize=25\n",
      "[Tune-y] 45: auc.test.mean=0.7773737; time: 1.0 min\n",
      "[Tune-x] 46: ntree=400; mtry=8; nodesize=25\n",
      "[Tune-y] 46: auc.test.mean=0.7863417; time: 1.1 min\n",
      "[Tune-x] 47: ntree=450; mtry=8; nodesize=25\n",
      "[Tune-y] 47: auc.test.mean=0.7857473; time: 1.3 min\n",
      "[Tune-x] 48: ntree=500; mtry=8; nodesize=25\n",
      "[Tune-y] 48: auc.test.mean=0.7798379; time: 1.4 min\n",
      "[Tune-x] 49: ntree=350; mtry=5; nodesize=30\n",
      "[Tune-y] 49: auc.test.mean=0.7837284; time: 0.9 min\n",
      "[Tune-x] 50: ntree=400; mtry=5; nodesize=30\n",
      "[Tune-y] 50: auc.test.mean=0.7757232; time: 1.0 min\n",
      "[Tune-x] 51: ntree=450; mtry=5; nodesize=30\n",
      "[Tune-y] 51: auc.test.mean=0.7804518; time: 1.2 min\n",
      "[Tune-x] 52: ntree=500; mtry=5; nodesize=30\n",
      "[Tune-y] 52: auc.test.mean=0.7787895; time: 1.3 min\n",
      "[Tune-x] 53: ntree=350; mtry=6; nodesize=30\n",
      "[Tune-y] 53: auc.test.mean=0.7777078; time: 0.9 min\n",
      "[Tune-x] 54: ntree=400; mtry=6; nodesize=30\n",
      "[Tune-y] 54: auc.test.mean=0.7751537; time: 1.1 min\n",
      "[Tune-x] 55: ntree=450; mtry=6; nodesize=30\n",
      "[Tune-y] 55: auc.test.mean=0.7789615; time: 1.2 min\n",
      "[Tune-x] 56: ntree=500; mtry=6; nodesize=30\n",
      "[Tune-y] 56: auc.test.mean=0.7809657; time: 1.3 min\n",
      "[Tune-x] 57: ntree=350; mtry=7; nodesize=30\n",
      "[Tune-y] 57: auc.test.mean=0.7791283; time: 1.0 min\n",
      "[Tune-x] 58: ntree=400; mtry=7; nodesize=30\n",
      "[Tune-y] 58: auc.test.mean=0.7781445; time: 1.1 min\n",
      "[Tune-x] 59: ntree=450; mtry=7; nodesize=30\n",
      "[Tune-y] 59: auc.test.mean=0.7777194; time: 1.2 min\n",
      "[Tune-x] 60: ntree=500; mtry=7; nodesize=30\n",
      "[Tune-y] 60: auc.test.mean=0.7759698; time: 1.4 min\n",
      "[Tune-x] 61: ntree=350; mtry=8; nodesize=30\n",
      "[Tune-y] 61: auc.test.mean=0.7812450; time: 1.0 min\n",
      "[Tune-x] 62: ntree=400; mtry=8; nodesize=30\n",
      "[Tune-y] 62: auc.test.mean=0.7843683; time: 1.1 min\n",
      "[Tune-x] 63: ntree=450; mtry=8; nodesize=30\n",
      "[Tune-y] 63: auc.test.mean=0.7810348; time: 1.3 min\n",
      "[Tune-x] 64: ntree=500; mtry=8; nodesize=30\n",
      "[Tune-y] 64: auc.test.mean=0.7822861; time: 1.4 min\n",
      "[Tune] Result: ntree=400; mtry=8; nodesize=25 : auc.test.mean=0.7863417\n"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "options(warn=-1)\n",
    "\n",
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.randomForest\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    makeDiscreteParam(\"ntree\", value = c(350, 400, 450, 500)),\n",
    "    makeDiscreteParam(\"mtry\", value = c(5,6,7,8)),\n",
    "    makeDiscreteParam(\"nodesize\", value = c(15,20,25,30))\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: auc.test.mean=0.7863417\n",
    "* ntree=400\n",
    "* mtry=8 \n",
    "* nodesize=25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.17\n",
       "  prob.0 prob.1 response\n",
       "1  0.790  0.210        0\n",
       "2  0.768  0.232        0\n",
       "3  0.464  0.536        1\n",
       "4  0.800  0.200        0\n",
       "5  0.686  0.314        0\n",
       "6  0.730  0.270        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Random Forest/RF_1_specified.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Support Vector Machines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute a Support Vector Machines algorithm using the \"C\" and Sigma parameters as tuning with each a vector of 4 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we only use specified parameters and don't use the random search method as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune] Started tuning learner classif.ksvm for parameter set:\n",
      "          Type len Def                   Constr Req Tunable Trafo\n",
      "C     discrete   -   - 0.00390625,0.0625,0.25,1   -    TRUE     -\n",
      "sigma discrete   -   -   0.00390625,0.0625,1,16   -    TRUE     -\n",
      "With control class: TuneControlGrid\n",
      "Imputation value: -0\n",
      "[Tune-x] 1: C=0.00390625; sigma=0.00390625\n",
      "[Tune-y] 1: auc.test.mean=0.7125595; time: 0.7 min\n",
      "[Tune-x] 2: C=0.0625; sigma=0.00390625\n",
      "[Tune-y] 2: auc.test.mean=0.7312467; time: 0.8 min\n",
      "[Tune-x] 3: C=0.25; sigma=0.00390625\n",
      "[Tune-y] 3: auc.test.mean=0.7204375; time: 0.9 min\n",
      "[Tune-x] 4: C=1; sigma=0.00390625\n",
      "[Tune-y] 4: auc.test.mean=0.7064651; time: 1.0 min\n",
      "[Tune-x] 5: C=0.00390625; sigma=0.0625\n",
      "[Tune-y] 5: auc.test.mean=0.7215510; time: 0.7 min\n",
      "[Tune-x] 6: C=0.0625; sigma=0.0625\n",
      "[Tune-y] 6: auc.test.mean=0.7269083; time: 0.9 min\n",
      "[Tune-x] 7: C=0.25; sigma=0.0625\n",
      "[Tune-y] 7: auc.test.mean=0.7270716; time: 0.9 min\n",
      "[Tune-x] 8: C=1; sigma=0.0625\n",
      "[Tune-y] 8: auc.test.mean=0.7239400; time: 1.0 min\n",
      "[Tune-x] 9: C=0.00390625; sigma=1\n",
      "[Tune-y] 9: auc.test.mean=0.6893410; time: 1.2 min\n",
      "[Tune-x] 10: C=0.0625; sigma=1\n",
      "[Tune-y] 10: auc.test.mean=0.7073704; time: 3.2 min\n",
      "[Tune-x] 11: C=0.25; sigma=1\n",
      "[Tune-y] 11: auc.test.mean=0.7076541; time: 3.5 min\n",
      "[Tune-x] 12: C=1; sigma=1\n",
      "[Tune-y] 12: auc.test.mean=0.7077265; time: 4.0 min\n",
      "[Tune-x] 13: C=0.00390625; sigma=16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of iterations reached 0.0003721085 0.0003675106maximum number of iterations reached 0.001212506 0.001175038maximum number of iterations reached 0.0006623332 0.000647217maximum number of iterations reached 0.001073277 0.001042444maximum number of iterations reached 0.0007306673 0.000716231maximum number of iterations reached 0.0007113655 0.0006966653maximum number of iterations reached 0.0009906305 0.000963609maximum number of iterations reached 0.000370054 0.0003654973maximum number of iterations reached 0.0007563655 0.000740203maximum number of iterations reached 0.0007290808 0.0007142691"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-y] 13: auc.test.mean=0.6455570; time: 1.0 min\n",
      "[Tune-x] 14: C=0.0625; sigma=16\n",
      "[Tune-y] 14: auc.test.mean=0.6680935; time: 3.0 min\n",
      "[Tune-x] 15: C=0.25; sigma=16\n",
      "[Tune-y] 15: auc.test.mean=0.6686167; time: 3.5 min\n",
      "[Tune-x] 16: C=1; sigma=16\n",
      "[Tune-y] 16: auc.test.mean=0.6689628; time: 3.8 min\n",
      "[Tune] Result: C=0.0625; sigma=0.00390625 : auc.test.mean=0.7312467\n"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "options(warn=-1)\n",
    "\n",
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "# Define the model\n",
    "learner <- makeLearner(\"classif.ksvm\", predict.type=\"prob\", fix.factors.prediction=T)\n",
    "\n",
    "# Define the task\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    makeDiscreteParam(\"C\", values = 2^c(-8,-4,-2,0)), #cost parameters\n",
    "    makeDiscreteParam(\"sigma\", values = 2^c(-8,-4,0,4)) #RBF Kernel Parameter\n",
    ")\n",
    "ctrl = makeTuneControlGrid()\n",
    "\n",
    "# Run the hyper parameter tuning with k-fold CV\n",
    "if (length(tune_params$pars) > 0) {\n",
    "    # Run parameter tuning\n",
    "    res <- tuneParams(learner, task=train_task, resampling=rdesc,\n",
    "      par.set=tune_params, control=ctrl, measures=list(mlr::auc))\n",
    "    \n",
    "    # Extract best model\n",
    "    best_learner <- res$learner\n",
    "    \n",
    "} else {\n",
    "    # Simple cross-validation\n",
    "    res <- resample(learner, train_task, rdesc, measures=list(mlr::auc))\n",
    "    \n",
    "    # No parameter for tuning, only 1 best learner\n",
    "    best_learner <- learner\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: auc.test.mean=0.7312467\n",
    "* C=0.0625\n",
    "* sigma=0.00390625\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 0.48\n",
       "     prob.0    prob.1 response\n",
       "1 0.9001657 0.0998343        0\n",
       "2 0.8972640 0.1027360        0\n",
       "3 0.4235989 0.5764011        1\n",
       "4 0.8949203 0.1050797        0\n",
       "5 0.8937147 0.1062853        0\n",
       "6 0.8942526 0.1057474        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/SVM/SVM_1_simple.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Other algorithms and Bagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is an extra part which is not included in the report. Here we focus on trying different algorithms that are available in the MLR package. Moreover, we try to compoute bagging of few algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more overall definition, this part is a playground where we try techniques in modeling part and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we try the Model Multiplexer function of MLR. This allow us in one run to tune parameters of a model, but also to try many models at the same time. Here, we choosed KNN, PlsdaCaret, Earth, ctree, C50 and Boosting algorithm in order to predict probabilities. For each algorithm which have tuning parameters, we specified ranges of value and we select randomly those value. The function will try many different values for different models and will return the best algorithm between the 6 selected with the best tuned parameters that the function tried. Here, we specify again 250 iterations un order to obtain relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: kknn\n",
      "\n",
      "Attaching package: 'kknn'\n",
      "\n",
      "The following object is masked from 'package:caret':\n",
      "\n",
      "    contr.dummy\n",
      "\n",
      "[Tune] Started tuning learner ModelMultiplexer for parameter set:\n",
      "                                  Type len Def\n",
      "selected.learner              discrete   -   -\n",
      "classif.kknn.k                 integer   -   -\n",
      "classif.plsdaCaret.probMethod discrete   -   -\n",
      "classif.plsdaCaret.ncomp       integer   -   -\n",
      "classif.ctree.mincriterion     numeric   -   -\n",
      "classif.ctree.minsplit         integer   -   -\n",
      "classif.ctree.minbucket        integer   -   -\n",
      "classif.C50.CF                 numeric   -   -\n",
      "classif.C50.minCases           integer   -   -\n",
      "classif.boosting.mfinal        integer   -   -\n",
      "                                                                Constr Req\n",
      "selected.learner              classif.kknn,classif.plsdaCaret,class...   -\n",
      "classif.kknn.k                                            500 to 1e+03   Y\n",
      "classif.plsdaCaret.probMethod                                  softmax   Y\n",
      "classif.plsdaCaret.ncomp                                       1 to 15   Y\n",
      "classif.ctree.mincriterion                                    0.8 to 1   Y\n",
      "classif.ctree.minsplit                                        10 to 40   Y\n",
      "classif.ctree.minbucket                                        5 to 10   Y\n",
      "classif.C50.CF                                                  0 to 1   Y\n",
      "classif.C50.minCases                                           2 to 20   Y\n",
      "classif.boosting.mfinal                                      50 to 300   Y\n",
      "                              Tunable Trafo\n",
      "selected.learner                 TRUE     -\n",
      "classif.kknn.k                   TRUE     -\n",
      "classif.plsdaCaret.probMethod    TRUE     -\n",
      "classif.plsdaCaret.ncomp         TRUE     -\n",
      "classif.ctree.mincriterion       TRUE     -\n",
      "classif.ctree.minsplit           TRUE     -\n",
      "classif.ctree.minbucket          TRUE     -\n",
      "classif.C50.CF                   TRUE     -\n",
      "classif.C50.minCases             TRUE     -\n",
      "classif.boosting.mfinal          TRUE     -\n",
      "With control class: TuneControlRandom\n",
      "Imputation value: -0\n",
      "[Tune-x] 1: selected.learner=classif.boos...; mfinal=169\n",
      "[Tune-y] 1: auc.test.mean=0.7868010; time: 11.5 min\n",
      "[Tune-x] 2: selected.learner=classif.C50; CF=0.106; minCases=12\n",
      "[Tune-y] 2: auc.test.mean=0.6277728; time: 0.1 min\n",
      "[Tune-x] 3: selected.learner=classif.C50; CF=0.294; minCases=9\n",
      "[Tune-y] 3: auc.test.mean=0.7136285; time: 0.1 min\n",
      "[Tune-x] 4: selected.learner=classif.ctree; mincriterion=0.842; minsplit=37; minbucket=7\n",
      "[Tune-y] 4: auc.test.mean=0.7555477; time: 0.0 min\n",
      "[Tune-x] 5: selected.learner=classif.C50; CF=0.787; minCases=11\n",
      "[Tune-y] 5: auc.test.mean=0.7146379; time: 0.1 min\n",
      "[Tune-x] 6: selected.learner=classif.C50; CF=0.364; minCases=18\n",
      "[Tune-y] 6: auc.test.mean=0.6762553; time: 0.1 min\n",
      "[Tune-x] 7: selected.learner=classif.kknn; k=803\n",
      "[Tune-y] 7: auc.test.mean=0.7884661; time: 0.1 min\n",
      "[Tune-x] 8: selected.learner=classif.C50; CF=0.851; minCases=15\n",
      "[Tune-y] 8: auc.test.mean=0.7146536; time: 0.1 min\n",
      "[Tune-x] 9: selected.learner=classif.ctree; mincriterion=0.813; minsplit=31; minbucket=10\n",
      "[Tune-y] 9: auc.test.mean=0.7592218; time: 0.0 min\n",
      "[Tune-x] 10: selected.learner=classif.kknn; k=972\n",
      "[Tune-y] 10: auc.test.mean=0.7884663; time: 0.1 min\n",
      "[Tune-x] 11: selected.learner=classif.boos...; mfinal=60\n",
      "[Tune-y] 11: auc.test.mean=0.7818501; time: 3.7 min\n",
      "[Tune-x] 12: selected.learner=classif.boos...; mfinal=51\n",
      "[Tune-y] 12: auc.test.mean=0.7840119; time: 3.5 min\n",
      "[Tune-x] 13: selected.learner=classif.ctree; mincriterion=0.87; minsplit=32; minbucket=9\n",
      "[Tune-y] 13: auc.test.mean=0.7573815; time: 0.0 min\n",
      "[Tune-x] 14: selected.learner=classif.boos...; mfinal=129\n",
      "[Tune-y] 14: auc.test.mean=0.7866575; time: 9.9 min\n",
      "[Tune-x] 15: selected.learner=classif.kknn; k=982\n",
      "[Tune-y] 15: auc.test.mean=0.7884548; time: 0.2 min\n",
      "[Tune-x] 16: selected.learner=classif.ctree; mincriterion=0.924; minsplit=27; minbucket=9\n",
      "[Tune-y] 16: auc.test.mean=0.7577156; time: 0.0 min\n",
      "[Tune-x] 17: selected.learner=classif.C50; CF=0.35; minCases=8\n",
      "[Tune-y] 17: auc.test.mean=0.7126660; time: 0.1 min\n",
      "[Tune-x] 18: selected.learner=classif.ctree; mincriterion=0.893; minsplit=12; minbucket=9\n",
      "[Tune-y] 18: auc.test.mean=0.7579595; time: 0.0 min\n",
      "[Tune-x] 19: selected.learner=classif.ctree; mincriterion=0.836; minsplit=20; minbucket=5\n",
      "[Tune-y] 19: auc.test.mean=0.7579106; time: 0.0 min\n",
      "[Tune-x] 20: selected.learner=classif.ctree; mincriterion=0.959; minsplit=36; minbucket=6\n",
      "[Tune-y] 20: auc.test.mean=0.7571533; time: 0.0 min\n",
      "[Tune-x] 21: selected.learner=classif.ctree; mincriterion=0.851; minsplit=28; minbucket=6\n",
      "[Tune-y] 21: auc.test.mean=0.7551114; time: 0.0 min\n",
      "[Tune-x] 22: selected.learner=classif.ctree; mincriterion=0.844; minsplit=27; minbucket=9\n",
      "[Tune-y] 22: auc.test.mean=0.7576253; time: 0.0 min\n",
      "[Tune-x] 23: selected.learner=classif.ctree; mincriterion=0.963; minsplit=26; minbucket=6\n",
      "[Tune-y] 23: auc.test.mean=0.7571533; time: 0.0 min\n",
      "[Tune-x] 24: selected.learner=classif.ctree; mincriterion=0.83; minsplit=25; minbucket=9\n",
      "[Tune-y] 24: auc.test.mean=0.7585262; time: 0.0 min\n",
      "[Tune-x] 25: selected.learner=classif.C50; CF=0.169; minCases=4\n",
      "[Tune-y] 25: auc.test.mean=0.7153687; time: 0.1 min\n",
      "[Tune-x] 26: selected.learner=classif.boos...; mfinal=291\n",
      "[Tune-y] 26: auc.test.mean=0.7860610; time: 21.8 min\n",
      "[Tune-x] 27: selected.learner=classif.C50; CF=0.866; minCases=17\n",
      "[Tune-y] 27: auc.test.mean=0.7007216; time: 0.1 min\n",
      "[Tune-x] 28: selected.learner=classif.ctree; mincriterion=0.983; minsplit=28; minbucket=6\n",
      "[Tune-y] 28: auc.test.mean=0.7565140; time: 0.0 min\n",
      "[Tune-x] 29: selected.learner=classif.C50; CF=0.137; minCases=4\n",
      "[Tune-y] 29: auc.test.mean=0.6792513; time: 0.1 min\n",
      "[Tune-x] 30: selected.learner=classif.boos...; mfinal=108\n",
      "[Tune-y] 30: auc.test.mean=0.7860072; time: 8.4 min\n",
      "[Tune-x] 31: selected.learner=classif.ctree; mincriterion=0.93; minsplit=35; minbucket=8\n",
      "[Tune-y] 31: auc.test.mean=0.7576356; time: 0.0 min\n",
      "[Tune-x] 32: selected.learner=classif.kknn; k=633\n",
      "[Tune-y] 32: auc.test.mean=0.7883842; time: 0.1 min\n",
      "[Tune-x] 33: selected.learner=classif.ctree; mincriterion=0.881; minsplit=18; minbucket=5\n",
      "[Tune-y] 33: auc.test.mean=0.7573077; time: 0.0 min\n",
      "[Tune-x] 34: selected.learner=classif.ctree; mincriterion=0.885; minsplit=22; minbucket=8\n",
      "[Tune-y] 34: auc.test.mean=0.7581372; time: 0.0 min\n",
      "[Tune-x] 35: selected.learner=classif.ctree; mincriterion=0.937; minsplit=37; minbucket=8\n",
      "[Tune-y] 35: auc.test.mean=0.7569688; time: 0.0 min\n",
      "[Tune-x] 36: selected.learner=classif.boos...; mfinal=86\n",
      "[Tune-y] 36: auc.test.mean=0.7853633; time: 6.4 min\n",
      "[Tune-x] 37: selected.learner=classif.plsd...; probMethod=softmax; ncomp=1\n",
      "[Tune-y] 37: auc.test.mean=0.5766518; time: 0.0 min\n",
      "[Tune-x] 38: selected.learner=classif.boos...; mfinal=126\n",
      "[Tune-y] 38: auc.test.mean=0.7857187; time: 9.9 min\n",
      "[Tune-x] 39: selected.learner=classif.C50; CF=0.816; minCases=12\n",
      "[Tune-y] 39: auc.test.mean=0.7147596; time: 0.1 min\n",
      "[Tune-x] 40: selected.learner=classif.C50; CF=0.476; minCases=4\n",
      "[Tune-y] 40: auc.test.mean=0.7197322; time: 0.1 min\n",
      "[Tune-x] 41: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 41: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 42: selected.learner=classif.boos...; mfinal=281\n",
      "[Tune-y] 42: auc.test.mean=0.7867817; time: 18.6 min\n",
      "[Tune-x] 43: selected.learner=classif.C50; CF=0.135; minCases=13\n",
      "[Tune-y] 43: auc.test.mean=0.6394490; time: 0.1 min\n",
      "[Tune-x] 44: selected.learner=classif.kknn; k=645\n",
      "[Tune-y] 44: auc.test.mean=0.7883874; time: 0.1 min\n",
      "[Tune-x] 45: selected.learner=classif.boos...; mfinal=178\n",
      "[Tune-y] 45: auc.test.mean=0.7848510; time: 11.4 min\n",
      "[Tune-x] 46: selected.learner=classif.plsd...; probMethod=softmax; ncomp=3\n",
      "[Tune-y] 46: auc.test.mean=0.7500435; time: 0.0 min\n",
      "[Tune-x] 47: selected.learner=classif.ctree; mincriterion=0.818; minsplit=10; minbucket=6\n",
      "[Tune-y] 47: auc.test.mean=0.7560508; time: 0.0 min\n",
      "[Tune-x] 48: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 48: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 49: selected.learner=classif.kknn; k=837\n",
      "[Tune-y] 49: auc.test.mean=0.7884101; time: 0.1 min\n",
      "[Tune-x] 50: selected.learner=classif.plsd...; probMethod=softmax; ncomp=13\n",
      "[Tune-y] 50: auc.test.mean=0.7871003; time: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-x] 51: selected.learner=classif.plsd...; probMethod=softmax; ncomp=10\n",
      "[Tune-y] 51: auc.test.mean=0.7815264; time: 0.0 min\n",
      "[Tune-x] 52: selected.learner=classif.C50; CF=0.39; minCases=2\n",
      "[Tune-y] 52: auc.test.mean=0.7138605; time: 0.1 min\n",
      "[Tune-x] 53: selected.learner=classif.C50; CF=0.342; minCases=13\n",
      "[Tune-y] 53: auc.test.mean=0.7144613; time: 0.1 min\n",
      "[Tune-x] 54: selected.learner=classif.kknn; k=998\n",
      "[Tune-y] 54: auc.test.mean=0.7884535; time: 0.1 min\n",
      "[Tune-x] 55: selected.learner=classif.kknn; k=974\n",
      "[Tune-y] 55: auc.test.mean=0.7884621; time: 0.1 min\n",
      "[Tune-x] 56: selected.learner=classif.C50; CF=0.303; minCases=18\n",
      "[Tune-y] 56: auc.test.mean=0.6762553; time: 0.1 min\n",
      "[Tune-x] 57: selected.learner=classif.plsd...; probMethod=softmax; ncomp=14\n",
      "[Tune-y] 57: auc.test.mean=0.7844433; time: 0.0 min\n",
      "[Tune-x] 58: selected.learner=classif.boos...; mfinal=153\n",
      "[Tune-y] 58: auc.test.mean=0.7876320; time: 10.4 min\n",
      "[Tune-x] 59: selected.learner=classif.kknn; k=763\n",
      "[Tune-y] 59: auc.test.mean=0.7884527; time: 0.1 min\n",
      "[Tune-x] 60: selected.learner=classif.ctree; mincriterion=0.873; minsplit=23; minbucket=5\n",
      "[Tune-y] 60: auc.test.mean=0.7573077; time: 0.0 min\n",
      "[Tune-x] 61: selected.learner=classif.boos...; mfinal=249\n",
      "[Tune-y] 61: auc.test.mean=0.7855385; time: 15.7 min\n",
      "[Tune-x] 62: selected.learner=classif.ctree; mincriterion=0.971; minsplit=20; minbucket=7\n",
      "[Tune-y] 62: auc.test.mean=0.7571585; time: 0.0 min\n",
      "[Tune-x] 63: selected.learner=classif.kknn; k=959\n",
      "[Tune-y] 63: auc.test.mean=0.7884798; time: 0.1 min\n",
      "[Tune-x] 64: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 64: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 65: selected.learner=classif.plsd...; probMethod=softmax; ncomp=1\n",
      "[Tune-y] 65: auc.test.mean=0.5766518; time: 0.0 min\n",
      "[Tune-x] 66: selected.learner=classif.boos...; mfinal=171\n",
      "[Tune-y] 66: auc.test.mean=0.7866067; time: 10.9 min\n",
      "[Tune-x] 67: selected.learner=classif.plsd...; probMethod=softmax; ncomp=3\n",
      "[Tune-y] 67: auc.test.mean=0.7500435; time: 0.0 min\n",
      "[Tune-x] 68: selected.learner=classif.C50; CF=0.322; minCases=7\n",
      "[Tune-y] 68: auc.test.mean=0.7136949; time: 0.1 min\n",
      "[Tune-x] 69: selected.learner=classif.plsd...; probMethod=softmax; ncomp=10\n",
      "[Tune-y] 69: auc.test.mean=0.7815264; time: 0.0 min\n",
      "[Tune-x] 70: selected.learner=classif.boos...; mfinal=105\n",
      "[Tune-y] 70: auc.test.mean=0.7808495; time: 6.4 min\n",
      "[Tune-x] 71: selected.learner=classif.kknn; k=870\n",
      "[Tune-y] 71: auc.test.mean=0.7884450; time: 0.1 min\n",
      "[Tune-x] 72: selected.learner=classif.plsd...; probMethod=softmax; ncomp=6\n",
      "[Tune-y] 72: auc.test.mean=0.7492430; time: 0.0 min\n",
      "[Tune-x] 73: selected.learner=classif.boos...; mfinal=126\n",
      "[Tune-y] 73: auc.test.mean=0.7841771; time: 7.2 min\n",
      "[Tune-x] 74: selected.learner=classif.kknn; k=800\n",
      "[Tune-y] 74: auc.test.mean=0.7884261; time: 0.1 min\n",
      "[Tune-x] 75: selected.learner=classif.ctree; mincriterion=0.86; minsplit=27; minbucket=6\n",
      "[Tune-y] 75: auc.test.mean=0.7572154; time: 0.0 min\n",
      "[Tune-x] 76: selected.learner=classif.ctree; mincriterion=0.916; minsplit=39; minbucket=5\n",
      "[Tune-y] 76: auc.test.mean=0.7580247; time: 0.0 min\n",
      "[Tune-x] 77: selected.learner=classif.ctree; mincriterion=0.821; minsplit=14; minbucket=5\n",
      "[Tune-y] 77: auc.test.mean=0.7558099; time: 0.0 min\n",
      "[Tune-x] 78: selected.learner=classif.ctree; mincriterion=0.971; minsplit=17; minbucket=10\n",
      "[Tune-y] 78: auc.test.mean=0.7570790; time: 0.0 min\n",
      "[Tune-x] 79: selected.learner=classif.C50; CF=0.377; minCases=13\n",
      "[Tune-y] 79: auc.test.mean=0.7145044; time: 0.1 min\n",
      "[Tune-x] 80: selected.learner=classif.ctree; mincriterion=0.928; minsplit=23; minbucket=9\n",
      "[Tune-y] 80: auc.test.mean=0.7576292; time: 0.0 min\n",
      "[Tune-x] 81: selected.learner=classif.ctree; mincriterion=0.976; minsplit=15; minbucket=10\n",
      "[Tune-y] 81: auc.test.mean=0.7568734; time: 0.0 min\n",
      "[Tune-x] 82: selected.learner=classif.kknn; k=631\n",
      "[Tune-y] 82: auc.test.mean=0.7883945; time: 0.1 min\n",
      "[Tune-x] 83: selected.learner=classif.boos...; mfinal=208\n",
      "[Tune-y] 83: auc.test.mean=0.7858281; time: 11.9 min\n",
      "[Tune-x] 84: selected.learner=classif.kknn; k=527\n",
      "[Tune-y] 84: auc.test.mean=0.7880494; time: 0.1 min\n",
      "[Tune-x] 85: selected.learner=classif.plsd...; probMethod=softmax; ncomp=11\n",
      "[Tune-y] 85: auc.test.mean=0.7857520; time: 0.0 min\n",
      "[Tune-x] 86: selected.learner=classif.ctree; mincriterion=0.833; minsplit=17; minbucket=6\n",
      "[Tune-y] 86: auc.test.mean=0.7579844; time: 0.0 min\n",
      "[Tune-x] 87: selected.learner=classif.C50; CF=0.241; minCases=9\n",
      "[Tune-y] 87: auc.test.mean=0.7013362; time: 0.1 min\n",
      "[Tune-x] 88: selected.learner=classif.C50; CF=0.373; minCases=15\n",
      "[Tune-y] 88: auc.test.mean=0.7146536; time: 0.1 min\n",
      "[Tune-x] 89: selected.learner=classif.ctree; mincriterion=0.956; minsplit=24; minbucket=5\n",
      "[Tune-y] 89: auc.test.mean=0.7565340; time: 0.0 min\n",
      "[Tune-x] 90: selected.learner=classif.C50; CF=0.115; minCases=3\n",
      "[Tune-y] 90: auc.test.mean=0.6182981; time: 0.1 min\n",
      "[Tune-x] 91: selected.learner=classif.C50; CF=0.00326; minCases=9\n",
      "[Tune-y] 91: auc.test.mean=0.5952067; time: 0.1 min\n",
      "[Tune-x] 92: selected.learner=classif.boos...; mfinal=221\n",
      "[Tune-y] 92: auc.test.mean=0.7848094; time: 12.5 min\n",
      "[Tune-x] 93: selected.learner=classif.plsd...; probMethod=softmax; ncomp=4\n",
      "[Tune-y] 93: auc.test.mean=0.7512491; time: 0.0 min\n",
      "[Tune-x] 94: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 94: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 95: selected.learner=classif.plsd...; probMethod=softmax; ncomp=1\n",
      "[Tune-y] 95: auc.test.mean=0.5766518; time: 0.0 min\n",
      "[Tune-x] 96: selected.learner=classif.C50; CF=0.27; minCases=2\n",
      "[Tune-y] 96: auc.test.mean=0.7136787; time: 0.1 min\n",
      "[Tune-x] 97: selected.learner=classif.C50; CF=0.0381; minCases=3\n",
      "[Tune-y] 97: auc.test.mean=0.5952351; time: 0.1 min\n",
      "[Tune-x] 98: selected.learner=classif.C50; CF=0.0262; minCases=12\n",
      "[Tune-y] 98: auc.test.mean=0.5952067; time: 0.1 min\n",
      "[Tune-x] 99: selected.learner=classif.kknn; k=960\n",
      "[Tune-y] 99: auc.test.mean=0.7884809; time: 0.1 min\n",
      "[Tune-x] 100: selected.learner=classif.ctree; mincriterion=0.813; minsplit=30; minbucket=8\n",
      "[Tune-y] 100: auc.test.mean=0.7577571; time: 0.0 min\n",
      "[Tune-x] 101: selected.learner=classif.boos...; mfinal=182\n",
      "[Tune-y] 101: auc.test.mean=0.7869782; time: 10.1 min\n",
      "[Tune-x] 102: selected.learner=classif.C50; CF=0.475; minCases=2\n",
      "[Tune-y] 102: auc.test.mean=0.7138605; time: 0.1 min\n",
      "[Tune-x] 103: selected.learner=classif.boos...; mfinal=270\n",
      "[Tune-y] 103: auc.test.mean=0.7879782; time: 15.1 min\n",
      "[Tune-x] 104: selected.learner=classif.kknn; k=643\n",
      "[Tune-y] 104: auc.test.mean=0.7883728; time: 0.1 min\n",
      "[Tune-x] 105: selected.learner=classif.kknn; k=724\n",
      "[Tune-y] 105: auc.test.mean=0.7885144; time: 0.1 min\n",
      "[Tune-x] 106: selected.learner=classif.plsd...; probMethod=softmax; ncomp=5\n",
      "[Tune-y] 106: auc.test.mean=0.7511923; time: 0.0 min\n",
      "[Tune-x] 107: selected.learner=classif.C50; CF=0.397; minCases=18\n",
      "[Tune-y] 107: auc.test.mean=0.6886387; time: 0.1 min\n",
      "[Tune-x] 108: selected.learner=classif.kknn; k=517\n",
      "[Tune-y] 108: auc.test.mean=0.7878845; time: 0.1 min\n",
      "[Tune-x] 109: selected.learner=classif.plsd...; probMethod=softmax; ncomp=4\n",
      "[Tune-y] 109: auc.test.mean=0.7512491; time: 0.0 min\n",
      "[Tune-x] 110: selected.learner=classif.C50; CF=0.937; minCases=10\n",
      "[Tune-y] 110: auc.test.mean=0.7147912; time: 0.1 min\n",
      "[Tune-x] 111: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 111: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 112: selected.learner=classif.ctree; mincriterion=0.974; minsplit=10; minbucket=8\n",
      "[Tune-y] 112: auc.test.mean=0.7568734; time: 0.0 min\n",
      "[Tune-x] 113: selected.learner=classif.plsd...; probMethod=softmax; ncomp=6\n",
      "[Tune-y] 113: auc.test.mean=0.7492430; time: 0.0 min\n",
      "[Tune-x] 114: selected.learner=classif.plsd...; probMethod=softmax; ncomp=1\n",
      "[Tune-y] 114: auc.test.mean=0.5766518; time: 0.0 min\n",
      "[Tune-x] 115: selected.learner=classif.C50; CF=0.439; minCases=18\n",
      "[Tune-y] 115: auc.test.mean=0.6886387; time: 0.1 min\n",
      "[Tune-x] 116: selected.learner=classif.plsd...; probMethod=softmax; ncomp=8\n",
      "[Tune-y] 116: auc.test.mean=0.7826943; time: 0.0 min\n",
      "[Tune-x] 117: selected.learner=classif.plsd...; probMethod=softmax; ncomp=5\n",
      "[Tune-y] 117: auc.test.mean=0.7511923; time: 0.0 min\n",
      "[Tune-x] 118: selected.learner=classif.ctree; mincriterion=0.91; minsplit=26; minbucket=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-y] 118: auc.test.mean=0.7579754; time: 0.0 min\n",
      "[Tune-x] 119: selected.learner=classif.ctree; mincriterion=0.894; minsplit=19; minbucket=8\n",
      "[Tune-y] 119: auc.test.mean=0.7579595; time: 0.0 min\n",
      "[Tune-x] 120: selected.learner=classif.ctree; mincriterion=0.878; minsplit=13; minbucket=5\n",
      "[Tune-y] 120: auc.test.mean=0.7573077; time: 0.0 min\n",
      "[Tune-x] 121: selected.learner=classif.C50; CF=0.271; minCases=17\n",
      "[Tune-y] 121: auc.test.mean=0.6763987; time: 0.1 min\n",
      "[Tune-x] 122: selected.learner=classif.ctree; mincriterion=0.872; minsplit=11; minbucket=6\n",
      "[Tune-y] 122: auc.test.mean=0.7573815; time: 0.0 min\n",
      "[Tune-x] 123: selected.learner=classif.kknn; k=554\n",
      "[Tune-y] 123: auc.test.mean=0.7881570; time: 0.1 min\n",
      "[Tune-x] 124: selected.learner=classif.boos...; mfinal=280\n",
      "[Tune-y] 124: auc.test.mean=0.7860864; time: 16.5 min\n",
      "[Tune-x] 125: selected.learner=classif.boos...; mfinal=138\n",
      "[Tune-y] 125: auc.test.mean=0.7908456; time: 8.3 min\n",
      "[Tune-x] 126: selected.learner=classif.ctree; mincriterion=0.952; minsplit=35; minbucket=10\n",
      "[Tune-y] 126: auc.test.mean=0.7564664; time: 0.0 min\n",
      "[Tune-x] 127: selected.learner=classif.plsd...; probMethod=softmax; ncomp=14\n",
      "[Tune-y] 127: auc.test.mean=0.7844433; time: 0.0 min\n",
      "[Tune-x] 128: selected.learner=classif.C50; CF=0.72; minCases=6\n",
      "[Tune-y] 128: auc.test.mean=0.7187969; time: 0.1 min\n",
      "[Tune-x] 129: selected.learner=classif.ctree; mincriterion=0.969; minsplit=23; minbucket=8\n",
      "[Tune-y] 129: auc.test.mean=0.7573940; time: 0.0 min\n",
      "[Tune-x] 130: selected.learner=classif.boos...; mfinal=54\n",
      "[Tune-y] 130: auc.test.mean=0.7814446; time: 3.2 min\n",
      "[Tune-x] 131: selected.learner=classif.C50; CF=0.933; minCases=17\n",
      "[Tune-y] 131: auc.test.mean=0.7007216; time: 0.1 min\n",
      "[Tune-x] 132: selected.learner=classif.C50; CF=0.317; minCases=17\n",
      "[Tune-y] 132: auc.test.mean=0.6763987; time: 0.1 min\n",
      "[Tune-x] 133: selected.learner=classif.kknn; k=772\n",
      "[Tune-y] 133: auc.test.mean=0.7884425; time: 0.1 min\n",
      "[Tune-x] 134: selected.learner=classif.plsd...; probMethod=softmax; ncomp=1\n",
      "[Tune-y] 134: auc.test.mean=0.5766518; time: 0.0 min\n",
      "[Tune-x] 135: selected.learner=classif.plsd...; probMethod=softmax; ncomp=13\n",
      "[Tune-y] 135: auc.test.mean=0.7871003; time: 0.0 min\n",
      "[Tune-x] 136: selected.learner=classif.plsd...; probMethod=softmax; ncomp=11\n",
      "[Tune-y] 136: auc.test.mean=0.7857520; time: 0.0 min\n",
      "[Tune-x] 137: selected.learner=classif.C50; CF=0.0188; minCases=12\n",
      "[Tune-y] 137: auc.test.mean=0.5952067; time: 0.1 min\n",
      "[Tune-x] 138: selected.learner=classif.boos...; mfinal=150\n",
      "[Tune-y] 138: auc.test.mean=0.7870929; time: 10.2 min\n",
      "[Tune-x] 139: selected.learner=classif.C50; CF=0.977; minCases=7\n",
      "[Tune-y] 139: auc.test.mean=0.7187851; time: 0.1 min\n",
      "[Tune-x] 140: selected.learner=classif.kknn; k=825\n",
      "[Tune-y] 140: auc.test.mean=0.7884132; time: 0.2 min\n",
      "[Tune-x] 141: selected.learner=classif.C50; CF=0.685; minCases=17\n",
      "[Tune-y] 141: auc.test.mean=0.7007216; time: 0.1 min\n",
      "[Tune-x] 142: selected.learner=classif.C50; CF=0.631; minCases=16\n",
      "[Tune-y] 142: auc.test.mean=0.7009852; time: 0.1 min\n",
      "[Tune-x] 143: selected.learner=classif.boos...; mfinal=247\n",
      "[Tune-y] 143: auc.test.mean=0.7876479; time: 15.5 min\n",
      "[Tune-x] 144: selected.learner=classif.C50; CF=0.285; minCases=20\n",
      "[Tune-y] 144: auc.test.mean=0.6762553; time: 0.1 min\n",
      "[Tune-x] 145: selected.learner=classif.kknn; k=883\n",
      "[Tune-y] 145: auc.test.mean=0.7884122; time: 0.1 min\n",
      "[Tune-x] 146: selected.learner=classif.boos...; mfinal=150\n",
      "[Tune-y] 146: auc.test.mean=0.7892076; time: 9.1 min\n",
      "[Tune-x] 147: selected.learner=classif.kknn; k=790\n",
      "[Tune-y] 147: auc.test.mean=0.7884499; time: 0.1 min\n",
      "[Tune-x] 148: selected.learner=classif.plsd...; probMethod=softmax; ncomp=15\n",
      "[Tune-y] 148: auc.test.mean=0.7851131; time: 0.0 min\n",
      "[Tune-x] 149: selected.learner=classif.boos...; mfinal=150\n",
      "[Tune-y] 149: auc.test.mean=0.7880321; time: 8.4 min\n",
      "[Tune-x] 150: selected.learner=classif.ctree; mincriterion=0.984; minsplit=10; minbucket=9\n",
      "[Tune-y] 150: auc.test.mean=0.7547240; time: 0.0 min\n",
      "[Tune-x] 151: selected.learner=classif.C50; CF=0.179; minCases=6\n",
      "[Tune-y] 151: auc.test.mean=0.7153687; time: 0.1 min\n",
      "[Tune-x] 152: selected.learner=classif.boos...; mfinal=50\n",
      "[Tune-y] 152: auc.test.mean=0.7787529; time: 2.8 min\n",
      "[Tune-x] 153: selected.learner=classif.ctree; mincriterion=0.912; minsplit=12; minbucket=7\n",
      "[Tune-y] 153: auc.test.mean=0.7579754; time: 0.0 min\n",
      "[Tune-x] 154: selected.learner=classif.boos...; mfinal=138\n",
      "[Tune-y] 154: auc.test.mean=0.7858329; time: 7.5 min\n",
      "[Tune-x] 155: selected.learner=classif.plsd...; probMethod=softmax; ncomp=11\n",
      "[Tune-y] 155: auc.test.mean=0.7857520; time: 0.0 min\n",
      "[Tune-x] 156: selected.learner=classif.C50; CF=0.923; minCases=7\n",
      "[Tune-y] 156: auc.test.mean=0.7187851; time: 0.1 min\n",
      "[Tune-x] 157: selected.learner=classif.kknn; k=693\n",
      "[Tune-y] 157: auc.test.mean=0.7885170; time: 0.1 min\n",
      "[Tune-x] 158: selected.learner=classif.C50; CF=0.786; minCases=8\n",
      "[Tune-y] 158: auc.test.mean=0.7127874; time: 0.1 min\n",
      "[Tune-x] 159: selected.learner=classif.plsd...; probMethod=softmax; ncomp=10\n",
      "[Tune-y] 159: auc.test.mean=0.7815264; time: 0.0 min\n",
      "[Tune-x] 160: selected.learner=classif.plsd...; probMethod=softmax; ncomp=9\n",
      "[Tune-y] 160: auc.test.mean=0.7814007; time: 0.0 min\n",
      "[Tune-x] 161: selected.learner=classif.kknn; k=887\n",
      "[Tune-y] 161: auc.test.mean=0.7884079; time: 0.1 min\n",
      "[Tune-x] 162: selected.learner=classif.plsd...; probMethod=softmax; ncomp=13\n",
      "[Tune-y] 162: auc.test.mean=0.7871003; time: 0.0 min\n",
      "[Tune-x] 163: selected.learner=classif.ctree; mincriterion=0.83; minsplit=16; minbucket=10\n",
      "[Tune-y] 163: auc.test.mean=0.7599909; time: 0.0 min\n",
      "[Tune-x] 164: selected.learner=classif.plsd...; probMethod=softmax; ncomp=13\n",
      "[Tune-y] 164: auc.test.mean=0.7871003; time: 0.0 min\n",
      "[Tune-x] 165: selected.learner=classif.plsd...; probMethod=softmax; ncomp=7\n",
      "[Tune-y] 165: auc.test.mean=0.7827350; time: 0.0 min\n",
      "[Tune-x] 166: selected.learner=classif.boos...; mfinal=115\n",
      "[Tune-y] 166: auc.test.mean=0.7835393; time: 6.5 min\n",
      "[Tune-x] 167: selected.learner=classif.kknn; k=519\n",
      "[Tune-y] 167: auc.test.mean=0.7879294; time: 0.1 min\n",
      "[Tune-x] 168: selected.learner=classif.boos...; mfinal=124\n",
      "[Tune-y] 168: auc.test.mean=0.7862792; time: 6.9 min\n",
      "[Tune-x] 169: selected.learner=classif.kknn; k=926\n",
      "[Tune-y] 169: auc.test.mean=0.7884008; time: 0.1 min\n",
      "[Tune-x] 170: selected.learner=classif.plsd...; probMethod=softmax; ncomp=12\n",
      "[Tune-y] 170: auc.test.mean=0.7865138; time: 0.0 min\n",
      "[Tune-x] 171: selected.learner=classif.ctree; mincriterion=0.924; minsplit=18; minbucket=7\n",
      "[Tune-y] 171: auc.test.mean=0.7577156; time: 0.0 min\n",
      "[Tune-x] 172: selected.learner=classif.ctree; mincriterion=0.994; minsplit=16; minbucket=7\n",
      "[Tune-y] 172: auc.test.mean=0.7517226; time: 0.0 min\n",
      "[Tune-x] 173: selected.learner=classif.boos...; mfinal=268\n",
      "[Tune-y] 173: auc.test.mean=0.7883076; time: 14.7 min\n",
      "[Tune-x] 174: selected.learner=classif.boos...; mfinal=299\n",
      "[Tune-y] 174: auc.test.mean=0.7872205; time: 16.4 min\n",
      "[Tune-x] 175: selected.learner=classif.kknn; k=678\n",
      "[Tune-y] 175: auc.test.mean=0.7885729; time: 0.1 min\n",
      "[Tune-x] 176: selected.learner=classif.ctree; mincriterion=0.953; minsplit=28; minbucket=7\n",
      "[Tune-y] 176: auc.test.mean=0.7565340; time: 0.0 min\n",
      "[Tune-x] 177: selected.learner=classif.ctree; mincriterion=0.887; minsplit=11; minbucket=10\n",
      "[Tune-y] 177: auc.test.mean=0.7584426; time: 0.0 min\n",
      "[Tune-x] 178: selected.learner=classif.boos...; mfinal=253\n",
      "[Tune-y] 178: auc.test.mean=0.7821779; time: 13.9 min\n",
      "[Tune-x] 179: selected.learner=classif.plsd...; probMethod=softmax; ncomp=3\n",
      "[Tune-y] 179: auc.test.mean=0.7500435; time: 0.0 min\n",
      "[Tune-x] 180: selected.learner=classif.plsd...; probMethod=softmax; ncomp=11\n",
      "[Tune-y] 180: auc.test.mean=0.7857520; time: 0.0 min\n",
      "[Tune-x] 181: selected.learner=classif.kknn; k=931\n",
      "[Tune-y] 181: auc.test.mean=0.7884290; time: 0.1 min\n",
      "[Tune-x] 182: selected.learner=classif.boos...; mfinal=190\n",
      "[Tune-y] 182: auc.test.mean=0.7875438; time: 10.7 min\n",
      "[Tune-x] 183: selected.learner=classif.boos...; mfinal=266\n",
      "[Tune-y] 183: auc.test.mean=0.7865121; time: 15.6 min\n",
      "[Tune-x] 184: selected.learner=classif.boos...; mfinal=75\n",
      "[Tune-y] 184: auc.test.mean=0.7851982; time: 4.8 min\n",
      "[Tune-x] 185: selected.learner=classif.plsd...; probMethod=softmax; ncomp=10\n",
      "[Tune-y] 185: auc.test.mean=0.7815264; time: 0.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Tune-x] 186: selected.learner=classif.C50; CF=0.551; minCases=8\n",
      "[Tune-y] 186: auc.test.mean=0.7127874; time: 0.1 min\n",
      "[Tune-x] 187: selected.learner=classif.ctree; mincriterion=0.842; minsplit=35; minbucket=9\n",
      "[Tune-y] 187: auc.test.mean=0.7576253; time: 0.0 min\n",
      "[Tune-x] 188: selected.learner=classif.kknn; k=636\n",
      "[Tune-y] 188: auc.test.mean=0.7883632; time: 0.1 min\n",
      "[Tune-x] 189: selected.learner=classif.C50; CF=0.305; minCases=12\n",
      "[Tune-y] 189: auc.test.mean=0.7147122; time: 0.1 min\n",
      "[Tune-x] 190: selected.learner=classif.boos...; mfinal=115\n",
      "[Tune-y] 190: auc.test.mean=0.7819687; time: 7.6 min\n",
      "[Tune-x] 191: selected.learner=classif.boos...; mfinal=74\n",
      "[Tune-y] 191: auc.test.mean=0.7828720; time: 4.6 min\n",
      "[Tune-x] 192: selected.learner=classif.C50; CF=0.535; minCases=7\n",
      "[Tune-y] 192: auc.test.mean=0.7187798; time: 0.1 min\n",
      "[Tune-x] 193: selected.learner=classif.kknn; k=848\n",
      "[Tune-y] 193: auc.test.mean=0.7884627; time: 0.1 min\n",
      "[Tune-x] 194: selected.learner=classif.C50; CF=0.262; minCases=3\n",
      "[Tune-y] 194: auc.test.mean=0.7137218; time: 0.1 min\n",
      "[Tune-x] 195: selected.learner=classif.C50; CF=0.0883; minCases=15\n",
      "[Tune-y] 195: auc.test.mean=0.6031984; time: 0.1 min\n",
      "[Tune-x] 196: selected.learner=classif.ctree; mincriterion=0.995; minsplit=37; minbucket=8\n",
      "[Tune-y] 196: auc.test.mean=0.7501968; time: 0.0 min\n",
      "[Tune-x] 197: selected.learner=classif.plsd...; probMethod=softmax; ncomp=3\n",
      "[Tune-y] 197: auc.test.mean=0.7500435; time: 0.0 min\n",
      "[Tune-x] 198: selected.learner=classif.boos...; mfinal=282\n",
      "[Tune-y] 198: auc.test.mean=0.7856252; time: 17.1 min\n",
      "[Tune-x] 199: selected.learner=classif.plsd...; probMethod=softmax; ncomp=2\n",
      "[Tune-y] 199: auc.test.mean=0.7482641; time: 0.0 min\n",
      "[Tune-x] 200: selected.learner=classif.ctree; mincriterion=0.898; minsplit=34; minbucket=10\n",
      "[Tune-y] 200: auc.test.mean=0.7582829; time: 0.0 min\n",
      "[Tune-x] 201: selected.learner=classif.boos...; mfinal=83\n",
      "[Tune-y] 201: auc.test.mean=0.7811168; time: 5.3 min\n",
      "[Tune-x] 202: selected.learner=classif.C50; CF=0.621; minCases=13\n",
      "[Tune-y] 202: auc.test.mean=0.7145044; time: 0.1 min\n",
      "[Tune-x] 203: selected.learner=classif.C50; CF=0.992; minCases=15\n",
      "[Tune-y] 203: auc.test.mean=0.7146536; time: 0.1 min\n",
      "[Tune-x] 204: selected.learner=classif.ctree; mincriterion=0.865; minsplit=10; minbucket=8\n",
      "[Tune-y] 204: auc.test.mean=0.7572154; time: 0.0 min\n",
      "[Tune-x] 205: selected.learner=classif.ctree; mincriterion=0.816; minsplit=17; minbucket=7\n",
      "[Tune-y] 205: auc.test.mean=0.7560508; time: 0.0 min\n",
      "[Tune-x] 206: selected.learner=classif.kknn; k=786\n",
      "[Tune-y] 206: auc.test.mean=0.7884676; time: 0.1 min\n",
      "[Tune-x] 207: selected.learner=classif.plsd...; probMethod=softmax; ncomp=3\n",
      "[Tune-y] 207: auc.test.mean=0.7500435; time: 0.0 min\n",
      "[Tune-x] 208: selected.learner=classif.boos...; mfinal=262\n",
      "[Tune-y] 208: auc.test.mean=0.7868939; time: 16.6 min\n",
      "[Tune-x] 209: selected.learner=classif.boos...; mfinal=227\n",
      "[Tune-y] 209: auc.test.mean=0.7840060; time: 14.8 min\n",
      "[Tune-x] 210: selected.learner=classif.kknn; k=694\n",
      "[Tune-y] 210: auc.test.mean=0.7884921; time: 0.1 min\n",
      "[Tune-x] 211: selected.learner=classif.boos...; mfinal=248\n",
      "[Tune-y] 211: auc.test.mean=0.7854504; time: 15.8 min\n",
      "[Tune-x] 212: selected.learner=classif.kknn; k=636\n",
      "[Tune-y] 212: auc.test.mean=0.7883632; time: 0.1 min\n",
      "[Tune-x] 213: selected.learner=classif.ctree; mincriterion=0.837; minsplit=18; minbucket=9\n",
      "[Tune-y] 213: auc.test.mean=0.7596908; time: 0.0 min\n",
      "[Tune-x] 214: selected.learner=classif.plsd...; probMethod=softmax; ncomp=15\n",
      "[Tune-y] 214: auc.test.mean=0.7851131; time: 0.0 min\n",
      "[Tune-x] 215: selected.learner=classif.C50; CF=0.273; minCases=10\n",
      "[Tune-y] 215: auc.test.mean=0.7147210; time: 0.1 min\n",
      "[Tune-x] 216: selected.learner=classif.kknn; k=814\n",
      "[Tune-y] 216: auc.test.mean=0.7884338; time: 0.1 min\n",
      "[Tune-x] 217: selected.learner=classif.C50; CF=0.757; minCases=3\n",
      "[Tune-y] 217: auc.test.mean=0.7137527; time: 0.1 min\n",
      "[Tune-x] 218: selected.learner=classif.kknn; k=891\n",
      "[Tune-y] 218: auc.test.mean=0.7884175; time: 0.1 min\n",
      "[Tune-x] 219: selected.learner=classif.ctree; mincriterion=0.985; minsplit=28; minbucket=5\n",
      "[Tune-y] 219: auc.test.mean=0.7547240; time: 0.0 min\n",
      "[Tune-x] 220: selected.learner=classif.plsd...; probMethod=softmax; ncomp=9\n",
      "[Tune-y] 220: auc.test.mean=0.7814007; time: 0.0 min\n",
      "[Tune-x] 221: selected.learner=classif.boos...; mfinal=69\n",
      "[Tune-y] 221: auc.test.mean=0.7860127; time: 4.4 min\n",
      "[Tune-x] 222: selected.learner=classif.ctree; mincriterion=0.866; minsplit=25; minbucket=10\n",
      "[Tune-y] 222: auc.test.mean=0.7576174; time: 0.0 min\n",
      "[Tune-x] 223: selected.learner=classif.plsd...; probMethod=softmax; ncomp=7\n",
      "[Tune-y] 223: auc.test.mean=0.7827350; time: 0.0 min\n",
      "[Tune-x] 224: selected.learner=classif.plsd...; probMethod=softmax; ncomp=12\n",
      "[Tune-y] 224: auc.test.mean=0.7865138; time: 0.0 min\n",
      "[Tune-x] 225: selected.learner=classif.C50; CF=0.0418; minCases=11\n",
      "[Tune-y] 225: auc.test.mean=0.5952067; time: 0.1 min\n",
      "[Tune-x] 226: selected.learner=classif.C50; CF=0.15; minCases=5\n",
      "[Tune-y] 226: auc.test.mean=0.7015814; time: 0.1 min\n",
      "[Tune-x] 227: selected.learner=classif.plsd...; probMethod=softmax; ncomp=14\n",
      "[Tune-y] 227: auc.test.mean=0.7844433; time: 0.0 min\n",
      "[Tune-x] 228: selected.learner=classif.kknn; k=531\n",
      "[Tune-y] 228: auc.test.mean=0.7880734; time: 0.1 min\n",
      "[Tune-x] 229: selected.learner=classif.ctree; mincriterion=0.913; minsplit=39; minbucket=7\n",
      "[Tune-y] 229: auc.test.mean=0.7579754; time: 0.0 min\n",
      "[Tune-x] 230: selected.learner=classif.ctree; mincriterion=0.855; minsplit=37; minbucket=8\n",
      "[Tune-y] 230: auc.test.mean=0.7571890; time: 0.0 min\n",
      "[Tune-x] 231: selected.learner=classif.ctree; mincriterion=0.869; minsplit=32; minbucket=6\n",
      "[Tune-y] 231: auc.test.mean=0.7573815; time: 0.0 min\n",
      "[Tune-x] 232: selected.learner=classif.boos...; mfinal=102\n",
      "[Tune-y] 232: auc.test.mean=0.7799452; time: 6.5 min\n",
      "[Tune-x] 233: selected.learner=classif.boos...; mfinal=191\n",
      "[Tune-y] 233: auc.test.mean=0.7859409; time: 11.9 min\n",
      "[Tune-x] 234: selected.learner=classif.ctree; mincriterion=0.927; minsplit=13; minbucket=9\n",
      "[Tune-y] 234: auc.test.mean=0.7576292; time: 0.0 min\n",
      "[Tune-x] 235: selected.learner=classif.boos...; mfinal=293\n",
      "[Tune-y] 235: auc.test.mean=0.7892037; time: 18.3 min\n",
      "[Tune-x] 236: selected.learner=classif.C50; CF=0.385; minCases=6\n",
      "[Tune-y] 236: auc.test.mean=0.7128242; time: 0.1 min\n",
      "[Tune-x] 237: selected.learner=classif.plsd...; probMethod=softmax; ncomp=11\n",
      "[Tune-y] 237: auc.test.mean=0.7857520; time: 0.0 min\n",
      "[Tune-x] 238: selected.learner=classif.plsd...; probMethod=softmax; ncomp=15\n",
      "[Tune-y] 238: auc.test.mean=0.7851131; time: 0.0 min\n",
      "[Tune-x] 239: selected.learner=classif.C50; CF=0.809; minCases=13\n",
      "[Tune-y] 239: auc.test.mean=0.7145044; time: 0.1 min\n",
      "[Tune-x] 240: selected.learner=classif.kknn; k=669\n",
      "[Tune-y] 240: auc.test.mean=0.7884913; time: 0.1 min\n",
      "[Tune-x] 241: selected.learner=classif.C50; CF=0.885; minCases=5\n",
      "[Tune-y] 241: auc.test.mean=0.7194148; time: 0.1 min\n",
      "[Tune-x] 242: selected.learner=classif.kknn; k=583\n",
      "[Tune-y] 242: auc.test.mean=0.7882109; time: 0.1 min\n",
      "[Tune-x] 243: selected.learner=classif.C50; CF=0.472; minCases=4\n",
      "[Tune-y] 243: auc.test.mean=0.7197322; time: 0.1 min\n",
      "[Tune-x] 244: selected.learner=classif.plsd...; probMethod=softmax; ncomp=9\n",
      "[Tune-y] 244: auc.test.mean=0.7814007; time: 0.0 min\n",
      "[Tune-x] 245: selected.learner=classif.plsd...; probMethod=softmax; ncomp=12\n",
      "[Tune-y] 245: auc.test.mean=0.7865138; time: 0.0 min\n",
      "[Tune-x] 246: selected.learner=classif.C50; CF=0.585; minCases=12\n",
      "[Tune-y] 246: auc.test.mean=0.7147596; time: 0.1 min\n",
      "[Tune-x] 247: selected.learner=classif.C50; CF=0.0709; minCases=5\n",
      "[Tune-y] 247: auc.test.mean=0.5969379; time: 0.1 min\n",
      "[Tune-x] 248: selected.learner=classif.ctree; mincriterion=0.841; minsplit=18; minbucket=10\n",
      "[Tune-y] 248: auc.test.mean=0.7594984; time: 0.0 min\n",
      "[Tune-x] 249: selected.learner=classif.boos...; mfinal=185\n",
      "[Tune-y] 249: auc.test.mean=0.7854697; time: 11.7 min\n",
      "[Tune-x] 250: selected.learner=classif.plsd...; probMethod=softmax; ncomp=9\n",
      "[Tune-y] 250: auc.test.mean=0.7814007; time: 0.0 min\n",
      "[Tune] Result: selected.learner=classif.boos...; classif.boosting.mfinal=138 : auc.test.mean=0.7908456\n"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "options(warn=-1)\n",
    "\n",
    "# Set up cross-validation for 10 folds:\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "#We create the list of algorithms\n",
    "bls = list(\n",
    "    makeLearner(\"classif.kknn\", predict.type = \"prob\"),\n",
    "    makeLearner(\"classif.plsdaCaret\", predict.type = \"prob\"),\n",
    "    makeLearner(\"classif.ctree\", predict.type = \"prob\"),\n",
    "    makeLearner(\"classif.C50\", predict.type = \"prob\"),\n",
    "    makeLearner(\"classif.boosting\", predict.type = \"prob\"))\n",
    "\n",
    "#We assign this list to a learn using the makeModelMultiplexer\n",
    "\n",
    "lrn = makeModelMultiplexer(bls)\n",
    "\n",
    "#We create the traintask\n",
    "train_task <- makeClassifTask(id=\"bank_train\", data=DataClean, target=\"subscribe\")\n",
    "\n",
    "#We assign the tuning parameters to model that contains\n",
    "ps = makeModelMultiplexerParamSet(lrn,\n",
    "#KNN parameters:\n",
    "    classif.kknn = makeParamSet(makeIntegerParam(\"k\", lower = 500, upper = 1000)),\n",
    "#plsdaCaret parameters:\n",
    "    classif.plsdaCaret = makeParamSet(makeDiscreteLearnerParam(\"probMethod\", values = \"softmax\"),\n",
    "                         makeIntegerParam(\"ncomp\",lower = 1, upper = 15)),\n",
    "#Ctree parameters:\n",
    "    classif.ctree = makeParamSet(makeNumericParam('mincriterion', lower = 0.80, upper = 1),\n",
    "                    makeIntegerParam(\"minsplit\", lower = 10, upper = 40),\n",
    "                    makeIntegerParam(\"minbucket\", lower = 5, upper = 10)),\n",
    "#C50 parameters:\n",
    "    classif.C50 = makeParamSet(makeNumericParam(\"CF\", lower = 0, upper = 1),\n",
    "                  makeIntegerParam(\"minCases\", lower = 2, upper = 20)),\n",
    "#boosting parameters:\n",
    "    classif.boosting = makeParamSet(makeIntegerParam(\"mfinal\", lower = 50, upper = 300))                  \n",
    ")\n",
    "\n",
    "#Define the number of iterations:\n",
    "ctrl = makeTuneControlRandom(maxit = 250L)\n",
    "\n",
    "#Run the test and assign the best model:\n",
    "res <- tuneParams(lrn, task=train_task, resampling=rdesc,par.set=ps, control=ctrl, measures=list(mlr::auc))\n",
    "                                  \n",
    "best_learner <- res$learner\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results algorithm choosed: classif.boosting\n",
    "* mfinal=138\n",
    "* auc.test.mean=0.7908456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_md <- mlr::train(best_learner, train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(best_md, newdata=PredictClean)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted file\n",
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=pred$data$prob.1)\n",
    "write.csv(output, 'C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Bagging/SelectedModel.csv', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, we will try a bagging of algorithm using the stacked function of the MLR Package. We wrrited the following code in order to attempt to find the best bagging model within a list of specified algorithm. To do so, we will in a first time try to see which algorithm perform the best individually and then we will append to it all other algorithms one by one. If the new composition of algorithms lead to a increase, then the new model is assigned. The goal is to try to bagg algorithms but to do it in a relevant way only using bagging of algorithms that lead to an increase of the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We initiate a list of algorithm:\n",
    "algorithms =  c(\"classif.naiveBayes\",\"classif.earth\", \"classif.xgboost\", \"classif.kknn\", \"classif.randomForest\",\n",
    "                \"classif.logreg\", \"classif.lda\", \"classif.plsdaCaret\", \"classif.ctree\", \"classif.C50\", \"classif.boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.naiveBayes\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7988700 \n",
      "[Resample] iter 2:    0.8054608 \n",
      "[Resample] iter 3:    0.7812904 \n",
      "[Resample] iter 4:    0.7464614 \n",
      "[Resample] iter 5:    0.7630235 \n",
      "[Resample] iter 6:    0.7456773 \n",
      "[Resample] iter 7:    0.8222104 \n",
      "[Resample] iter 8:    0.7636934 \n",
      "[Resample] iter 9:    0.7451712 \n",
      "[Resample] iter 10:   0.7617907 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7733649\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.earth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8411105 \n",
      "[Resample] iter 2:    0.7795801 \n",
      "[Resample] iter 3:    0.7510887 \n",
      "[Resample] iter 4:    0.7790420 \n",
      "[Resample] iter 5:    0.7767610 \n",
      "[Resample] iter 6:    0.8465629 \n",
      "[Resample] iter 7:    0.7627451 \n",
      "[Resample] iter 8:    0.7748787 \n",
      "[Resample] iter 9:    0.8167467 \n",
      "[Resample] iter 10:   0.8390152 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7967531\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.earth is now the best Model by beating 0.773364925831676\"\n",
      "[1] \"classif.xgboost\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7332964 \n",
      "[Resample] iter 2:    0.7855344 \n",
      "[Resample] iter 3:    0.7499568 \n",
      "[Resample] iter 4:    0.7920492 \n",
      "[Resample] iter 5:    0.8206791 \n",
      "[Resample] iter 6:    0.7924093 \n",
      "[Resample] iter 7:    0.7633028 \n",
      "[Resample] iter 8:    0.7587243 \n",
      "[Resample] iter 9:    0.7881552 \n",
      "[Resample] iter 10:   0.7858177 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7769925\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.xgboost didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.kknn\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: kknn\n",
      "\n",
      "Attaching package: 'kknn'\n",
      "\n",
      "The following object is masked from 'package:caret':\n",
      "\n",
      "    contr.dummy\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7119311 \n",
      "[Resample] iter 2:    0.7108928 \n",
      "[Resample] iter 3:    0.7227862 \n",
      "[Resample] iter 4:    0.7272377 \n",
      "[Resample] iter 5:    0.7362022 \n",
      "[Resample] iter 6:    0.7537938 \n",
      "[Resample] iter 7:    0.7330560 \n",
      "[Resample] iter 8:    0.7448892 \n",
      "[Resample] iter 9:    0.7441566 \n",
      "[Resample] iter 10:   0.7471247 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7332070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.kknn didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.randomForest\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8163357 \n",
      "[Resample] iter 2:    0.7746605 \n",
      "[Resample] iter 3:    0.7690100 \n",
      "[Resample] iter 4:    0.8264572 \n",
      "[Resample] iter 5:    0.7986800 \n",
      "[Resample] iter 6:    0.7659740 \n",
      "[Resample] iter 7:    0.7379578 \n",
      "[Resample] iter 8:    0.7587345 \n",
      "[Resample] iter 9:    0.7699424 \n",
      "[Resample] iter 10:   0.8032895 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7821042\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.randomForest didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.logreg\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7741583 \n",
      "[Resample] iter 2:    0.7897418 \n",
      "[Resample] iter 3:    0.7954541 \n",
      "[Resample] iter 4:    0.7775801 \n",
      "[Resample] iter 5:    0.7797379 \n",
      "[Resample] iter 6:    0.8432623 \n",
      "[Resample] iter 7:    0.7841875 \n",
      "[Resample] iter 8:    0.7508657 \n",
      "[Resample] iter 9:    0.7755645 \n",
      "[Resample] iter 10:   0.8231293 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7893682\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.logreg didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.lda\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7941882 \n",
      "[Resample] iter 2:    0.7843093 \n",
      "[Resample] iter 3:    0.7979032 \n",
      "[Resample] iter 4:    0.7658706 \n",
      "[Resample] iter 5:    0.7395734 \n",
      "[Resample] iter 6:    0.8021333 \n",
      "[Resample] iter 7:    0.8016531 \n",
      "[Resample] iter 8:    0.8129606 \n",
      "[Resample] iter 9:    0.7885988 \n",
      "[Resample] iter 10:   0.7650885 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7852279\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.lda didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.plsdaCaret\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7394880 \n",
      "[Resample] iter 2:    0.7513379 \n",
      "[Resample] iter 3:    0.7292675 \n",
      "[Resample] iter 4:    0.7696623 \n",
      "[Resample] iter 5:    0.7209073 \n",
      "[Resample] iter 6:    0.7582427 \n",
      "[Resample] iter 7:    0.7514482 \n",
      "[Resample] iter 8:    0.7501318 \n",
      "[Resample] iter 9:    0.7837208 \n",
      "[Resample] iter 10:   0.7352522 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7489459\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.plsdaCaret didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.ctree\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7313528 \n",
      "[Resample] iter 2:    0.7594560 \n",
      "[Resample] iter 3:    0.8056603 \n",
      "[Resample] iter 4:    0.7395805 \n",
      "[Resample] iter 5:    0.7020286 \n",
      "[Resample] iter 6:    0.7647245 \n",
      "[Resample] iter 7:    0.8157928 \n",
      "[Resample] iter 8:    0.7192084 \n",
      "[Resample] iter 9:    0.7177174 \n",
      "[Resample] iter 10:   0.7595770 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7515098\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.ctree didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.C50\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.6903147 \n",
      "[Resample] iter 2:    0.6812658 \n",
      "[Resample] iter 3:    0.7794247 \n",
      "[Resample] iter 4:    0.7391170 \n",
      "[Resample] iter 5:    0.6851363 \n",
      "[Resample] iter 6:    0.7102522 \n",
      "[Resample] iter 7:    0.5881924 \n",
      "[Resample] iter 8:    0.5696725 \n",
      "[Resample] iter 9:    0.7231995 \n",
      "[Resample] iter 10:   0.7275535 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.6894129\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.C50 didn't reach the AUC of classif.earth\"\n",
      "[1] \"classif.boosting\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7881345 \n",
      "[Resample] iter 2:    0.7715130 \n",
      "[Resample] iter 3:    0.7623106 \n",
      "[Resample] iter 4:    0.8080882 \n",
      "[Resample] iter 5:    0.7877676 \n",
      "[Resample] iter 6:    0.7906598 \n",
      "[Resample] iter 7:    0.8071467 \n",
      "[Resample] iter 8:    0.7960447 \n",
      "[Resample] iter 9:    0.7410970 \n",
      "[Resample] iter 10:   0.7883405 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7841103\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.boosting didn't reach the AUC of classif.earth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8064540 \n",
      "[Resample] iter 2:    0.8090978 \n",
      "[Resample] iter 3:    0.8002270 \n",
      "[Resample] iter 4:    0.8070066 \n",
      "[Resample] iter 5:    0.8386035 \n",
      "[Resample] iter 6:    0.8044164 \n",
      "[Resample] iter 7:    0.7807530 \n",
      "[Resample] iter 8:    0.7910400 \n",
      "[Resample] iter 9:    0.7529638 \n",
      "[Resample] iter 10:   0.7734017 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7963964\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8151498 \n",
      "[Resample] iter 2:    0.7847093 \n",
      "[Resample] iter 3:    0.8422112 \n",
      "[Resample] iter 4:    0.8512608 \n",
      "[Resample] iter 5:    0.7947172 \n",
      "[Resample] iter 6:    0.8070385 \n",
      "[Resample] iter 7:    0.8237517 \n",
      "[Resample] iter 8:    0.7620206 \n",
      "[Resample] iter 9:    0.7394989 \n",
      "[Resample] iter 10:   0.7723886 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7992746\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bagging is now the best Model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8084476 \n",
      "[Resample] iter 2:    0.7910795 \n",
      "[Resample] iter 3:    0.7766567 \n",
      "[Resample] iter 4:    0.7730609 \n",
      "[Resample] iter 5:    0.8242286 \n",
      "[Resample] iter 6:    0.8006101 \n",
      "[Resample] iter 7:    0.8026404 \n",
      "[Resample] iter 8:    0.7685634 \n",
      "[Resample] iter 9:    0.8330582 \n",
      "[Resample] iter 10:   0.7883484 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7966694\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8055412 \n",
      "[Resample] iter 2:    0.8030685 \n",
      "[Resample] iter 3:    0.8042043 \n",
      "[Resample] iter 4:    0.7762925 \n",
      "[Resample] iter 5:    0.8124553 \n",
      "[Resample] iter 6:    0.7716160 \n",
      "[Resample] iter 7:    0.8004578 \n",
      "[Resample] iter 8:    0.8011916 \n",
      "[Resample] iter 9:    0.8133140 \n",
      "[Resample] iter 10:   0.8022891 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7990430\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7267937 \n",
      "[Resample] iter 2:    0.8422449 \n",
      "[Resample] iter 3:    0.8552563 \n",
      "[Resample] iter 4:    0.7208468 \n",
      "[Resample] iter 5:    0.7814547 \n",
      "[Resample] iter 6:    0.8134313 \n",
      "[Resample] iter 7:    0.8073553 \n",
      "[Resample] iter 8:    0.7887720 \n",
      "[Resample] iter 9:    0.8203230 \n",
      "[Resample] iter 10:   0.7989350 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7955413\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7957473 \n",
      "[Resample] iter 2:    0.7857448 \n",
      "[Resample] iter 3:    0.8123454 \n",
      "[Resample] iter 4:    0.7765070 \n",
      "[Resample] iter 5:    0.7346745 \n",
      "[Resample] iter 6:    0.8375641 \n",
      "[Resample] iter 7:    0.8425078 \n",
      "[Resample] iter 8:    0.8201068 \n",
      "[Resample] iter 9:    0.7725028 \n",
      "[Resample] iter 10:   0.7645444 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7942245\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8096735 \n",
      "[Resample] iter 2:    0.7829479 \n",
      "[Resample] iter 3:    0.8202268 \n",
      "[Resample] iter 4:    0.7524546 \n",
      "[Resample] iter 5:    0.8351983 \n",
      "[Resample] iter 6:    0.7395136 \n",
      "[Resample] iter 7:    0.8682540 \n",
      "[Resample] iter 8:    0.7716747 \n",
      "[Resample] iter 9:    0.7974317 \n",
      "[Resample] iter 10:   0.8015075 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7978883\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8254281 \n",
      "[Resample] iter 2:    0.7864930 \n",
      "[Resample] iter 3:    0.7676376 \n",
      "[Resample] iter 4:    0.8012137 \n",
      "[Resample] iter 5:    0.7800640 \n",
      "[Resample] iter 6:    0.7653455 \n",
      "[Resample] iter 7:    0.8029330 \n",
      "[Resample] iter 8:    0.8144229 \n",
      "[Resample] iter 9:    0.7969354 \n",
      "[Resample] iter 10:   0.7701366 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7910610\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8312165 \n",
      "[Resample] iter 2:    0.7749413 \n",
      "[Resample] iter 3:    0.8154430 \n",
      "[Resample] iter 4:    0.8046266 \n",
      "[Resample] iter 5:    0.8231217 \n",
      "[Resample] iter 6:    0.7785576 \n",
      "[Resample] iter 7:    0.6722525 \n",
      "[Resample] iter 8:    0.7961640 \n",
      "[Resample] iter 9:    0.7955990 \n",
      "[Resample] iter 10:   0.8192796 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7911202\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7885189 \n",
      "[Resample] iter 2:    0.7784027 \n",
      "[Resample] iter 3:    0.8232314 \n",
      "[Resample] iter 4:    0.8131371 \n",
      "[Resample] iter 5:    0.7979531 \n",
      "[Resample] iter 6:    0.7746871 \n",
      "[Resample] iter 7:    0.8162708 \n",
      "[Resample] iter 8:    0.8018776 \n",
      "[Resample] iter 9:    0.7733900 \n",
      "[Resample] iter 10:   0.7635107 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7930979\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7936070 \n",
      "[Resample] iter 2:    0.8260831 \n",
      "[Resample] iter 3:    0.8279483 \n",
      "[Resample] iter 4:    0.7192876 \n",
      "[Resample] iter 5:    0.8171385 \n",
      "[Resample] iter 6:    0.7467332 \n",
      "[Resample] iter 7:    0.8297544 \n",
      "[Resample] iter 8:    0.8383523 \n",
      "[Resample] iter 9:    0.7882608 \n",
      "[Resample] iter 10:   0.8037669 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7990932\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7742235 \n",
      "[Resample] iter 2:    0.7696197 \n",
      "[Resample] iter 3:    0.8019519 \n",
      "[Resample] iter 4:    0.7896812 \n",
      "[Resample] iter 5:    0.7636251 \n",
      "[Resample] iter 6:    0.8358842 \n",
      "[Resample] iter 7:    0.8648975 \n",
      "[Resample] iter 8:    0.8132229 \n",
      "[Resample] iter 9:    0.7665623 \n",
      "[Resample] iter 10:   0.8182114 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7997880\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bagging is now the best Model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8197502 \n",
      "[Resample] iter 2:    0.8053426 \n",
      "[Resample] iter 3:    0.7308531 \n",
      "[Resample] iter 4:    0.8059577 \n",
      "[Resample] iter 5:    0.7919960 \n",
      "[Resample] iter 6:    0.8123329 \n",
      "[Resample] iter 7:    0.7935855 \n",
      "[Resample] iter 8:    0.8048564 \n",
      "[Resample] iter 9:    0.7901206 \n",
      "[Resample] iter 10:   0.8179590 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7972754\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7905158 \n",
      "[Resample] iter 2:    0.8174878 \n",
      "[Resample] iter 3:    0.8042822 \n",
      "[Resample] iter 4:    0.7505904 \n",
      "[Resample] iter 5:    0.8059995 \n",
      "[Resample] iter 6:    0.8578030 \n",
      "[Resample] iter 7:    0.7819875 \n",
      "[Resample] iter 8:    0.8021606 \n",
      "[Resample] iter 9:    0.7788371 \n",
      "[Resample] iter 10:   0.7894553 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7979119\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8129743 \n",
      "[Resample] iter 2:    0.8005356 \n",
      "[Resample] iter 3:    0.8122829 \n",
      "[Resample] iter 4:    0.8122429 \n",
      "[Resample] iter 5:    0.7946223 \n",
      "[Resample] iter 6:    0.7956863 \n",
      "[Resample] iter 7:    0.7768936 \n",
      "[Resample] iter 8:    0.8223491 \n",
      "[Resample] iter 9:    0.7418626 \n",
      "[Resample] iter 10:   0.8140969 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7983546\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7539200 \n",
      "[Resample] iter 2:    0.7871945 \n",
      "[Resample] iter 3:    0.8037877 \n",
      "[Resample] iter 4:    0.8475512 \n",
      "[Resample] iter 5:    0.7882639 \n",
      "[Resample] iter 6:    0.7552117 \n",
      "[Resample] iter 7:    0.8191029 \n",
      "[Resample] iter 8:    0.7957765 \n",
      "[Resample] iter 9:    0.8372701 \n",
      "[Resample] iter 10:   0.7850572 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7973136\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7698689 \n",
      "[Resample] iter 2:    0.7801324 \n",
      "[Resample] iter 3:    0.8086935 \n",
      "[Resample] iter 4:    0.8131944 \n",
      "[Resample] iter 5:    0.8420143 \n",
      "[Resample] iter 6:    0.8145289 \n",
      "[Resample] iter 7:    0.8084520 \n",
      "[Resample] iter 8:    0.7032849 \n",
      "[Resample] iter 9:    0.8017982 \n",
      "[Resample] iter 10:   0.7807079 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7922675\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7484553 \n",
      "[Resample] iter 2:    0.7629576 \n",
      "[Resample] iter 3:    0.7080548 \n",
      "[Resample] iter 4:    0.7817808 \n",
      "[Resample] iter 5:    0.7842903 \n",
      "[Resample] iter 6:    0.7844818 \n",
      "[Resample] iter 7:    0.7924469 \n",
      "[Resample] iter 8:    0.8542405 \n",
      "[Resample] iter 9:    0.7939375 \n",
      "[Resample] iter 10:   0.8238497 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7834495\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7882174 \n",
      "[Resample] iter 2:    0.8208960 \n",
      "[Resample] iter 3:    0.8438312 \n",
      "[Resample] iter 4:    0.7872225 \n",
      "[Resample] iter 5:    0.8011765 \n",
      "[Resample] iter 6:    0.7752989 \n",
      "[Resample] iter 7:    0.7656149 \n",
      "[Resample] iter 8:    0.7862169 \n",
      "[Resample] iter 9:    0.7611141 \n",
      "[Resample] iter 10:   0.8502350 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7979824\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7979224 \n",
      "[Resample] iter 2:    0.8381253 \n",
      "[Resample] iter 3:    0.8037509 \n",
      "[Resample] iter 4:    0.7146293 \n",
      "[Resample] iter 5:    0.7875504 \n",
      "[Resample] iter 6:    0.7870482 \n",
      "[Resample] iter 7:    0.8077960 \n",
      "[Resample] iter 8:    0.8183281 \n",
      "[Resample] iter 9:    0.7961384 \n",
      "[Resample] iter 10:   0.8338841 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7985173\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8172756 \n",
      "[Resample] iter 2:    0.7473400 \n",
      "[Resample] iter 3:    0.8236538 \n",
      "[Resample] iter 4:    0.8068739 \n",
      "[Resample] iter 5:    0.7588774 \n",
      "[Resample] iter 6:    0.7908497 \n",
      "[Resample] iter 7:    0.7844928 \n",
      "[Resample] iter 8:    0.8427722 \n",
      "[Resample] iter 9:    0.7411898 \n",
      "[Resample] iter 10:   0.8324597 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7945785\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8035066 \n",
      "[Resample] iter 2:    0.7817357 \n",
      "[Resample] iter 3:    0.7316028 \n",
      "[Resample] iter 4:    0.8149183 \n",
      "[Resample] iter 5:    0.8340314 \n",
      "[Resample] iter 6:    0.8080011 \n",
      "[Resample] iter 7:    0.7993305 \n",
      "[Resample] iter 8:    0.8032070 \n",
      "[Resample] iter 9:    0.8068934 \n",
      "[Resample] iter 10:   0.7662900 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7949517\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7678125 \n",
      "[Resample] iter 2:    0.7802139 \n",
      "[Resample] iter 3:    0.8594898 \n",
      "[Resample] iter 4:    0.7711163 \n",
      "[Resample] iter 5:    0.8313077 \n",
      "[Resample] iter 6:    0.7969461 \n",
      "[Resample] iter 7:    0.8093831 \n",
      "[Resample] iter 8:    0.7969245 \n",
      "[Resample] iter 9:    0.8108560 \n",
      "[Resample] iter 10:   0.7554463 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7979496\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7988235 \n",
      "[Resample] iter 2:    0.7833083 \n",
      "[Resample] iter 3:    0.7760350 \n",
      "[Resample] iter 4:    0.8209577 \n",
      "[Resample] iter 5:    0.7596408 \n",
      "[Resample] iter 6:    0.8057374 \n",
      "[Resample] iter 7:    0.7770702 \n",
      "[Resample] iter 8:    0.8008863 \n",
      "[Resample] iter 9:    0.8240292 \n",
      "[Resample] iter 10:   0.7990998 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7945588\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7853079 \n",
      "[Resample] iter 2:    0.7277102 \n",
      "[Resample] iter 3:    0.7764116 \n",
      "[Resample] iter 4:    0.7795692 \n",
      "[Resample] iter 5:    0.8157017 \n",
      "[Resample] iter 6:    0.7922824 \n",
      "[Resample] iter 7:    0.7843695 \n",
      "[Resample] iter 8:    0.8300561 \n",
      "[Resample] iter 9:    0.7844443 \n",
      "[Resample] iter 10:   0.7944714 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7870324\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7658925 \n",
      "[Resample] iter 2:    0.7988084 \n",
      "[Resample] iter 3:    0.8066880 \n",
      "[Resample] iter 4:    0.8147447 \n",
      "[Resample] iter 5:    0.8140856 \n",
      "[Resample] iter 6:    0.7464607 \n",
      "[Resample] iter 7:    0.8155715 \n",
      "[Resample] iter 8:    0.7751639 \n",
      "[Resample] iter 9:    0.8409590 \n",
      "[Resample] iter 10:   0.7945852 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7972960\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7885028 \n",
      "[Resample] iter 2:    0.7607645 \n",
      "[Resample] iter 3:    0.8048761 \n",
      "[Resample] iter 4:    0.7988975 \n",
      "[Resample] iter 5:    0.8182177 \n",
      "[Resample] iter 6:    0.7868204 \n",
      "[Resample] iter 7:    0.7692097 \n",
      "[Resample] iter 8:    0.7988291 \n",
      "[Resample] iter 9:    0.8211706 \n",
      "[Resample] iter 10:   0.8227701 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7970058\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8327865 \n",
      "[Resample] iter 2:    0.8633770 \n",
      "[Resample] iter 3:    0.7833707 \n",
      "[Resample] iter 4:    0.8511355 \n",
      "[Resample] iter 5:    0.7854865 \n",
      "[Resample] iter 6:    0.7611293 \n",
      "[Resample] iter 7:    0.7821712 \n",
      "[Resample] iter 8:    0.7878398 \n",
      "[Resample] iter 9:    0.7768521 \n",
      "[Resample] iter 10:   0.7794439 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.8003593\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bagging is now the best Model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7801523 \n",
      "[Resample] iter 2:    0.8185156 \n",
      "[Resample] iter 3:    0.7839331 \n",
      "[Resample] iter 4:    0.7644160 \n",
      "[Resample] iter 5:    0.8087040 \n",
      "[Resample] iter 6:    0.7705458 \n",
      "[Resample] iter 7:    0.8134928 \n",
      "[Resample] iter 8:    0.7978773 \n",
      "[Resample] iter 9:    0.8113206 \n",
      "[Resample] iter 10:   0.8088066 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7957764\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7884629 \n",
      "[Resample] iter 2:    0.7970724 \n",
      "[Resample] iter 3:    0.7703629 \n",
      "[Resample] iter 4:    0.7633692 \n",
      "[Resample] iter 5:    0.7996093 \n",
      "[Resample] iter 6:    0.8569117 \n",
      "[Resample] iter 7:    0.7917014 \n",
      "[Resample] iter 8:    0.8229384 \n",
      "[Resample] iter 9:    0.8336134 \n",
      "[Resample] iter 10:   0.7863387 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.8010380\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bagging is now the best Model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8298148 \n",
      "[Resample] iter 2:    0.7774500 \n",
      "[Resample] iter 3:    0.8120311 \n",
      "[Resample] iter 4:    0.7822636 \n",
      "[Resample] iter 5:    0.8010501 \n",
      "[Resample] iter 6:    0.8439339 \n",
      "[Resample] iter 7:    0.7388310 \n",
      "[Resample] iter 8:    0.8379898 \n",
      "[Resample] iter 9:    0.8056083 \n",
      "[Resample] iter 10:   0.7442902 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7973263\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8005265 \n",
      "[Resample] iter 2:    0.8488997 \n",
      "[Resample] iter 3:    0.8247772 \n",
      "[Resample] iter 4:    0.8057342 \n",
      "[Resample] iter 5:    0.8149362 \n",
      "[Resample] iter 6:    0.7556301 \n",
      "[Resample] iter 7:    0.7631519 \n",
      "[Resample] iter 8:    0.7510081 \n",
      "[Resample] iter 9:    0.8044658 \n",
      "[Resample] iter 10:   0.7995157 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7968645\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7584000 \n",
      "[Resample] iter 2:    0.7504266 \n",
      "[Resample] iter 3:    0.8195307 \n",
      "[Resample] iter 4:    0.8265685 \n",
      "[Resample] iter 5:    0.8088707 \n",
      "[Resample] iter 6:    0.7952305 \n",
      "[Resample] iter 7:    0.8031971 \n",
      "[Resample] iter 8:    0.7535699 \n",
      "[Resample] iter 9:    0.8073141 \n",
      "[Resample] iter 10:   0.8021470 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7925255\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8510282 \n",
      "[Resample] iter 2:    0.7912404 \n",
      "[Resample] iter 3:    0.8369005 \n",
      "[Resample] iter 4:    0.8232234 \n",
      "[Resample] iter 5:    0.7917952 \n",
      "[Resample] iter 6:    0.7345805 \n",
      "[Resample] iter 7:    0.7727585 \n",
      "[Resample] iter 8:    0.8155734 \n",
      "[Resample] iter 9:    0.7787199 \n",
      "[Resample] iter 10:   0.7776578 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7973478\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7418950 \n",
      "[Resample] iter 2:    0.7372548 \n",
      "[Resample] iter 3:    0.8554750 \n",
      "[Resample] iter 4:    0.7995208 \n",
      "[Resample] iter 5:    0.7940894 \n",
      "[Resample] iter 6:    0.7498115 \n",
      "[Resample] iter 7:    0.8037087 \n",
      "[Resample] iter 8:    0.7860697 \n",
      "[Resample] iter 9:    0.8152545 \n",
      "[Resample] iter 10:   0.8235834 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7906663\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8642944 \n",
      "[Resample] iter 2:    0.7705950 \n",
      "[Resample] iter 3:    0.7704980 \n",
      "[Resample] iter 4:    0.7792271 \n",
      "[Resample] iter 5:    0.8071038 \n",
      "[Resample] iter 6:    0.7586694 \n",
      "[Resample] iter 7:    0.8229372 \n",
      "[Resample] iter 8:    0.8070141 \n",
      "[Resample] iter 9:    0.7960673 \n",
      "[Resample] iter 10:   0.7832159 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7959622\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7382114 \n",
      "[Resample] iter 2:    0.8450448 \n",
      "[Resample] iter 3:    0.7830140 \n",
      "[Resample] iter 4:    0.8604082 \n",
      "[Resample] iter 5:    0.7753011 \n",
      "[Resample] iter 6:    0.7696184 \n",
      "[Resample] iter 7:    0.8083060 \n",
      "[Resample] iter 8:    0.7802278 \n",
      "[Resample] iter 9:    0.8309980 \n",
      "[Resample] iter 10:   0.7978633 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7988993\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7876842 \n",
      "[Resample] iter 2:    0.7586137 \n",
      "[Resample] iter 3:    0.7802451 \n",
      "[Resample] iter 4:    0.7798948 \n",
      "[Resample] iter 5:    0.8075304 \n",
      "[Resample] iter 6:    0.8006788 \n",
      "[Resample] iter 7:    0.8454735 \n",
      "[Resample] iter 8:    0.8256039 \n",
      "[Resample] iter 9:    0.8274890 \n",
      "[Resample] iter 10:   0.7599159 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7973129\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7899818 \n",
      "[Resample] iter 2:    0.7880631 \n",
      "[Resample] iter 3:    0.7902165 \n",
      "[Resample] iter 4:    0.7970674 \n",
      "[Resample] iter 5:    0.7646130 \n",
      "[Resample] iter 6:    0.8092588 \n",
      "[Resample] iter 7:    0.8242370 \n",
      "[Resample] iter 8:    0.7863719 \n",
      "[Resample] iter 9:    0.7875183 \n",
      "[Resample] iter 10:   0.7823581 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7919686\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7996881 \n",
      "[Resample] iter 2:    0.8090703 \n",
      "[Resample] iter 3:    0.8320518 \n",
      "[Resample] iter 4:    0.8040746 \n",
      "[Resample] iter 5:    0.8008631 \n",
      "[Resample] iter 6:    0.7357013 \n",
      "[Resample] iter 7:    0.8202187 \n",
      "[Resample] iter 8:    0.7940381 \n",
      "[Resample] iter 9:    0.7948135 \n",
      "[Resample] iter 10:   0.7517393 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7942259\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7967010 \n",
      "[Resample] iter 2:    0.8383258 \n",
      "[Resample] iter 3:    0.7935340 \n",
      "[Resample] iter 4:    0.7434364 \n",
      "[Resample] iter 5:    0.8321691 \n",
      "[Resample] iter 6:    0.7597781 \n",
      "[Resample] iter 7:    0.7971988 \n",
      "[Resample] iter 8:    0.8161901 \n",
      "[Resample] iter 9:    0.7634508 \n",
      "[Resample] iter 10:   0.7568177 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7897602\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8183486 \n",
      "[Resample] iter 2:    0.7767356 \n",
      "[Resample] iter 3:    0.7888769 \n",
      "[Resample] iter 4:    0.8357596 \n",
      "[Resample] iter 5:    0.7865519 \n",
      "[Resample] iter 6:    0.8107104 \n",
      "[Resample] iter 7:    0.7873469 \n",
      "[Resample] iter 8:    0.7367886 \n",
      "[Resample] iter 9:    0.8296335 \n",
      "[Resample] iter 10:   0.8031855 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7973938\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7913152 \n",
      "[Resample] iter 2:    0.8090636 \n",
      "[Resample] iter 3:    0.7771364 \n",
      "[Resample] iter 4:    0.8238725 \n",
      "[Resample] iter 5:    0.8214113 \n",
      "[Resample] iter 6:    0.7660803 \n",
      "[Resample] iter 7:    0.7390449 \n",
      "[Resample] iter 8:    0.8079932 \n",
      "[Resample] iter 9:    0.8274597 \n",
      "[Resample] iter 10:   0.8036359 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7967013\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8392451 \n",
      "[Resample] iter 2:    0.7940980 \n",
      "[Resample] iter 3:    0.8061458 \n",
      "[Resample] iter 4:    0.7856177 \n",
      "[Resample] iter 5:    0.7823710 \n",
      "[Resample] iter 6:    0.7003045 \n",
      "[Resample] iter 7:    0.8156098 \n",
      "[Resample] iter 8:    0.8188736 \n",
      "[Resample] iter 9:    0.8195082 \n",
      "[Resample] iter 10:   0.8090835 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7970857\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8234069 \n",
      "[Resample] iter 2:    0.7891156 \n",
      "[Resample] iter 3:    0.8185570 \n",
      "[Resample] iter 4:    0.7569256 \n",
      "[Resample] iter 5:    0.8123484 \n",
      "[Resample] iter 6:    0.8057166 \n",
      "[Resample] iter 7:    0.7946598 \n",
      "[Resample] iter 8:    0.7554636 \n",
      "[Resample] iter 9:    0.8362656 \n",
      "[Resample] iter 10:   0.7894983 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7981957\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8562562 \n",
      "[Resample] iter 2:    0.8492404 \n",
      "[Resample] iter 3:    0.8074631 \n",
      "[Resample] iter 4:    0.8377017 \n",
      "[Resample] iter 5:    0.7568044 \n",
      "[Resample] iter 6:    0.7388260 \n",
      "[Resample] iter 7:    0.8011190 \n",
      "[Resample] iter 8:    0.7926867 \n",
      "[Resample] iter 9:    0.8022733 \n",
      "[Resample] iter 10:   0.7400915 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7982462\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8105282 \n",
      "[Resample] iter 2:    0.7790007 \n",
      "[Resample] iter 3:    0.7736619 \n",
      "[Resample] iter 4:    0.8062429 \n",
      "[Resample] iter 5:    0.8119615 \n",
      "[Resample] iter 6:    0.7650772 \n",
      "[Resample] iter 7:    0.7877379 \n",
      "[Resample] iter 8:    0.8110182 \n",
      "[Resample] iter 9:    0.7937277 \n",
      "[Resample] iter 10:   0.8061314 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7945088\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8102159 \n",
      "[Resample] iter 2:    0.8336568 \n",
      "[Resample] iter 3:    0.7720774 \n",
      "[Resample] iter 4:    0.7864157 \n",
      "[Resample] iter 5:    0.7968299 \n",
      "[Resample] iter 6:    0.7996357 \n",
      "[Resample] iter 7:    0.8089013 \n",
      "[Resample] iter 8:    0.8140341 \n",
      "[Resample] iter 9:    0.7105114 \n",
      "[Resample] iter 10:   0.8137578 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7946036\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7677774 \n",
      "[Resample] iter 2:    0.7596131 \n",
      "[Resample] iter 3:    0.8243625 \n",
      "[Resample] iter 4:    0.8398305 \n",
      "[Resample] iter 5:    0.8260559 \n",
      "[Resample] iter 6:    0.7908263 \n",
      "[Resample] iter 7:    0.8046071 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Resample] iter 8:    0.7718472 \n",
      "[Resample] iter 9:    0.7757367 \n",
      "[Resample] iter 10:   0.7863340 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7946991\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7572548 \n",
      "[Resample] iter 2:    0.7907795 \n",
      "[Resample] iter 3:    0.8114214 \n",
      "[Resample] iter 4:    0.7584515 \n",
      "[Resample] iter 5:    0.8319605 \n",
      "[Resample] iter 6:    0.8152272 \n",
      "[Resample] iter 7:    0.7789091 \n",
      "[Resample] iter 8:    0.8242944 \n",
      "[Resample] iter 9:    0.8117734 \n",
      "[Resample] iter 10:   0.7904567 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7970529\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7859516 \n",
      "[Resample] iter 2:    0.7609465 \n",
      "[Resample] iter 3:    0.8202063 \n",
      "[Resample] iter 4:    0.7885869 \n",
      "[Resample] iter 5:    0.8517484 \n",
      "[Resample] iter 6:    0.8060142 \n",
      "[Resample] iter 7:    0.7665756 \n",
      "[Resample] iter 8:    0.8066354 \n",
      "[Resample] iter 9:    0.7767565 \n",
      "[Resample] iter 10:   0.8160401 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7979461\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8249862 \n",
      "[Resample] iter 2:    0.7526442 \n",
      "[Resample] iter 3:    0.8062289 \n",
      "[Resample] iter 4:    0.8242873 \n",
      "[Resample] iter 5:    0.8092261 \n",
      "[Resample] iter 6:    0.7838895 \n",
      "[Resample] iter 7:    0.6020086 \n",
      "[Resample] iter 8:    0.7574478 \n",
      "[Resample] iter 9:    0.7307984 \n",
      "[Resample] iter 10:   0.7405442 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7632061\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7542143 \n",
      "[Resample] iter 2:    0.7516490 \n",
      "[Resample] iter 3:    0.7912424 \n",
      "[Resample] iter 4:    0.8170146 \n",
      "[Resample] iter 5:    0.7720509 \n",
      "[Resample] iter 6:    0.8230847 \n",
      "[Resample] iter 7:    0.8483827 \n",
      "[Resample] iter 8:    0.8092268 \n",
      "[Resample] iter 9:    0.7614255 \n",
      "[Resample] iter 10:   0.8064037 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7934695\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7608468 \n",
      "[Resample] iter 2:    0.7603702 \n",
      "[Resample] iter 3:    0.7957331 \n",
      "[Resample] iter 4:    0.8034970 \n",
      "[Resample] iter 5:    0.8520503 \n",
      "[Resample] iter 6:    0.7895879 \n",
      "[Resample] iter 7:    0.7731205 \n",
      "[Resample] iter 8:    0.7679941 \n",
      "[Resample] iter 9:    0.8705948 \n",
      "[Resample] iter 10:   0.7867432 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7960538\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8188593 \n",
      "[Resample] iter 2:    0.7677257 \n",
      "[Resample] iter 3:    0.8135508 \n",
      "[Resample] iter 4:    0.7597632 \n",
      "[Resample] iter 5:    0.8224097 \n",
      "[Resample] iter 6:    0.7933806 \n",
      "[Resample] iter 7:    0.8096428 \n",
      "[Resample] iter 8:    0.8052845 \n",
      "[Resample] iter 9:    0.8082630 \n",
      "[Resample] iter 10:   0.7755380 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7974418\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8317026 \n",
      "[Resample] iter 2:    0.7689855 \n",
      "[Resample] iter 3:    0.7962940 \n",
      "[Resample] iter 4:    0.7822895 \n",
      "[Resample] iter 5:    0.7792038 \n",
      "[Resample] iter 6:    0.7912673 \n",
      "[Resample] iter 7:    0.8383022 \n",
      "[Resample] iter 8:    0.7775332 \n",
      "[Resample] iter 9:    0.7856034 \n",
      "[Resample] iter 10:   0.7842548 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7935436\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8045173 \n",
      "[Resample] iter 2:    0.8244731 \n",
      "[Resample] iter 3:    0.8240225 \n",
      "[Resample] iter 4:    0.7973036 \n",
      "[Resample] iter 5:    0.8067002 \n",
      "[Resample] iter 6:    0.7838151 \n",
      "[Resample] iter 7:    0.8030786 \n",
      "[Resample] iter 8:    0.7923981 \n",
      "[Resample] iter 9:    0.7204462 \n",
      "[Resample] iter 10:   0.8014489 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7958204\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8112994 \n",
      "[Resample] iter 2:    0.7612933 \n",
      "[Resample] iter 3:    0.8176217 \n",
      "[Resample] iter 4:    0.7698526 \n",
      "[Resample] iter 5:    0.8208366 \n",
      "[Resample] iter 6:    0.7790818 \n",
      "[Resample] iter 7:    0.7854687 \n",
      "[Resample] iter 8:    0.7950733 \n",
      "[Resample] iter 9:    0.7857460 \n",
      "[Resample] iter 10:   0.8390652 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7965338\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8223787 \n",
      "[Resample] iter 2:    0.7458836 \n",
      "[Resample] iter 3:    0.7946011 \n",
      "[Resample] iter 4:    0.7766637 \n",
      "[Resample] iter 5:    0.8021220 \n",
      "[Resample] iter 6:    0.8664945 \n",
      "[Resample] iter 7:    0.8160841 \n",
      "[Resample] iter 8:    0.7906048 \n",
      "[Resample] iter 9:    0.7730761 \n",
      "[Resample] iter 10:   0.7952093 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7983118\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7803954 \n",
      "[Resample] iter 2:    0.8027748 \n",
      "[Resample] iter 3:    0.8555314 \n",
      "[Resample] iter 4:    0.7749683 \n",
      "[Resample] iter 5:    0.7952393 \n",
      "[Resample] iter 6:    0.7934063 \n",
      "[Resample] iter 7:    0.8106339 \n",
      "[Resample] iter 8:    0.7656009 \n",
      "[Resample] iter 9:    0.8555093 \n",
      "[Resample] iter 10:   0.7457290 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7979789\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8284794 \n",
      "[Resample] iter 2:    0.8025726 \n",
      "[Resample] iter 3:    0.7632143 \n",
      "[Resample] iter 4:    0.7786531 \n",
      "[Resample] iter 5:    0.8145578 \n",
      "[Resample] iter 6:    0.7810929 \n",
      "[Resample] iter 7:    0.7860658 \n",
      "[Resample] iter 8:    0.8058434 \n",
      "[Resample] iter 9:    0.8177419 \n",
      "[Resample] iter 10:   0.7745098 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7952731\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7558249 \n",
      "[Resample] iter 2:    0.8074532 \n",
      "[Resample] iter 3:    0.8166129 \n",
      "[Resample] iter 4:    0.8113488 \n",
      "[Resample] iter 5:    0.7522449 \n",
      "[Resample] iter 6:    0.7610530 \n",
      "[Resample] iter 7:    0.7976266 \n",
      "[Resample] iter 8:    0.7881387 \n",
      "[Resample] iter 9:    0.8254435 \n",
      "[Resample] iter 10:   0.8047815 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7920528\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7865670 \n",
      "[Resample] iter 2:    0.8110892 \n",
      "[Resample] iter 3:    0.8214390 \n",
      "[Resample] iter 4:    0.7988169 \n",
      "[Resample] iter 5:    0.7967345 \n",
      "[Resample] iter 6:    0.7886248 \n",
      "[Resample] iter 7:    0.8594104 \n",
      "[Resample] iter 8:    0.7466918 \n",
      "[Resample] iter 9:    0.7885001 \n",
      "[Resample] iter 10:   0.7592607 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7957134\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8265315 \n",
      "[Resample] iter 2:    0.7667298 \n",
      "[Resample] iter 3:    0.7574152 \n",
      "[Resample] iter 4:    0.8054496 \n",
      "[Resample] iter 5:    0.8123887 \n",
      "[Resample] iter 6:    0.8319273 \n",
      "[Resample] iter 7:    0.7765609 \n",
      "[Resample] iter 8:    0.7849238 \n",
      "[Resample] iter 9:    0.8102997 \n",
      "[Resample] iter 10:   0.7794025 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7951629\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7868519 \n",
      "[Resample] iter 2:    0.7733214 \n",
      "[Resample] iter 3:    0.7668665 \n",
      "[Resample] iter 4:    0.7551639 \n",
      "[Resample] iter 5:    0.7836111 \n",
      "[Resample] iter 6:    0.8383693 \n",
      "[Resample] iter 7:    0.8405962 \n",
      "[Resample] iter 8:    0.8245527 \n",
      "[Resample] iter 9:    0.8062947 \n",
      "[Resample] iter 10:   0.7920827 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7967711\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7788587 \n",
      "[Resample] iter 2:    0.7939993 \n",
      "[Resample] iter 3:    0.8151196 \n",
      "[Resample] iter 4:    0.7879484 \n",
      "[Resample] iter 5:    0.8194380 \n",
      "[Resample] iter 6:    0.8000397 \n",
      "[Resample] iter 7:    0.7796910 \n",
      "[Resample] iter 8:    0.7397418 \n",
      "[Resample] iter 9:    0.8821506 \n",
      "[Resample] iter 10:   0.7890726 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7986060\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.8251927 \n",
      "[Resample] iter 2:    0.7968579 \n",
      "[Resample] iter 3:    0.7846071 \n",
      "[Resample] iter 4:    0.7752061 \n",
      "[Resample] iter 5:    0.7551893 \n",
      "[Resample] iter 6:    0.8481583 \n",
      "[Resample] iter 7:    0.7618173 \n",
      "[Resample] iter 8:    0.7460534 \n",
      "[Resample] iter 9:    0.8499302 \n",
      "[Resample] iter 10:   0.8150323 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7958045\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7787517 \n",
      "[Resample] iter 2:    0.8530444 \n",
      "[Resample] iter 3:    0.8241697 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Resample] iter 4:    0.8122571 \n",
      "[Resample] iter 5:    0.7784060 \n",
      "[Resample] iter 6:    0.7636364 \n",
      "[Resample] iter 7:    0.8082581 \n",
      "[Resample] iter 8:    0.8001223 \n",
      "[Resample] iter 9:    0.7929691 \n",
      "[Resample] iter 10:   0.7998366 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.8011451\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bagging is now the best Model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7972836 \n",
      "[Resample] iter 2:    0.7148321 \n",
      "[Resample] iter 3:    0.8148625 \n",
      "[Resample] iter 4:    0.8196613 \n",
      "[Resample] iter 5:    0.7933325 \n",
      "[Resample] iter 6:    0.7811386 \n",
      "[Resample] iter 7:    0.7879470 \n",
      "[Resample] iter 8:    0.8154773 \n",
      "[Resample] iter 9:    0.7861736 \n",
      "[Resample] iter 10:   0.8071130 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7917822\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7529994 \n",
      "[Resample] iter 2:    0.7742137 \n",
      "[Resample] iter 3:    0.7825092 \n",
      "[Resample] iter 4:    0.7472535 \n",
      "[Resample] iter 5:    0.7875954 \n",
      "[Resample] iter 6:    0.8126184 \n",
      "[Resample] iter 7:    0.8010807 \n",
      "[Resample] iter 8:    0.8361614 \n",
      "[Resample] iter 9:    0.7901024 \n",
      "[Resample] iter 10:   0.8334630 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7917997\n",
      "\n",
      "\n",
      "Resampling: cross-validation\n",
      "Measures:             auc       \n",
      "[Resample] iter 1:    0.7961362 \n",
      "[Resample] iter 2:    0.8016062 \n",
      "[Resample] iter 3:    0.7649644 \n",
      "[Resample] iter 4:    0.7524825 \n",
      "[Resample] iter 5:    0.8186704 \n",
      "[Resample] iter 6:    0.8314377 \n",
      "[Resample] iter 7:    0.7730374 \n",
      "[Resample] iter 8:    0.8105600 \n",
      "[Resample] iter 9:    0.7662585 \n",
      "[Resample] iter 10:   0.7937820 \n",
      "\n",
      "\n",
      "Aggregated Result: auc.test.mean=0.7908935\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "options(warn=-1)\n",
    "\n",
    "#We initiate empty vector for listing the algorithms:\n",
    "base = c()\n",
    "bestCombination = c()\n",
    "'%ni%' <- Negate('%in%')\n",
    "\n",
    "#We create the task\n",
    "tsk = makeClassifTask(data = DataClean, target = \"subscribe\")\n",
    "\n",
    "#And define the cross validaiton method with 10 folds\n",
    "rdesc = makeResampleDesc(\"CV\", iters=10)\n",
    "\n",
    "#Initiate a to 1 to iterate with it in a for loop\n",
    "a = 1\n",
    "\n",
    "#We create an ovreall for loop to iterate over the length of vector algorithms\n",
    "for (x in 1:length(algorithms)){\n",
    "    #If it's the first iteration, we will simply build a model:\n",
    "    if (x == 1){\n",
    "        #We try for every algorithm in the vector\n",
    "        for (i in algorithms){\n",
    "        #If it's the first iteration, we assign BestAUC and BestLearner to the score and the algorithm of the first model computed\n",
    "            if (a == 1){\n",
    "                print(i)\n",
    "                learner <- makeLearner(i, predict.type=\"prob\", fix.factors.prediction=T)\n",
    "                res <- resample(learner, tsk, rdesc, measures=list(mlr::auc))\n",
    "                BestAUC = res$aggr\n",
    "                BestLearner = learner\n",
    "            }\n",
    "            #From the moment we try a second algorithm, we compute it and check if the AUC is higher than the previous one\n",
    "            else {\n",
    "                print(i)\n",
    "                learner <- makeLearner(i, predict.type=\"prob\", fix.factors.prediction=T)\n",
    "                res <- resample(learner, tsk, rdesc, measures=list(mlr::auc))\n",
    "                #If the new computed AUC is higher, we assign the new AUC to the one obtained with the new model and assign \n",
    "                #the best learner to the algorithm\n",
    "                if (BestAUC < res$aggr){\n",
    "                    print(paste(i, \"is now the best Model by beating\", BestAUC))\n",
    "                    BestAUC = res$aggr\n",
    "                    BestLearner = i\n",
    "                }\n",
    "                else{\n",
    "                    print(paste(i, \"didn't reach the AUC of\", BestLearner))\n",
    "                }\n",
    "            }\n",
    "            a = a +1\n",
    "        }\n",
    "        #At the end of this first for loop, it will append to a list the algorithm with the best AUC overall algorithms tried\n",
    "        #individually.\n",
    "            base = append(base, BestLearner)\n",
    "            bestCombination = append(bestCombination, BestLearner)\n",
    "    }\n",
    "    else{\n",
    "        #Now, we're going to try to create stacked learner with the first algorithm and try to bag it with all others algorithms\n",
    "        for (i in algorithms){\n",
    "            if (i %ni% base){\n",
    "                base = append(base, i)\n",
    "                lrns = lapply(base, makeLearner)\n",
    "                lrns = lapply(lrns, setPredictType, \"prob\")\n",
    "                Bagging = makeStackedLearner(base.learners = lrns,\n",
    "                predict.type = \"prob\", method = \"hill.climb\")\n",
    "                resBag <- resample(Bagging, tsk, rdesc, measures=list(mlr::auc))\n",
    "                \n",
    "                #If the bag algorithm is better we assign the new auc to BestAUC and the best combination to the combination\n",
    "                #that over performed the previous one.\n",
    "                if (BestAUC < resBag$aggr){\n",
    "                    print(paste(\"Bagging is now the best Model\"))\n",
    "                    bestCombination = base\n",
    "                    BestAUC = resBag$aggr\n",
    "\n",
    "                }\n",
    "                #Else; we delete in the list the algorithm appened to the list of algorithm to try another one\n",
    "                else {\n",
    "                    base = base[-length(base)]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#At the end, it will have try every possible combinaisons with the best model individually and it will return the best stacked \n",
    "#model based on the AUC.\n",
    "\n",
    "#We compute the model with the best score previously found:\n",
    "lrns = lapply(bestCombination, makeLearner)\n",
    "lrns = lapply(lrns, setPredictType, \"prob\")\n",
    "BaggingFinal = makeStackedLearner(base.learners = lrns,\n",
    "                predict.type = \"prob\", method = \"hill.climb\")\n",
    "\n",
    "#And train it:\n",
    "\n",
    "AlgoTrainedFinal = mlr::train(BaggingFinal, tsk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>auc.test.mean:</strong> 0.801145135245697"
      ],
      "text/latex": [
       "\\textbf{auc.test.mean:} 0.801145135245697"
      ],
      "text/markdown": [
       "**auc.test.mean:** 0.801145135245697"
      ],
      "text/plain": [
       "auc.test.mean \n",
       "    0.8011451 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BestAUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated Result: auc.test.mean = 0.801145135245697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classif.earth\"      \"classif.xgboost\"    \"classif.kknn\"      \n",
      "[4] \"classif.naiveBayes\" \"classif.logreg\"     \"classif.plsdaCaret\"\n"
     ]
    }
   ],
   "source": [
    "print(bestCombination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction: 3000 observations\n",
       "predict.type: prob\n",
       "threshold: 0=0.50,1=0.50\n",
       "time: 3.58\n",
       "     prob.0    prob.1 response\n",
       "1 0.7827963 0.2172037        0\n",
       "2 0.7886434 0.2113566        0\n",
       "3 0.5821655 0.4178345        0\n",
       "4 0.8012568 0.1987432        0\n",
       "5 0.8001272 0.1998728        0\n",
       "6 0.7725639 0.2274361        0\n",
       "... (#rows: 3000, #cols: 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#and now use it to predict:\n",
    "out = predict(AlgoTrainedFinal, newdata = PredictClean) \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0    1 \n",
       "2867  133 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(out$data$response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- data.frame(client_id=PredictClean$client_id, subscribe=out$data$prob.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(output, \"C:/Users/spavot/Desktop/Big Data/Statistical & Machine Learning/Kaggle/Data/Model/Bagging/baggingStackedFunction2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
